[INFO] Scanning for projects...
[INFO] 
[INFO] -------------------< com.example:WuzzufJobsAnalysis >-------------------
[INFO] Building WuzzufJobsAnalysis 0.0.1-SNAPSHOT
[INFO] --------------------------------[ jar ]---------------------------------
[WARNING] The artifact org.slf4j:slf4j-log4j12:jar:1.7.36 has been relocated to org.slf4j:slf4j-reload4j:jar:1.7.36
[INFO] 
[INFO] --- exec-maven-plugin:3.0.0:java (default-cli) @ WuzzufJobsAnalysis ---
SLF4J: Class path contains multiple SLF4J bindings.
SLF4J: Found binding in [jar:file:/Users/sue/.m2/repository/ch/qos/logback/logback-classic/1.2.10/logback-classic-1.2.10.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: Found binding in [jar:file:/Users/sue/.m2/repository/org/slf4j/slf4j-reload4j/1.7.36/slf4j-reload4j-1.7.36.jar!/org/slf4j/impl/StaticLoggerBinder.class]
SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
SLF4J: Actual binding is of type [ch.qos.logback.classic.util.ContextSelectorStaticBinder]
22:57:11.967 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.SparkContext - Running Spark version 3.0.1
22:57:12.093 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.hadoop.metrics2.lib.MutableMetricsFactory - field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginSuccess with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, about="", sampleName="Ops", type=DEFAULT, valueName="Time", value={"Rate of successful kerberos logins and latency (milliseconds)"})
22:57:12.112 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.hadoop.metrics2.lib.MutableMetricsFactory - field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.loginFailure with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, about="", sampleName="Ops", type=DEFAULT, valueName="Time", value={"Rate of failed kerberos logins and latency (milliseconds)"})
22:57:12.112 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.hadoop.metrics2.lib.MutableMetricsFactory - field org.apache.hadoop.metrics2.lib.MutableRate org.apache.hadoop.security.UserGroupInformation$UgiMetrics.getGroups with annotation @org.apache.hadoop.metrics2.annotation.Metric(always=false, about="", sampleName="Ops", type=DEFAULT, valueName="Time", value={"GetGroups"})
22:57:12.127 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.hadoop.metrics2.impl.MetricsSystemImpl - UgiMetrics, User and group related metrics
22:57:12.607 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.hadoop.util.Shell - Failed to detect a valid hadoop home directory
java.io.IOException: Hadoop home directory  does not exist, is not a directory, or is not an absolute path.
	at org.apache.hadoop.util.Shell.checkHadoopHome(Shell.java:339)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:354)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:80)
	at org.apache.hadoop.security.SecurityUtil.getAuthenticationMethod(SecurityUtil.java:611)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:274)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:262)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:807)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:777)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:650)
	at org.apache.spark.util.Utils$.$anonfun$getCurrentUserName$1(Utils.scala:2412)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2412)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:308)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2574)
	at org.apache.spark.sql.SparkSession$Builder.$anonfun$getOrCreate$2(SparkSession.scala:934)
	at scala.Option.getOrElse(Option.scala:189)
	at org.apache.spark.sql.SparkSession$Builder.getOrCreate(SparkSession.scala:928)
	at com.example.main.WuzzufJobsAnalysisApplication.main(WuzzufJobsAnalysisApplication.java:51)
	at org.codehaus.mojo.exec.ExecJavaMojo$1.run(ExecJavaMojo.java:254)
	at java.base/java.lang.Thread.run(Thread.java:834)
22:57:12.634 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.hadoop.util.Shell - setsid is not available on this machine. So not using it.
22:57:12.634 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.hadoop.util.Shell - setsid exited with exit code 0
22:57:12.749 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.hadoop.security.authentication.util.KerberosName - Kerberos krb5 configuration not found, setting default realm to empty
22:57:12.755 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.hadoop.security.Groups -  Creating new Groups object
22:57:12.757 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.hadoop.util.NativeCodeLoader - Trying to load the custom-built native-hadoop library...
22:57:12.757 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.hadoop.util.NativeCodeLoader - Failed to load native-hadoop with error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path: [/Users/sue/Library/Java/Extensions, /Library/Java/Extensions, /Network/Library/Java/Extensions, /System/Library/Java/Extensions, /usr/lib/java, .]
22:57:12.757 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.hadoop.util.NativeCodeLoader - java.library.path=/Users/sue/Library/Java/Extensions:/Library/Java/Extensions:/Network/Library/Java/Extensions:/System/Library/Java/Extensions:/usr/lib/java:.
22:57:12.757 [com.example.main.WuzzufJobsAnalysisApplication.main()] WARN org.apache.hadoop.util.NativeCodeLoader - Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
22:57:12.758 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.hadoop.util.PerformanceAdvisory - Falling back to shell based
22:57:12.758 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback - Group mapping impl=org.apache.hadoop.security.ShellBasedUnixGroupsMapping
22:57:12.935 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.hadoop.security.Groups - Group mapping impl=org.apache.hadoop.security.JniBasedUnixGroupsMappingWithFallback; cacheTimeout=300000; warningDeltaMs=5000
22:57:13.002 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.hadoop.security.UserGroupInformation - hadoop login
22:57:13.003 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.hadoop.security.UserGroupInformation - hadoop login commit
22:57:13.006 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.hadoop.security.UserGroupInformation - using local user:UnixPrincipal: sue
22:57:13.006 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.hadoop.security.UserGroupInformation - Using user: "UnixPrincipal: sue" with name sue
22:57:13.007 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.hadoop.security.UserGroupInformation - User entry: "sue"
22:57:13.008 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.hadoop.security.UserGroupInformation - Assuming keytab is managed externally since logged in from subject.
22:57:13.010 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.hadoop.security.UserGroupInformation - UGI loginUser:sue (auth:SIMPLE)
22:57:13.068 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.resource.ResourceUtils - ==============================================================
22:57:13.071 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.resource.ResourceUtils - Resources for spark.driver:

22:57:13.071 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.resource.ResourceUtils - ==============================================================
22:57:13.073 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.SparkContext - Submitted application: Java Spark ML project
22:57:13.177 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.SecurityManager - Changing view acls to: sue
22:57:13.177 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.SecurityManager - Changing modify acls to: sue
22:57:13.178 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.SecurityManager - Changing view acls groups to: 
22:57:13.178 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.SecurityManager - Changing modify acls groups to: 
22:57:13.178 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.SecurityManager - SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(sue); groups with view permissions: Set(); users  with modify permissions: Set(sue); groups with modify permissions: Set()
22:57:13.478 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG io.netty.util.internal.logging.InternalLoggerFactory - Using SLF4J as the default logging framework
22:57:13.480 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG io.netty.util.internal.InternalThreadLocalMap - -Dio.netty.threadLocalMap.stringBuilder.initialSize: 1024
22:57:13.480 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG io.netty.util.internal.InternalThreadLocalMap - -Dio.netty.threadLocalMap.stringBuilder.maxSize: 4096
22:57:13.499 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG io.netty.channel.MultithreadEventLoopGroup - -Dio.netty.eventLoopThreads: 16
22:57:13.543 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG io.netty.util.internal.PlatformDependent0 - -Dio.netty.noUnsafe: false
22:57:13.544 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG io.netty.util.internal.PlatformDependent0 - Java version: 11
22:57:13.545 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG io.netty.util.internal.PlatformDependent0 - sun.misc.Unsafe.theUnsafe: available
22:57:13.547 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG io.netty.util.internal.PlatformDependent0 - sun.misc.Unsafe.copyMemory: available
22:57:13.547 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG io.netty.util.internal.PlatformDependent0 - java.nio.Buffer.address: available
22:57:13.548 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG io.netty.util.internal.PlatformDependent0 - direct buffer constructor: unavailable: Reflective setAccessible(true) disabled
22:57:13.548 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG io.netty.util.internal.PlatformDependent0 - java.nio.Bits.unaligned: available, true
22:57:13.551 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG io.netty.util.internal.PlatformDependent0 - jdk.internal.misc.Unsafe.allocateUninitializedArray(int): unavailable: class io.netty.util.internal.PlatformDependent0$6 cannot access class jdk.internal.misc.Unsafe (in module java.base) because module java.base does not export jdk.internal.misc to unnamed module @7c449e45
22:57:13.558 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG io.netty.util.internal.PlatformDependent0 - java.nio.DirectByteBuffer.<init>(long, int): unavailable
22:57:13.558 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG io.netty.util.internal.PlatformDependent - sun.misc.Unsafe: available
22:57:13.570 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG io.netty.util.internal.PlatformDependent - maxDirectMemory: 2147483648 bytes (maybe)
22:57:13.571 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG io.netty.util.internal.PlatformDependent - -Dio.netty.tmpdir: /var/folders/1_/2llzxgqj7sd21dsnfnl414_m0000gn/T (java.io.tmpdir)
22:57:13.571 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG io.netty.util.internal.PlatformDependent - -Dio.netty.bitMode: 64 (sun.arch.data.model)
22:57:13.572 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG io.netty.util.internal.PlatformDependent - Platform: MacOS
22:57:13.573 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG io.netty.util.internal.PlatformDependent - -Dio.netty.maxDirectMemory: -1 bytes
22:57:13.573 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG io.netty.util.internal.PlatformDependent - -Dio.netty.uninitializedArrayAllocationThreshold: -1
22:57:13.575 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG io.netty.util.internal.CleanerJava9 - java.nio.ByteBuffer.cleaner(): available
22:57:13.575 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG io.netty.util.internal.PlatformDependent - -Dio.netty.noPreferDirect: false
22:57:13.575 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG io.netty.channel.nio.NioEventLoop - -Dio.netty.noKeySetOptimization: false
22:57:13.575 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG io.netty.channel.nio.NioEventLoop - -Dio.netty.selectorAutoRebuildThreshold: 512
22:57:13.584 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG io.netty.util.internal.PlatformDependent - org.jctools-core.MpscChunkedArrayQueue: available
22:57:13.604 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG io.netty.util.ResourceLeakDetector - -Dio.netty.leakDetection.level: simple
22:57:13.604 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG io.netty.util.ResourceLeakDetector - -Dio.netty.leakDetection.targetRecords: 4
22:57:13.609 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.numHeapArenas: 16
22:57:13.609 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.numDirectArenas: 16
22:57:13.609 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.pageSize: 8192
22:57:13.609 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.maxOrder: 11
22:57:13.609 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.chunkSize: 16777216
22:57:13.609 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.smallCacheSize: 256
22:57:13.609 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.normalCacheSize: 64
22:57:13.609 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.maxCachedBufferCapacity: 32768
22:57:13.609 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.cacheTrimInterval: 8192
22:57:13.609 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.cacheTrimIntervalMillis: 0
22:57:13.609 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.useCacheForAllThreads: true
22:57:13.609 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG io.netty.buffer.PooledByteBufAllocator - -Dio.netty.allocator.maxCachedByteBuffersPerChunk: 1023
22:57:13.681 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG io.netty.channel.DefaultChannelId - -Dio.netty.processId: 95669 (auto-detected)
22:57:13.684 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG io.netty.util.NetUtil - -Djava.net.preferIPv4Stack: false
22:57:13.684 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG io.netty.util.NetUtil - -Djava.net.preferIPv6Addresses: false
22:57:13.690 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG io.netty.util.NetUtilInitializations - Loopback interface: lo0 (lo0, 0:0:0:0:0:0:0:1%lo0)
22:57:13.691 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG io.netty.util.NetUtil - Failed to get SOMAXCONN from sysctl and file /proc/sys/net/core/somaxconn. Default: 128
22:57:13.694 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG io.netty.channel.DefaultChannelId - -Dio.netty.machineId: 3c:a6:f6:ff:fe:4e:65:1a (auto-detected)
22:57:13.729 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG io.netty.buffer.ByteBufUtil - -Dio.netty.allocator.type: pooled
22:57:13.730 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG io.netty.buffer.ByteBufUtil - -Dio.netty.threadLocalDirectBufferSize: 0
22:57:13.730 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG io.netty.buffer.ByteBufUtil - -Dio.netty.maxThreadLocalCharBufferSize: 16384
22:57:13.827 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.network.server.TransportServer - Shuffle server started on port: 55956
22:57:13.847 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.util.Utils - Successfully started service 'sparkDriver' on port 55956.
22:57:13.853 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.SparkEnv - Using serializer: class org.apache.spark.serializer.JavaSerializer
22:57:13.894 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.SparkEnv - Registering MapOutputTracker
22:57:13.895 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.MapOutputTrackerMasterEndpoint - init
22:57:13.971 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.SparkEnv - Registering BlockManagerMaster
22:57:13.995 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.storage.BlockManagerMasterEndpoint - Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
22:57:13.996 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.storage.BlockManagerMasterEndpoint - BlockManagerMasterEndpoint up
22:57:14.006 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.SparkEnv - Registering BlockManagerMasterHeartbeat
22:57:14.027 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.storage.DiskBlockManager - Created local directory at /private/var/folders/1_/2llzxgqj7sd21dsnfnl414_m0000gn/T/blockmgr-ebecf8e2-263d-4944-bae1-f5e1eaed5ce0
22:57:14.028 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.storage.DiskBlockManager - Adding shutdown hook
22:57:14.030 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.util.ShutdownHookManager - Adding shutdown hook
22:57:14.068 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.storage.memory.MemoryStore - MemoryStore started with capacity 1048.8 MiB
22:57:14.098 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.SparkEnv - Registering OutputCommitCoordinator
22:57:14.099 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint - init
22:57:14.138 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.SecurityManager - Created SSL options for ui: SSLOptions{enabled=false, port=None, keyStore=None, keyStorePassword=None, trustStore=None, trustStorePassword=None, protocol=None, enabledAlgorithms=Set()}
22:57:14.262 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.log - Logging to Logger[org.sparkproject.jetty.util.log] via org.sparkproject.jetty.util.log.Slf4jLog
22:57:14.269 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.sparkproject.jetty.util.log - Logging initialized @11041ms to org.sparkproject.jetty.util.log.Slf4jLog
22:57:14.295 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.DecoratedObjectFactory - Adding Decorator: org.sparkproject.jetty.util.DeprecationWarning@66978ad0
22:57:14.301 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - o.s.j.s.ServletContextHandler@5d2d691a{/,null,UNAVAILABLE} added {ServletHandler@197508f{STOPPED},MANAGED}
22:57:14.308 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.DecoratedObjectFactory - Adding Decorator: org.sparkproject.jetty.util.DeprecationWarning@d334c4b
22:57:14.308 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - o.s.j.s.ServletContextHandler@6b382dd6{/,null,UNAVAILABLE} added {ServletHandler@6dcff13a{STOPPED},MANAGED}
22:57:14.309 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.DecoratedObjectFactory - Adding Decorator: org.sparkproject.jetty.util.DeprecationWarning@f2f55ac
22:57:14.309 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - o.s.j.s.ServletContextHandler@1fdad46b{/,null,UNAVAILABLE} added {ServletHandler@7f2aba89{STOPPED},MANAGED}
22:57:14.310 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.DecoratedObjectFactory - Adding Decorator: org.sparkproject.jetty.util.DeprecationWarning@1fe09a42
22:57:14.310 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - o.s.j.s.ServletContextHandler@56d07c59{/,null,UNAVAILABLE} added {ServletHandler@7c046af8{STOPPED},MANAGED}
22:57:14.318 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.DecoratedObjectFactory - Adding Decorator: org.sparkproject.jetty.util.DeprecationWarning@397c628d
22:57:14.319 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - o.s.j.s.ServletContextHandler@5e077920{/,null,UNAVAILABLE} added {ServletHandler@55ea2e38{STOPPED},MANAGED}
22:57:14.319 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.DecoratedObjectFactory - Adding Decorator: org.sparkproject.jetty.util.DeprecationWarning@6b776967
22:57:14.319 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - o.s.j.s.ServletContextHandler@edcc350{/,null,UNAVAILABLE} added {ServletHandler@ac3a75d{STOPPED},MANAGED}
22:57:14.319 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.DecoratedObjectFactory - Adding Decorator: org.sparkproject.jetty.util.DeprecationWarning@2b33b2b8
22:57:14.319 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - o.s.j.s.ServletContextHandler@3cdd11cc{/,null,UNAVAILABLE} added {ServletHandler@25b1a317{STOPPED},MANAGED}
22:57:14.319 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.DecoratedObjectFactory - Adding Decorator: org.sparkproject.jetty.util.DeprecationWarning@39e87eb3
22:57:14.319 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - o.s.j.s.ServletContextHandler@6def4b33{/,null,UNAVAILABLE} added {ServletHandler@456ee482{STOPPED},MANAGED}
22:57:14.320 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.DecoratedObjectFactory - Adding Decorator: org.sparkproject.jetty.util.DeprecationWarning@6246d25b
22:57:14.320 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - o.s.j.s.ServletContextHandler@50be7065{/,null,UNAVAILABLE} added {ServletHandler@5d2ea403{STOPPED},MANAGED}
22:57:14.320 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.DecoratedObjectFactory - Adding Decorator: org.sparkproject.jetty.util.DeprecationWarning@41c9dd58
22:57:14.320 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - o.s.j.s.ServletContextHandler@6257b05c{/,null,UNAVAILABLE} added {ServletHandler@4af11ea9{STOPPED},MANAGED}
22:57:14.324 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.DecoratedObjectFactory - Adding Decorator: org.sparkproject.jetty.util.DeprecationWarning@2f6305ae
22:57:14.325 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - o.s.j.s.ServletContextHandler@49d00b2a{/,null,UNAVAILABLE} added {ServletHandler@6b45f59d{STOPPED},MANAGED}
22:57:14.325 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.DecoratedObjectFactory - Adding Decorator: org.sparkproject.jetty.util.DeprecationWarning@1bc8f936
22:57:14.326 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - o.s.j.s.ServletContextHandler@5288383e{/,null,UNAVAILABLE} added {ServletHandler@16a5c6ef{STOPPED},MANAGED}
22:57:14.327 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.DecoratedObjectFactory - Adding Decorator: org.sparkproject.jetty.util.DeprecationWarning@7b0a035d
22:57:14.327 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - o.s.j.s.ServletContextHandler@4320206d{/,null,UNAVAILABLE} added {ServletHandler@19fd96ef{STOPPED},MANAGED}
22:57:14.327 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.DecoratedObjectFactory - Adding Decorator: org.sparkproject.jetty.util.DeprecationWarning@5e07dae7
22:57:14.327 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - o.s.j.s.ServletContextHandler@1ce6aa02{/,null,UNAVAILABLE} added {ServletHandler@69590d9f{STOPPED},MANAGED}
22:57:14.328 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.DecoratedObjectFactory - Adding Decorator: org.sparkproject.jetty.util.DeprecationWarning@4be46b64
22:57:14.333 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - o.s.j.s.ServletContextHandler@73adebf8{/,null,UNAVAILABLE} added {ServletHandler@7e44cf94{STOPPED},MANAGED}
22:57:14.334 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.DecoratedObjectFactory - Adding Decorator: org.sparkproject.jetty.util.DeprecationWarning@296f6e88
22:57:14.334 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - o.s.j.s.ServletContextHandler@3501a276{/,null,UNAVAILABLE} added {ServletHandler@6de1f32d{STOPPED},MANAGED}
22:57:14.336 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.DecoratedObjectFactory - Adding Decorator: org.sparkproject.jetty.util.DeprecationWarning@51d3c5fa
22:57:14.337 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - o.s.j.s.ServletContextHandler@328c8921{/,null,UNAVAILABLE} added {ServletHandler@4d623639{STOPPED},MANAGED}
22:57:14.337 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.DecoratedObjectFactory - Adding Decorator: org.sparkproject.jetty.util.DeprecationWarning@89462d8
22:57:14.337 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - o.s.j.s.ServletContextHandler@5e87276a{/,null,UNAVAILABLE} added {ServletHandler@208117d{STOPPED},MANAGED}
22:57:14.337 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.DecoratedObjectFactory - Adding Decorator: org.sparkproject.jetty.util.DeprecationWarning@3538ddf6
22:57:14.338 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - o.s.j.s.ServletContextHandler@7779b1d5{/,null,UNAVAILABLE} added {ServletHandler@5f84694d{STOPPED},MANAGED}
22:57:14.338 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.DecoratedObjectFactory - Adding Decorator: org.sparkproject.jetty.util.DeprecationWarning@66ec780c
22:57:14.338 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - o.s.j.s.ServletContextHandler@5ca18a1d{/,null,UNAVAILABLE} added {ServletHandler@20934ec5{STOPPED},MANAGED}
22:57:14.340 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.DecoratedObjectFactory - Adding Decorator: org.sparkproject.jetty.util.DeprecationWarning@26810417
22:57:14.341 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - o.s.j.s.ServletContextHandler@1c5e9364{/,null,UNAVAILABLE} added {ServletHandler@22776cdd{STOPPED},MANAGED}
22:57:14.347 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.http.PreEncodedHttpField - HttpField encoders loaded: []
22:57:14.355 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.DecoratedObjectFactory - Adding Decorator: org.sparkproject.jetty.util.DeprecationWarning@6dbedb66
22:57:14.357 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - o.s.j.s.ServletContextHandler@19db68e6{/,null,UNAVAILABLE} added {ServletHandler@6113941b{STOPPED},MANAGED}
22:57:14.358 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.DecoratedObjectFactory - Adding Decorator: org.sparkproject.jetty.util.DeprecationWarning@1a77062b
22:57:14.358 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - o.s.j.s.ServletContextHandler@24d35d46{/,null,UNAVAILABLE} added {ServletHandler@2ecc7caa{STOPPED},MANAGED}
22:57:14.364 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.DecoratedObjectFactory - Adding Decorator: org.sparkproject.jetty.util.DeprecationWarning@60adebfa
22:57:14.365 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - o.s.j.s.ServletContextHandler@6cd59fdc{/,null,UNAVAILABLE} added {ServletHandler@53b2b1c0{STOPPED},MANAGED}
22:57:14.365 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.DecoratedObjectFactory - Adding Decorator: org.sparkproject.jetty.util.DeprecationWarning@3628f9b9
22:57:14.365 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - o.s.j.s.ServletContextHandler@7052620d{/,null,UNAVAILABLE} added {ServletHandler@7a4fbee6{STOPPED},MANAGED}
22:57:14.373 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - Server@1e82d188{STOPPED}[9.4.z-SNAPSHOT] added {QueuedThreadPool[SparkUI]@5dac3d49{STOPPED,8<=0<=200,i=0,r=-1,q=0}[NO_TRY],AUTO}
22:57:14.376 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - Server@1e82d188{STOPPED}[9.4.z-SNAPSHOT] added {ErrorHandler@397cd7ee{STOPPED},AUTO}
22:57:14.379 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - Server@1e82d188{STOPPED}[9.4.z-SNAPSHOT] added {ContextHandlerCollection@7d170ea6{STOPPED},MANAGED}
22:57:14.380 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting Server@1e82d188{STOPPED}[9.4.z-SNAPSHOT]
22:57:14.381 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.sparkproject.jetty.server.Server - jetty-9.4.z-SNAPSHOT; built: 2019-04-29T20:42:08.989Z; git: e1bc35120a6617ee3df052294e433f3a25ce7097; jvm 11.0.14+8-LTS-263
22:57:14.402 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting Server@1e82d188{STARTING}[9.4.z-SNAPSHOT]
22:57:14.403 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting QueuedThreadPool[SparkUI]@5dac3d49{STOPPED,8<=0<=200,i=0,r=-1,q=0}[NO_TRY]
22:57:14.406 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.thread.ReservedThreadExecutor - ReservedThreadExecutor@23198107{s=0/8,p=0}
22:57:14.407 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - QueuedThreadPool[SparkUI]@5dac3d49{STARTING,8<=0<=200,i=0,r=-1,q=0}[ReservedThreadExecutor@23198107{s=0/8,p=0}] added {ReservedThreadExecutor@23198107{s=0/8,p=0},AUTO}
22:57:14.407 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting ReservedThreadExecutor@23198107{s=0/8,p=0}
22:57:14.416 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11189ms ReservedThreadExecutor@23198107{s=0/8,p=0}
22:57:14.417 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.thread.QueuedThreadPool - Starting Thread[SparkUI-31,5,com.example.main.WuzzufJobsAnalysisApplication]
22:57:14.417 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.thread.QueuedThreadPool - Starting Thread[SparkUI-32,5,com.example.main.WuzzufJobsAnalysisApplication]
22:57:14.418 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.thread.QueuedThreadPool - Starting Thread[SparkUI-33,5,com.example.main.WuzzufJobsAnalysisApplication]
22:57:14.419 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.thread.QueuedThreadPool - Starting Thread[SparkUI-34,5,com.example.main.WuzzufJobsAnalysisApplication]
22:57:14.419 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.thread.QueuedThreadPool - Starting Thread[SparkUI-35,5,com.example.main.WuzzufJobsAnalysisApplication]
22:57:14.419 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.thread.QueuedThreadPool - Starting Thread[SparkUI-36,5,com.example.main.WuzzufJobsAnalysisApplication]
22:57:14.420 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.thread.QueuedThreadPool - Starting Thread[SparkUI-37,5,com.example.main.WuzzufJobsAnalysisApplication]
22:57:14.420 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.thread.QueuedThreadPool - Starting Thread[SparkUI-38,5,com.example.main.WuzzufJobsAnalysisApplication]
22:57:14.421 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11194ms QueuedThreadPool[SparkUI]@5dac3d49{STARTED,8<=8<=200,i=7,r=8,q=0}[ReservedThreadExecutor@23198107{s=0/8,p=0}]
22:57:14.421 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting ErrorHandler@397cd7ee{STOPPED}
22:57:14.421 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting ErrorHandler@397cd7ee{STARTING}
22:57:14.421 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11194ms ErrorHandler@397cd7ee{STARTED}
22:57:14.421 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting ContextHandlerCollection@7d170ea6{STOPPED}
22:57:14.421 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting ContextHandlerCollection@7d170ea6{STARTING}
22:57:14.421 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11194ms ContextHandlerCollection@7d170ea6{STARTED}
22:57:14.421 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.sparkproject.jetty.server.Server - Started @11195ms
22:57:14.422 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11195ms Server@1e82d188{STARTED}[9.4.z-SNAPSHOT]
22:57:14.425 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.ui.JettyUtils - Using requestHeaderSize: 8192
22:57:14.437 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - HttpConnectionFactory@2acef75d[HTTP/1.1] added {HttpConfiguration@6305c5f3{32768/8192,8192/8192,https://:0,[]},POJO}
22:57:14.447 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - ServerConnector@71f030a{null,[]}{0.0.0.0:0} added {Server@1e82d188{STARTED}[9.4.z-SNAPSHOT],UNMANAGED}
22:57:14.448 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - ServerConnector@71f030a{null,[]}{0.0.0.0:0} added {QueuedThreadPool[SparkUI]@5dac3d49{STARTED,8<=8<=200,i=8,r=8,q=0}[ReservedThreadExecutor@23198107{s=0/8,p=0}],UNMANAGED}
22:57:14.448 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - ServerConnector@71f030a{null,[]}{0.0.0.0:0} added {ScheduledExecutorScheduler@71d5e1a6{STOPPED},AUTO}
22:57:14.448 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - ServerConnector@71f030a{null,[]}{0.0.0.0:0} added {org.sparkproject.jetty.io.ArrayByteBufferPool@62069aa8,POJO}
22:57:14.448 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - ServerConnector@71f030a{null,[http/1.1]}{0.0.0.0:0} added {HttpConnectionFactory@2acef75d[HTTP/1.1],AUTO}
22:57:14.448 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.AbstractConnector - ServerConnector@71f030a{HTTP/1.1,[http/1.1]}{0.0.0.0:0} added HttpConnectionFactory@2acef75d[HTTP/1.1]
22:57:14.455 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - ServerConnector@71f030a{HTTP/1.1,[http/1.1]}{0.0.0.0:0} added {SelectorManager@ServerConnector@71f030a{HTTP/1.1,[http/1.1]}{0.0.0.0:0},MANAGED}
22:57:14.455 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting ServerConnector@71f030a{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
22:57:14.455 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - ServerConnector@71f030a{HTTP/1.1,[http/1.1]}{0.0.0.0:4040} added {sun.nio.ch.ServerSocketChannelImpl[/0:0:0:0:0:0:0:0:4040],POJO}
22:57:14.456 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting ScheduledExecutorScheduler@71d5e1a6{STOPPED}
22:57:14.456 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11229ms ScheduledExecutorScheduler@71d5e1a6{STARTED}
22:57:14.456 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting HttpConnectionFactory@2acef75d[HTTP/1.1]
22:57:14.456 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11230ms HttpConnectionFactory@2acef75d[HTTP/1.1]
22:57:14.457 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting SelectorManager@ServerConnector@71f030a{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
22:57:14.474 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - EatWhatYouKill@4fac2436/SelectorProducer@46b8db83/IDLE/p=false/QueuedThreadPool[SparkUI]@5dac3d49{STARTED,8<=8<=200,i=8,r=8,q=0}[ReservedThreadExecutor@23198107{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-03-04T22:57:14.473378+02:00 added {SelectorProducer@46b8db83,POJO}
22:57:14.475 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - EatWhatYouKill@4fac2436/SelectorProducer@46b8db83/IDLE/p=false/QueuedThreadPool[SparkUI]@5dac3d49{STARTED,8<=8<=200,i=8,r=8,q=0}[ReservedThreadExecutor@23198107{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-03-04T22:57:14.47514+02:00 added {QueuedThreadPool[SparkUI]@5dac3d49{STARTED,8<=8<=200,i=8,r=8,q=0}[ReservedThreadExecutor@23198107{s=0/8,p=0}],UNMANAGED}
22:57:14.475 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill - EatWhatYouKill@4fac2436/SelectorProducer@46b8db83/IDLE/p=false/QueuedThreadPool[SparkUI]@5dac3d49{STARTED,8<=8<=200,i=8,r=8,q=0}[ReservedThreadExecutor@23198107{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-03-04T22:57:14.475693+02:00 created
22:57:14.476 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - ManagedSelector@1825ff79{STOPPED} id=0 keys=-1 selected=-1 updates=0 added {EatWhatYouKill@4fac2436/SelectorProducer@46b8db83/IDLE/p=false/QueuedThreadPool[SparkUI]@5dac3d49{STARTED,8<=8<=200,i=8,r=8,q=0}[ReservedThreadExecutor@23198107{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-03-04T22:57:14.476224+02:00,MANAGED}
22:57:14.476 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - SelectorManager@ServerConnector@71f030a{HTTP/1.1,[http/1.1]}{0.0.0.0:4040} added {ManagedSelector@1825ff79{STOPPED} id=0 keys=-1 selected=-1 updates=0,AUTO}
22:57:14.477 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - EatWhatYouKill@1a77f70d/SelectorProducer@5edc9d72/IDLE/p=false/QueuedThreadPool[SparkUI]@5dac3d49{STARTED,8<=8<=200,i=8,r=8,q=0}[ReservedThreadExecutor@23198107{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-03-04T22:57:14.477056+02:00 added {SelectorProducer@5edc9d72,POJO}
22:57:14.483 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - EatWhatYouKill@1a77f70d/SelectorProducer@5edc9d72/IDLE/p=false/QueuedThreadPool[SparkUI]@5dac3d49{STARTED,8<=8<=200,i=8,r=8,q=0}[ReservedThreadExecutor@23198107{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-03-04T22:57:14.480691+02:00 added {QueuedThreadPool[SparkUI]@5dac3d49{STARTED,8<=8<=200,i=8,r=8,q=0}[ReservedThreadExecutor@23198107{s=0/8,p=0}],UNMANAGED}
22:57:14.484 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill - EatWhatYouKill@1a77f70d/SelectorProducer@5edc9d72/IDLE/p=false/QueuedThreadPool[SparkUI]@5dac3d49{STARTED,8<=8<=200,i=8,r=8,q=0}[ReservedThreadExecutor@23198107{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-03-04T22:57:14.484633+02:00 created
22:57:14.485 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - ManagedSelector@4daaa898{STOPPED} id=1 keys=-1 selected=-1 updates=0 added {EatWhatYouKill@1a77f70d/SelectorProducer@5edc9d72/IDLE/p=false/QueuedThreadPool[SparkUI]@5dac3d49{STARTED,8<=8<=200,i=8,r=8,q=0}[ReservedThreadExecutor@23198107{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-03-04T22:57:14.485514+02:00,MANAGED}
22:57:14.486 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - SelectorManager@ServerConnector@71f030a{HTTP/1.1,[http/1.1]}{0.0.0.0:4040} added {ManagedSelector@4daaa898{STOPPED} id=1 keys=-1 selected=-1 updates=0,AUTO}
22:57:14.489 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - EatWhatYouKill@7a435863/SelectorProducer@5f2917b9/IDLE/p=false/QueuedThreadPool[SparkUI]@5dac3d49{STARTED,8<=8<=200,i=8,r=8,q=0}[ReservedThreadExecutor@23198107{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-03-04T22:57:14.489132+02:00 added {SelectorProducer@5f2917b9,POJO}
22:57:14.489 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - EatWhatYouKill@7a435863/SelectorProducer@5f2917b9/IDLE/p=false/QueuedThreadPool[SparkUI]@5dac3d49{STARTED,8<=8<=200,i=8,r=8,q=0}[ReservedThreadExecutor@23198107{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-03-04T22:57:14.489474+02:00 added {QueuedThreadPool[SparkUI]@5dac3d49{STARTED,8<=8<=200,i=8,r=8,q=0}[ReservedThreadExecutor@23198107{s=0/8,p=0}],UNMANAGED}
22:57:14.489 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill - EatWhatYouKill@7a435863/SelectorProducer@5f2917b9/IDLE/p=false/QueuedThreadPool[SparkUI]@5dac3d49{STARTED,8<=8<=200,i=8,r=8,q=0}[ReservedThreadExecutor@23198107{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-03-04T22:57:14.4897+02:00 created
22:57:14.489 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - ManagedSelector@65a42623{STOPPED} id=2 keys=-1 selected=-1 updates=0 added {EatWhatYouKill@7a435863/SelectorProducer@5f2917b9/IDLE/p=false/QueuedThreadPool[SparkUI]@5dac3d49{STARTED,8<=8<=200,i=8,r=8,q=0}[ReservedThreadExecutor@23198107{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-03-04T22:57:14.489922+02:00,MANAGED}
22:57:14.490 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - SelectorManager@ServerConnector@71f030a{HTTP/1.1,[http/1.1]}{0.0.0.0:4040} added {ManagedSelector@65a42623{STOPPED} id=2 keys=-1 selected=-1 updates=0,AUTO}
22:57:14.490 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - EatWhatYouKill@7dd2bc67/SelectorProducer@1db273fe/IDLE/p=false/QueuedThreadPool[SparkUI]@5dac3d49{STARTED,8<=8<=200,i=8,r=8,q=0}[ReservedThreadExecutor@23198107{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-03-04T22:57:14.490683+02:00 added {SelectorProducer@1db273fe,POJO}
22:57:14.490 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - EatWhatYouKill@7dd2bc67/SelectorProducer@1db273fe/IDLE/p=false/QueuedThreadPool[SparkUI]@5dac3d49{STARTED,8<=8<=200,i=8,r=8,q=0}[ReservedThreadExecutor@23198107{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-03-04T22:57:14.490883+02:00 added {QueuedThreadPool[SparkUI]@5dac3d49{STARTED,8<=8<=200,i=8,r=8,q=0}[ReservedThreadExecutor@23198107{s=0/8,p=0}],UNMANAGED}
22:57:14.491 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill - EatWhatYouKill@7dd2bc67/SelectorProducer@1db273fe/IDLE/p=false/QueuedThreadPool[SparkUI]@5dac3d49{STARTED,8<=8<=200,i=8,r=8,q=0}[ReservedThreadExecutor@23198107{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-03-04T22:57:14.491074+02:00 created
22:57:14.491 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - ManagedSelector@a312b2e{STOPPED} id=3 keys=-1 selected=-1 updates=0 added {EatWhatYouKill@7dd2bc67/SelectorProducer@1db273fe/IDLE/p=false/QueuedThreadPool[SparkUI]@5dac3d49{STARTED,8<=8<=200,i=8,r=8,q=0}[ReservedThreadExecutor@23198107{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-03-04T22:57:14.49138+02:00,MANAGED}
22:57:14.493 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - SelectorManager@ServerConnector@71f030a{HTTP/1.1,[http/1.1]}{0.0.0.0:4040} added {ManagedSelector@a312b2e{STOPPED} id=3 keys=-1 selected=-1 updates=0,AUTO}
22:57:14.493 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting ManagedSelector@1825ff79{STOPPED} id=0 keys=-1 selected=-1 updates=0
22:57:14.493 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting EatWhatYouKill@4fac2436/SelectorProducer@46b8db83/IDLE/p=false/QueuedThreadPool[SparkUI]@5dac3d49{STARTED,8<=8<=200,i=8,r=8,q=0}[ReservedThreadExecutor@23198107{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-03-04T22:57:14.493323+02:00
22:57:14.493 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11266ms EatWhatYouKill@4fac2436/SelectorProducer@46b8db83/IDLE/p=false/QueuedThreadPool[SparkUI]@5dac3d49{STARTED,8<=8<=200,i=8,r=8,q=0}[ReservedThreadExecutor@23198107{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-03-04T22:57:14.493534+02:00
22:57:14.496 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.thread.QueuedThreadPool - queue org.sparkproject.jetty.io.ManagedSelector$$Lambda$439/0x000000080065b840@289a97f0
22:57:14.496 [SparkUI-31] DEBUG org.sparkproject.jetty.util.thread.QueuedThreadPool - run org.sparkproject.jetty.io.ManagedSelector$$Lambda$439/0x000000080065b840@289a97f0
22:57:14.501 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.io.ManagedSelector - Queued change org.sparkproject.jetty.io.ManagedSelector$Start@eec4429 on ManagedSelector@1825ff79{STARTING} id=0 keys=0 selected=0 updates=0
22:57:14.505 [SparkUI-31] DEBUG org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill - EatWhatYouKill@4fac2436/SelectorProducer@46b8db83/IDLE/p=false/QueuedThreadPool[SparkUI]@5dac3d49{STARTED,8<=8<=200,i=7,r=8,q=0}[ReservedThreadExecutor@23198107{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-03-04T22:57:14.505121+02:00 tryProduce false
22:57:14.507 [SparkUI-31] DEBUG org.sparkproject.jetty.io.ManagedSelector - updateable 1
22:57:14.508 [SparkUI-31] DEBUG org.sparkproject.jetty.io.ManagedSelector - update org.sparkproject.jetty.io.ManagedSelector$Start@eec4429
22:57:14.508 [SparkUI-31] DEBUG org.sparkproject.jetty.io.ManagedSelector - updates 0
22:57:14.508 [SparkUI-31] DEBUG org.sparkproject.jetty.io.ManagedSelector - Selector sun.nio.ch.KQueueSelectorImpl@6e95f513 waiting with 0 keys
22:57:14.508 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11281ms ManagedSelector@1825ff79{STARTED} id=0 keys=0 selected=0 updates=0
22:57:14.508 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting ManagedSelector@4daaa898{STOPPED} id=1 keys=-1 selected=-1 updates=0
22:57:14.509 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting EatWhatYouKill@1a77f70d/SelectorProducer@5edc9d72/IDLE/p=false/QueuedThreadPool[SparkUI]@5dac3d49{STARTED,8<=8<=200,i=7,r=8,q=0}[ReservedThreadExecutor@23198107{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-03-04T22:57:14.509025+02:00
22:57:14.509 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11282ms EatWhatYouKill@1a77f70d/SelectorProducer@5edc9d72/IDLE/p=false/QueuedThreadPool[SparkUI]@5dac3d49{STARTED,8<=8<=200,i=7,r=8,q=0}[ReservedThreadExecutor@23198107{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-03-04T22:57:14.509298+02:00
22:57:14.509 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.thread.QueuedThreadPool - queue org.sparkproject.jetty.io.ManagedSelector$$Lambda$439/0x000000080065b840@72b56713
22:57:14.509 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.io.ManagedSelector - Queued change org.sparkproject.jetty.io.ManagedSelector$Start@54ee05d2 on ManagedSelector@4daaa898{STARTING} id=1 keys=0 selected=0 updates=0
22:57:14.509 [SparkUI-32] DEBUG org.sparkproject.jetty.util.thread.QueuedThreadPool - run org.sparkproject.jetty.io.ManagedSelector$$Lambda$439/0x000000080065b840@72b56713
22:57:14.509 [SparkUI-32] DEBUG org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill - EatWhatYouKill@1a77f70d/SelectorProducer@5edc9d72/IDLE/p=false/QueuedThreadPool[SparkUI]@5dac3d49{STARTED,8<=8<=200,i=6,r=8,q=0}[ReservedThreadExecutor@23198107{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-03-04T22:57:14.509645+02:00 tryProduce false
22:57:14.510 [SparkUI-32] DEBUG org.sparkproject.jetty.io.ManagedSelector - updateable 1
22:57:14.510 [SparkUI-32] DEBUG org.sparkproject.jetty.io.ManagedSelector - update org.sparkproject.jetty.io.ManagedSelector$Start@54ee05d2
22:57:14.510 [SparkUI-32] DEBUG org.sparkproject.jetty.io.ManagedSelector - updates 0
22:57:14.510 [SparkUI-32] DEBUG org.sparkproject.jetty.io.ManagedSelector - Selector sun.nio.ch.KQueueSelectorImpl@26c2c984 waiting with 0 keys
22:57:14.511 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11283ms ManagedSelector@4daaa898{STARTED} id=1 keys=0 selected=0 updates=0
22:57:14.512 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting ManagedSelector@65a42623{STOPPED} id=2 keys=-1 selected=-1 updates=0
22:57:14.512 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting EatWhatYouKill@7a435863/SelectorProducer@5f2917b9/IDLE/p=false/QueuedThreadPool[SparkUI]@5dac3d49{STARTED,8<=8<=200,i=6,r=8,q=0}[ReservedThreadExecutor@23198107{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-03-04T22:57:14.512413+02:00
22:57:14.512 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11285ms EatWhatYouKill@7a435863/SelectorProducer@5f2917b9/IDLE/p=false/QueuedThreadPool[SparkUI]@5dac3d49{STARTED,8<=8<=200,i=6,r=8,q=0}[ReservedThreadExecutor@23198107{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-03-04T22:57:14.512605+02:00
22:57:14.512 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.thread.QueuedThreadPool - queue org.sparkproject.jetty.io.ManagedSelector$$Lambda$439/0x000000080065b840@3669acbd
22:57:14.512 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.io.ManagedSelector - Queued change org.sparkproject.jetty.io.ManagedSelector$Start@5a487b55 on ManagedSelector@65a42623{STARTING} id=2 keys=0 selected=0 updates=0
22:57:14.512 [SparkUI-33] DEBUG org.sparkproject.jetty.util.thread.QueuedThreadPool - run org.sparkproject.jetty.io.ManagedSelector$$Lambda$439/0x000000080065b840@3669acbd
22:57:14.513 [SparkUI-33] DEBUG org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill - EatWhatYouKill@7a435863/SelectorProducer@5f2917b9/IDLE/p=false/QueuedThreadPool[SparkUI]@5dac3d49{STARTED,8<=8<=200,i=5,r=8,q=0}[ReservedThreadExecutor@23198107{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-03-04T22:57:14.513362+02:00 tryProduce false
22:57:14.513 [SparkUI-33] DEBUG org.sparkproject.jetty.io.ManagedSelector - updateable 1
22:57:14.513 [SparkUI-33] DEBUG org.sparkproject.jetty.io.ManagedSelector - update org.sparkproject.jetty.io.ManagedSelector$Start@5a487b55
22:57:14.513 [SparkUI-33] DEBUG org.sparkproject.jetty.io.ManagedSelector - updates 0
22:57:14.513 [SparkUI-33] DEBUG org.sparkproject.jetty.io.ManagedSelector - Selector sun.nio.ch.KQueueSelectorImpl@6247a142 waiting with 0 keys
22:57:14.513 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11286ms ManagedSelector@65a42623{STARTED} id=2 keys=0 selected=0 updates=0
22:57:14.513 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting ManagedSelector@a312b2e{STOPPED} id=3 keys=-1 selected=-1 updates=0
22:57:14.513 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting EatWhatYouKill@7dd2bc67/SelectorProducer@1db273fe/IDLE/p=false/QueuedThreadPool[SparkUI]@5dac3d49{STARTED,8<=8<=200,i=5,r=8,q=0}[ReservedThreadExecutor@23198107{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-03-04T22:57:14.513849+02:00
22:57:14.514 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11287ms EatWhatYouKill@7dd2bc67/SelectorProducer@1db273fe/IDLE/p=false/QueuedThreadPool[SparkUI]@5dac3d49{STARTED,8<=8<=200,i=5,r=8,q=0}[ReservedThreadExecutor@23198107{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-03-04T22:57:14.514509+02:00
22:57:14.514 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.thread.QueuedThreadPool - queue org.sparkproject.jetty.io.ManagedSelector$$Lambda$439/0x000000080065b840@5c0f0bc2
22:57:14.514 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.io.ManagedSelector - Queued change org.sparkproject.jetty.io.ManagedSelector$Start@458441c5 on ManagedSelector@a312b2e{STARTING} id=3 keys=0 selected=0 updates=0
22:57:14.514 [SparkUI-34] DEBUG org.sparkproject.jetty.util.thread.QueuedThreadPool - run org.sparkproject.jetty.io.ManagedSelector$$Lambda$439/0x000000080065b840@5c0f0bc2
22:57:14.515 [SparkUI-34] DEBUG org.sparkproject.jetty.util.thread.strategy.EatWhatYouKill - EatWhatYouKill@7dd2bc67/SelectorProducer@1db273fe/IDLE/p=false/QueuedThreadPool[SparkUI]@5dac3d49{STARTED,8<=8<=200,i=4,r=8,q=0}[ReservedThreadExecutor@23198107{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-03-04T22:57:14.515105+02:00 tryProduce false
22:57:14.515 [SparkUI-34] DEBUG org.sparkproject.jetty.io.ManagedSelector - updateable 1
22:57:14.515 [SparkUI-34] DEBUG org.sparkproject.jetty.io.ManagedSelector - update org.sparkproject.jetty.io.ManagedSelector$Start@458441c5
22:57:14.515 [SparkUI-34] DEBUG org.sparkproject.jetty.io.ManagedSelector - updates 0
22:57:14.515 [SparkUI-34] DEBUG org.sparkproject.jetty.io.ManagedSelector - Selector sun.nio.ch.KQueueSelectorImpl@200bfc09 waiting with 0 keys
22:57:14.515 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11288ms ManagedSelector@a312b2e{STARTED} id=3 keys=0 selected=0 updates=0
22:57:14.515 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11288ms SelectorManager@ServerConnector@71f030a{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
22:57:14.518 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - ServerConnector@71f030a{HTTP/1.1,[http/1.1]}{0.0.0.0:4040} added {acceptor-0@57c892,POJO}
22:57:14.519 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.thread.QueuedThreadPool - queue acceptor-0@57c892
22:57:14.519 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.sparkproject.jetty.server.AbstractConnector - Started ServerConnector@71f030a{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
22:57:14.519 [SparkUI-35] DEBUG org.sparkproject.jetty.util.thread.QueuedThreadPool - run acceptor-0@57c892
22:57:14.519 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11292ms ServerConnector@71f030a{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
22:57:14.519 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.util.Utils - Successfully started service 'SparkUI' on port 4040.
22:57:14.519 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - Server@1e82d188{STARTED}[9.4.z-SNAPSHOT] added {Spark@71f030a{HTTP/1.1,[http/1.1]}{0.0.0.0:4040},UNMANAGED}
22:57:14.544 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.http.pathmap.PathMappings - Added MappedResource[pathSpec=ServletPathSpec["*.svgz",pathDepth=0,group=SUFFIX_GLOB],resource=true] to PathMappings[size=1]
22:57:14.545 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.gzip.GzipHandler - GzipHandler@3ae6a9f6{STOPPED} mime types IncludeExclude@62401002{i=[],ip=org.sparkproject.jetty.util.IncludeExcludeSet$SetContainsPredicate@7bf13c9c,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=org.sparkproject.jetty.util.IncludeExcludeSet$SetContainsPredicate@531abb1c}
22:57:14.545 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - GzipHandler@3ae6a9f6{STOPPED} added {o.s.j.s.ServletContextHandler@5d2d691a{/jobs,null,UNAVAILABLE,@Spark},MANAGED}
22:57:14.546 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs->[{GzipHandler@3ae6a9f6{STOPPED},[o.s.j.s.ServletContextHandler@5d2d691a{/jobs,null,UNAVAILABLE,@Spark}]}]
22:57:14.546 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - ContextHandlerCollection@7d170ea6{STARTED} added {GzipHandler@3ae6a9f6{STOPPED},UNMANAGED}
22:57:14.546 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting o.s.j.s.ServletContextHandler@5d2d691a{/jobs,null,UNAVAILABLE,@Spark}
22:57:14.547 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting o.s.j.s.ServletContextHandler@5d2d691a{/jobs,null,STARTING,@Spark}
22:57:14.548 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting ServletHandler@197508f{STOPPED}
22:57:14.549 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$1-303294ce[EMBEDDED:null]
22:57:14.550 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.http.pathmap.PathMappings - Added MappedResource[pathSpec=ServletPathSpec["/",pathDepth=-1,group=DEFAULT],resource=org.apache.spark.ui.JettyUtils$$anon$1-303294ce@8f44cfe7==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true] to PathMappings[size=1]
22:57:14.550 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - filterNameMap={org.apache.spark.ui.HttpSecurityFilter-984e61c=org.apache.spark.ui.HttpSecurityFilter-984e61c@984e61c==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true}
22:57:14.554 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - pathFilters=[[/*]/[]/[INCLUDE, REQUEST, ASYNC, FORWARD, ERROR]=>org.apache.spark.ui.HttpSecurityFilter-984e61c]
22:57:14.555 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletFilterMap={}
22:57:14.555 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletPathMap=PathMappings[size=1]
22:57:14.555 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$1-303294ce=org.apache.spark.ui.JettyUtils$$anon$1-303294ce@8f44cfe7==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true}
22:57:14.555 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting ServletHandler@197508f{STARTING}
22:57:14.555 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11328ms ServletHandler@197508f{STARTED}
22:57:14.559 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting org.apache.spark.ui.HttpSecurityFilter-984e61c@984e61c==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true
22:57:14.559 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11332ms org.apache.spark.ui.HttpSecurityFilter-984e61c@984e61c==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true
22:57:14.559 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.FilterHolder - Filter.init org.apache.spark.ui.HttpSecurityFilter@2b2ab7b5
22:57:14.560 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting org.apache.spark.ui.JettyUtils$$anon$1-303294ce@8f44cfe7==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true
22:57:14.561 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11334ms org.apache.spark.ui.JettyUtils$$anon$1-303294ce@8f44cfe7==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true
22:57:14.562 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHolder - Servlet.init org.apache.spark.ui.JettyUtils$$anon$1@5852edf8 for org.apache.spark.ui.JettyUtils$$anon$1-303294ce
22:57:14.562 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5d2d691a{/jobs,null,AVAILABLE,@Spark}
22:57:14.562 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11335ms o.s.j.s.ServletContextHandler@5d2d691a{/jobs,null,AVAILABLE,@Spark}
22:57:14.562 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting GzipHandler@3ae6a9f6{STOPPED}
22:57:14.563 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting GzipHandler@3ae6a9f6{STARTING}
22:57:14.563 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11336ms GzipHandler@3ae6a9f6{STARTED}
22:57:14.564 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.http.pathmap.PathMappings - Added MappedResource[pathSpec=ServletPathSpec["*.svgz",pathDepth=0,group=SUFFIX_GLOB],resource=true] to PathMappings[size=1]
22:57:14.564 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.gzip.GzipHandler - GzipHandler@2c462871{STOPPED} mime types IncludeExclude@2d4c742e{i=[],ip=org.sparkproject.jetty.util.IncludeExcludeSet$SetContainsPredicate@62f6b8c0,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=org.sparkproject.jetty.util.IncludeExcludeSet$SetContainsPredicate@49f0bc27}
22:57:14.564 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - GzipHandler@2c462871{STOPPED} added {o.s.j.s.ServletContextHandler@6b382dd6{/jobs/json,null,UNAVAILABLE,@Spark},MANAGED}
22:57:14.565 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/json->[{GzipHandler@2c462871{STOPPED},[o.s.j.s.ServletContextHandler@6b382dd6{/jobs/json,null,UNAVAILABLE,@Spark}]}]
22:57:14.565 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs->[{GzipHandler@3ae6a9f6{STARTED},[o.s.j.s.ServletContextHandler@5d2d691a{/jobs,null,AVAILABLE,@Spark}]}]
22:57:14.565 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - ContextHandlerCollection@7d170ea6{STARTED} added {GzipHandler@2c462871{STOPPED},UNMANAGED}
22:57:14.565 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting o.s.j.s.ServletContextHandler@6b382dd6{/jobs/json,null,UNAVAILABLE,@Spark}
22:57:14.565 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting o.s.j.s.ServletContextHandler@6b382dd6{/jobs/json,null,STARTING,@Spark}
22:57:14.565 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting ServletHandler@6dcff13a{STOPPED}
22:57:14.566 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$1-34e82299[EMBEDDED:null]
22:57:14.566 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.http.pathmap.PathMappings - Added MappedResource[pathSpec=ServletPathSpec["/",pathDepth=-1,group=DEFAULT],resource=org.apache.spark.ui.JettyUtils$$anon$1-34e82299@b8815da2==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true] to PathMappings[size=1]
22:57:14.566 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - filterNameMap={org.apache.spark.ui.HttpSecurityFilter-505d50c=org.apache.spark.ui.HttpSecurityFilter-505d50c@505d50c==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true}
22:57:14.566 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - pathFilters=[[/*]/[]/[INCLUDE, REQUEST, ASYNC, FORWARD, ERROR]=>org.apache.spark.ui.HttpSecurityFilter-505d50c]
22:57:14.566 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletFilterMap={}
22:57:14.566 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletPathMap=PathMappings[size=1]
22:57:14.566 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$1-34e82299=org.apache.spark.ui.JettyUtils$$anon$1-34e82299@b8815da2==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true}
22:57:14.566 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting ServletHandler@6dcff13a{STARTING}
22:57:14.566 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11339ms ServletHandler@6dcff13a{STARTED}
22:57:14.566 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting org.apache.spark.ui.HttpSecurityFilter-505d50c@505d50c==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true
22:57:14.567 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11340ms org.apache.spark.ui.HttpSecurityFilter-505d50c@505d50c==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true
22:57:14.567 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.FilterHolder - Filter.init org.apache.spark.ui.HttpSecurityFilter@546810c8
22:57:14.568 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting org.apache.spark.ui.JettyUtils$$anon$1-34e82299@b8815da2==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true
22:57:14.568 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11341ms org.apache.spark.ui.JettyUtils$$anon$1-34e82299@b8815da2==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true
22:57:14.568 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHolder - Servlet.init org.apache.spark.ui.JettyUtils$$anon$1@1acbe36f for org.apache.spark.ui.JettyUtils$$anon$1-34e82299
22:57:14.568 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6b382dd6{/jobs/json,null,AVAILABLE,@Spark}
22:57:14.568 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11341ms o.s.j.s.ServletContextHandler@6b382dd6{/jobs/json,null,AVAILABLE,@Spark}
22:57:14.568 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting GzipHandler@2c462871{STOPPED}
22:57:14.568 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting GzipHandler@2c462871{STARTING}
22:57:14.568 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11341ms GzipHandler@2c462871{STARTED}
22:57:14.569 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.http.pathmap.PathMappings - Added MappedResource[pathSpec=ServletPathSpec["*.svgz",pathDepth=0,group=SUFFIX_GLOB],resource=true] to PathMappings[size=1]
22:57:14.570 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.gzip.GzipHandler - GzipHandler@171f458{STOPPED} mime types IncludeExclude@150b4aee{i=[],ip=org.sparkproject.jetty.util.IncludeExcludeSet$SetContainsPredicate@501e13bb,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=org.sparkproject.jetty.util.IncludeExcludeSet$SetContainsPredicate@7ede7ffe}
22:57:14.570 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - GzipHandler@171f458{STOPPED} added {o.s.j.s.ServletContextHandler@1fdad46b{/jobs/job,null,UNAVAILABLE,@Spark},MANAGED}
22:57:14.570 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/json->[{GzipHandler@2c462871{STARTED},[o.s.j.s.ServletContextHandler@6b382dd6{/jobs/json,null,AVAILABLE,@Spark}]}]
22:57:14.570 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/job->[{GzipHandler@171f458{STOPPED},[o.s.j.s.ServletContextHandler@1fdad46b{/jobs/job,null,UNAVAILABLE,@Spark}]}]
22:57:14.570 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs->[{GzipHandler@3ae6a9f6{STARTED},[o.s.j.s.ServletContextHandler@5d2d691a{/jobs,null,AVAILABLE,@Spark}]}]
22:57:14.570 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - ContextHandlerCollection@7d170ea6{STARTED} added {GzipHandler@171f458{STOPPED},UNMANAGED}
22:57:14.570 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting o.s.j.s.ServletContextHandler@1fdad46b{/jobs/job,null,UNAVAILABLE,@Spark}
22:57:14.570 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting o.s.j.s.ServletContextHandler@1fdad46b{/jobs/job,null,STARTING,@Spark}
22:57:14.570 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting ServletHandler@7f2aba89{STOPPED}
22:57:14.570 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$1-6a972089[EMBEDDED:null]
22:57:14.570 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.http.pathmap.PathMappings - Added MappedResource[pathSpec=ServletPathSpec["/",pathDepth=-1,group=DEFAULT],resource=org.apache.spark.ui.JettyUtils$$anon$1-6a972089@f179f436==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true] to PathMappings[size=1]
22:57:14.570 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - filterNameMap={org.apache.spark.ui.HttpSecurityFilter-4dac784e=org.apache.spark.ui.HttpSecurityFilter-4dac784e@4dac784e==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true}
22:57:14.570 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - pathFilters=[[/*]/[]/[INCLUDE, REQUEST, ASYNC, FORWARD, ERROR]=>org.apache.spark.ui.HttpSecurityFilter-4dac784e]
22:57:14.570 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletFilterMap={}
22:57:14.570 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletPathMap=PathMappings[size=1]
22:57:14.571 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$1-6a972089=org.apache.spark.ui.JettyUtils$$anon$1-6a972089@f179f436==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true}
22:57:14.573 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting ServletHandler@7f2aba89{STARTING}
22:57:14.574 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11346ms ServletHandler@7f2aba89{STARTED}
22:57:14.574 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting org.apache.spark.ui.HttpSecurityFilter-4dac784e@4dac784e==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true
22:57:14.574 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11347ms org.apache.spark.ui.HttpSecurityFilter-4dac784e@4dac784e==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true
22:57:14.574 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.FilterHolder - Filter.init org.apache.spark.ui.HttpSecurityFilter@70838749
22:57:14.575 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting org.apache.spark.ui.JettyUtils$$anon$1-6a972089@f179f436==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true
22:57:14.575 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11348ms org.apache.spark.ui.JettyUtils$$anon$1-6a972089@f179f436==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true
22:57:14.575 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHolder - Servlet.init org.apache.spark.ui.JettyUtils$$anon$1@3f10ef8d for org.apache.spark.ui.JettyUtils$$anon$1-6a972089
22:57:14.575 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1fdad46b{/jobs/job,null,AVAILABLE,@Spark}
22:57:14.575 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11348ms o.s.j.s.ServletContextHandler@1fdad46b{/jobs/job,null,AVAILABLE,@Spark}
22:57:14.575 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting GzipHandler@171f458{STOPPED}
22:57:14.575 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting GzipHandler@171f458{STARTING}
22:57:14.575 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11348ms GzipHandler@171f458{STARTED}
22:57:14.578 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.http.pathmap.PathMappings - Added MappedResource[pathSpec=ServletPathSpec["*.svgz",pathDepth=0,group=SUFFIX_GLOB],resource=true] to PathMappings[size=1]
22:57:14.578 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.gzip.GzipHandler - GzipHandler@469e8d9e{STOPPED} mime types IncludeExclude@44af7e1a{i=[],ip=org.sparkproject.jetty.util.IncludeExcludeSet$SetContainsPredicate@2eb6c85a,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=org.sparkproject.jetty.util.IncludeExcludeSet$SetContainsPredicate@31b4cc0}
22:57:14.578 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - GzipHandler@469e8d9e{STOPPED} added {o.s.j.s.ServletContextHandler@56d07c59{/jobs/job/json,null,UNAVAILABLE,@Spark},MANAGED}
22:57:14.578 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/json->[{GzipHandler@2c462871{STARTED},[o.s.j.s.ServletContextHandler@6b382dd6{/jobs/json,null,AVAILABLE,@Spark}]}]
22:57:14.578 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/job->[{GzipHandler@171f458{STARTED},[o.s.j.s.ServletContextHandler@1fdad46b{/jobs/job,null,AVAILABLE,@Spark}]}]
22:57:14.578 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/job/json->[{GzipHandler@469e8d9e{STOPPED},[o.s.j.s.ServletContextHandler@56d07c59{/jobs/job/json,null,UNAVAILABLE,@Spark}]}]
22:57:14.579 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs->[{GzipHandler@3ae6a9f6{STARTED},[o.s.j.s.ServletContextHandler@5d2d691a{/jobs,null,AVAILABLE,@Spark}]}]
22:57:14.580 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - ContextHandlerCollection@7d170ea6{STARTED} added {GzipHandler@469e8d9e{STOPPED},UNMANAGED}
22:57:14.580 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting o.s.j.s.ServletContextHandler@56d07c59{/jobs/job/json,null,UNAVAILABLE,@Spark}
22:57:14.580 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting o.s.j.s.ServletContextHandler@56d07c59{/jobs/job/json,null,STARTING,@Spark}
22:57:14.580 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting ServletHandler@7c046af8{STOPPED}
22:57:14.580 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$1-2f448e99[EMBEDDED:null]
22:57:14.581 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.http.pathmap.PathMappings - Added MappedResource[pathSpec=ServletPathSpec["/",pathDepth=-1,group=DEFAULT],resource=org.apache.spark.ui.JettyUtils$$anon$1-2f448e99@51c568af==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true] to PathMappings[size=1]
22:57:14.581 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - filterNameMap={org.apache.spark.ui.HttpSecurityFilter-1f33b01f=org.apache.spark.ui.HttpSecurityFilter-1f33b01f@1f33b01f==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true}
22:57:14.581 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - pathFilters=[[/*]/[]/[INCLUDE, REQUEST, ASYNC, FORWARD, ERROR]=>org.apache.spark.ui.HttpSecurityFilter-1f33b01f]
22:57:14.581 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletFilterMap={}
22:57:14.581 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletPathMap=PathMappings[size=1]
22:57:14.581 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$1-2f448e99=org.apache.spark.ui.JettyUtils$$anon$1-2f448e99@51c568af==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true}
22:57:14.581 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting ServletHandler@7c046af8{STARTING}
22:57:14.581 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11354ms ServletHandler@7c046af8{STARTED}
22:57:14.581 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting org.apache.spark.ui.HttpSecurityFilter-1f33b01f@1f33b01f==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true
22:57:14.581 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11354ms org.apache.spark.ui.HttpSecurityFilter-1f33b01f@1f33b01f==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true
22:57:14.581 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.FilterHolder - Filter.init org.apache.spark.ui.HttpSecurityFilter@315bc340
22:57:14.581 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting org.apache.spark.ui.JettyUtils$$anon$1-2f448e99@51c568af==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true
22:57:14.581 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11355ms org.apache.spark.ui.JettyUtils$$anon$1-2f448e99@51c568af==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true
22:57:14.581 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHolder - Servlet.init org.apache.spark.ui.JettyUtils$$anon$1@ebb24df for org.apache.spark.ui.JettyUtils$$anon$1-2f448e99
22:57:14.582 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@56d07c59{/jobs/job/json,null,AVAILABLE,@Spark}
22:57:14.582 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11355ms o.s.j.s.ServletContextHandler@56d07c59{/jobs/job/json,null,AVAILABLE,@Spark}
22:57:14.582 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting GzipHandler@469e8d9e{STOPPED}
22:57:14.582 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting GzipHandler@469e8d9e{STARTING}
22:57:14.582 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11355ms GzipHandler@469e8d9e{STARTED}
22:57:14.589 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.http.pathmap.PathMappings - Added MappedResource[pathSpec=ServletPathSpec["*.svgz",pathDepth=0,group=SUFFIX_GLOB],resource=true] to PathMappings[size=1]
22:57:14.591 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.gzip.GzipHandler - GzipHandler@4f73ea1c{STOPPED} mime types IncludeExclude@205d6171{i=[],ip=org.sparkproject.jetty.util.IncludeExcludeSet$SetContainsPredicate@77172479,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=org.sparkproject.jetty.util.IncludeExcludeSet$SetContainsPredicate@5fa23e48}
22:57:14.591 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - GzipHandler@4f73ea1c{STOPPED} added {o.s.j.s.ServletContextHandler@5e077920{/stages,null,UNAVAILABLE,@Spark},MANAGED}
22:57:14.591 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/json->[{GzipHandler@2c462871{STARTED},[o.s.j.s.ServletContextHandler@6b382dd6{/jobs/json,null,AVAILABLE,@Spark}]}]
22:57:14.591 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/job->[{GzipHandler@171f458{STARTED},[o.s.j.s.ServletContextHandler@1fdad46b{/jobs/job,null,AVAILABLE,@Spark}]}]
22:57:14.591 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/job/json->[{GzipHandler@469e8d9e{STARTED},[o.s.j.s.ServletContextHandler@56d07c59{/jobs/job/json,null,AVAILABLE,@Spark}]}]
22:57:14.591 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs->[{GzipHandler@3ae6a9f6{STARTED},[o.s.j.s.ServletContextHandler@5d2d691a{/jobs,null,AVAILABLE,@Spark}]}]
22:57:14.591 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages->[{GzipHandler@4f73ea1c{STOPPED},[o.s.j.s.ServletContextHandler@5e077920{/stages,null,UNAVAILABLE,@Spark}]}]
22:57:14.591 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - ContextHandlerCollection@7d170ea6{STARTED} added {GzipHandler@4f73ea1c{STOPPED},UNMANAGED}
22:57:14.591 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting o.s.j.s.ServletContextHandler@5e077920{/stages,null,UNAVAILABLE,@Spark}
22:57:14.592 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting o.s.j.s.ServletContextHandler@5e077920{/stages,null,STARTING,@Spark}
22:57:14.592 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting ServletHandler@55ea2e38{STOPPED}
22:57:14.592 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$1-31451dc8[EMBEDDED:null]
22:57:14.592 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.http.pathmap.PathMappings - Added MappedResource[pathSpec=ServletPathSpec["/",pathDepth=-1,group=DEFAULT],resource=org.apache.spark.ui.JettyUtils$$anon$1-31451dc8@c6073b55==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true] to PathMappings[size=1]
22:57:14.592 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - filterNameMap={org.apache.spark.ui.HttpSecurityFilter-16d7c94a=org.apache.spark.ui.HttpSecurityFilter-16d7c94a@16d7c94a==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true}
22:57:14.592 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - pathFilters=[[/*]/[]/[INCLUDE, REQUEST, ASYNC, FORWARD, ERROR]=>org.apache.spark.ui.HttpSecurityFilter-16d7c94a]
22:57:14.592 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletFilterMap={}
22:57:14.592 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletPathMap=PathMappings[size=1]
22:57:14.592 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$1-31451dc8=org.apache.spark.ui.JettyUtils$$anon$1-31451dc8@c6073b55==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true}
22:57:14.592 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting ServletHandler@55ea2e38{STARTING}
22:57:14.592 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11365ms ServletHandler@55ea2e38{STARTED}
22:57:14.592 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting org.apache.spark.ui.HttpSecurityFilter-16d7c94a@16d7c94a==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true
22:57:14.592 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11365ms org.apache.spark.ui.HttpSecurityFilter-16d7c94a@16d7c94a==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true
22:57:14.592 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.FilterHolder - Filter.init org.apache.spark.ui.HttpSecurityFilter@f510caa
22:57:14.592 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting org.apache.spark.ui.JettyUtils$$anon$1-31451dc8@c6073b55==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true
22:57:14.592 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11365ms org.apache.spark.ui.JettyUtils$$anon$1-31451dc8@c6073b55==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true
22:57:14.592 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHolder - Servlet.init org.apache.spark.ui.JettyUtils$$anon$1@44ccb2d8 for org.apache.spark.ui.JettyUtils$$anon$1-31451dc8
22:57:14.592 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5e077920{/stages,null,AVAILABLE,@Spark}
22:57:14.592 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11365ms o.s.j.s.ServletContextHandler@5e077920{/stages,null,AVAILABLE,@Spark}
22:57:14.593 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting GzipHandler@4f73ea1c{STOPPED}
22:57:14.593 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting GzipHandler@4f73ea1c{STARTING}
22:57:14.593 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11366ms GzipHandler@4f73ea1c{STARTED}
22:57:14.594 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.http.pathmap.PathMappings - Added MappedResource[pathSpec=ServletPathSpec["*.svgz",pathDepth=0,group=SUFFIX_GLOB],resource=true] to PathMappings[size=1]
22:57:14.594 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.gzip.GzipHandler - GzipHandler@44284db4{STOPPED} mime types IncludeExclude@439b2dc7{i=[],ip=org.sparkproject.jetty.util.IncludeExcludeSet$SetContainsPredicate@62601d1e,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=org.sparkproject.jetty.util.IncludeExcludeSet$SetContainsPredicate@5b79e354}
22:57:14.595 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - GzipHandler@44284db4{STOPPED} added {o.s.j.s.ServletContextHandler@edcc350{/stages/json,null,UNAVAILABLE,@Spark},MANAGED}
22:57:14.595 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/json->[{GzipHandler@2c462871{STARTED},[o.s.j.s.ServletContextHandler@6b382dd6{/jobs/json,null,AVAILABLE,@Spark}]}]
22:57:14.595 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/job->[{GzipHandler@171f458{STARTED},[o.s.j.s.ServletContextHandler@1fdad46b{/jobs/job,null,AVAILABLE,@Spark}]}]
22:57:14.595 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/job/json->[{GzipHandler@469e8d9e{STARTED},[o.s.j.s.ServletContextHandler@56d07c59{/jobs/job/json,null,AVAILABLE,@Spark}]}]
22:57:14.595 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/json->[{GzipHandler@44284db4{STOPPED},[o.s.j.s.ServletContextHandler@edcc350{/stages/json,null,UNAVAILABLE,@Spark}]}]
22:57:14.595 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs->[{GzipHandler@3ae6a9f6{STARTED},[o.s.j.s.ServletContextHandler@5d2d691a{/jobs,null,AVAILABLE,@Spark}]}]
22:57:14.595 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages->[{GzipHandler@4f73ea1c{STARTED},[o.s.j.s.ServletContextHandler@5e077920{/stages,null,AVAILABLE,@Spark}]}]
22:57:14.595 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - ContextHandlerCollection@7d170ea6{STARTED} added {GzipHandler@44284db4{STOPPED},UNMANAGED}
22:57:14.595 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting o.s.j.s.ServletContextHandler@edcc350{/stages/json,null,UNAVAILABLE,@Spark}
22:57:14.595 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting o.s.j.s.ServletContextHandler@edcc350{/stages/json,null,STARTING,@Spark}
22:57:14.595 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting ServletHandler@ac3a75d{STOPPED}
22:57:14.595 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$1-13f15090[EMBEDDED:null]
22:57:14.595 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.http.pathmap.PathMappings - Added MappedResource[pathSpec=ServletPathSpec["/",pathDepth=-1,group=DEFAULT],resource=org.apache.spark.ui.JettyUtils$$anon$1-13f15090@b52c524d==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true] to PathMappings[size=1]
22:57:14.595 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - filterNameMap={org.apache.spark.ui.HttpSecurityFilter-ab672d4=org.apache.spark.ui.HttpSecurityFilter-ab672d4@ab672d4==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true}
22:57:14.596 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - pathFilters=[[/*]/[]/[INCLUDE, REQUEST, ASYNC, FORWARD, ERROR]=>org.apache.spark.ui.HttpSecurityFilter-ab672d4]
22:57:14.596 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletFilterMap={}
22:57:14.596 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletPathMap=PathMappings[size=1]
22:57:14.596 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$1-13f15090=org.apache.spark.ui.JettyUtils$$anon$1-13f15090@b52c524d==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true}
22:57:14.596 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting ServletHandler@ac3a75d{STARTING}
22:57:14.596 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11369ms ServletHandler@ac3a75d{STARTED}
22:57:14.596 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting org.apache.spark.ui.HttpSecurityFilter-ab672d4@ab672d4==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true
22:57:14.596 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11369ms org.apache.spark.ui.HttpSecurityFilter-ab672d4@ab672d4==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true
22:57:14.596 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.FilterHolder - Filter.init org.apache.spark.ui.HttpSecurityFilter@5d3a4062
22:57:14.596 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting org.apache.spark.ui.JettyUtils$$anon$1-13f15090@b52c524d==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true
22:57:14.596 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11369ms org.apache.spark.ui.JettyUtils$$anon$1-13f15090@b52c524d==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true
22:57:14.596 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHolder - Servlet.init org.apache.spark.ui.JettyUtils$$anon$1@47d5ab17 for org.apache.spark.ui.JettyUtils$$anon$1-13f15090
22:57:14.596 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@edcc350{/stages/json,null,AVAILABLE,@Spark}
22:57:14.596 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11369ms o.s.j.s.ServletContextHandler@edcc350{/stages/json,null,AVAILABLE,@Spark}
22:57:14.596 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting GzipHandler@44284db4{STOPPED}
22:57:14.596 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting GzipHandler@44284db4{STARTING}
22:57:14.596 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11369ms GzipHandler@44284db4{STARTED}
22:57:14.596 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.http.pathmap.PathMappings - Added MappedResource[pathSpec=ServletPathSpec["*.svgz",pathDepth=0,group=SUFFIX_GLOB],resource=true] to PathMappings[size=1]
22:57:14.596 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.gzip.GzipHandler - GzipHandler@31c8e2ae{STOPPED} mime types IncludeExclude@3e7e9da5{i=[],ip=org.sparkproject.jetty.util.IncludeExcludeSet$SetContainsPredicate@5419046e,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=org.sparkproject.jetty.util.IncludeExcludeSet$SetContainsPredicate@fcd27c5}
22:57:14.596 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - GzipHandler@31c8e2ae{STOPPED} added {o.s.j.s.ServletContextHandler@3cdd11cc{/stages/stage,null,UNAVAILABLE,@Spark},MANAGED}
22:57:14.597 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/json->[{GzipHandler@2c462871{STARTED},[o.s.j.s.ServletContextHandler@6b382dd6{/jobs/json,null,AVAILABLE,@Spark}]}]
22:57:14.597 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/job->[{GzipHandler@171f458{STARTED},[o.s.j.s.ServletContextHandler@1fdad46b{/jobs/job,null,AVAILABLE,@Spark}]}]
22:57:14.597 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/job/json->[{GzipHandler@469e8d9e{STARTED},[o.s.j.s.ServletContextHandler@56d07c59{/jobs/job/json,null,AVAILABLE,@Spark}]}]
22:57:14.597 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/json->[{GzipHandler@44284db4{STARTED},[o.s.j.s.ServletContextHandler@edcc350{/stages/json,null,AVAILABLE,@Spark}]}]
22:57:14.597 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs->[{GzipHandler@3ae6a9f6{STARTED},[o.s.j.s.ServletContextHandler@5d2d691a{/jobs,null,AVAILABLE,@Spark}]}]
22:57:14.598 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages->[{GzipHandler@4f73ea1c{STARTED},[o.s.j.s.ServletContextHandler@5e077920{/stages,null,AVAILABLE,@Spark}]}]
22:57:14.598 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/stage->[{GzipHandler@31c8e2ae{STOPPED},[o.s.j.s.ServletContextHandler@3cdd11cc{/stages/stage,null,UNAVAILABLE,@Spark}]}]
22:57:14.598 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - ContextHandlerCollection@7d170ea6{STARTED} added {GzipHandler@31c8e2ae{STOPPED},UNMANAGED}
22:57:14.599 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting o.s.j.s.ServletContextHandler@3cdd11cc{/stages/stage,null,UNAVAILABLE,@Spark}
22:57:14.599 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting o.s.j.s.ServletContextHandler@3cdd11cc{/stages/stage,null,STARTING,@Spark}
22:57:14.599 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting ServletHandler@25b1a317{STOPPED}
22:57:14.599 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$1-f78bb96[EMBEDDED:null]
22:57:14.599 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.http.pathmap.PathMappings - Added MappedResource[pathSpec=ServletPathSpec["/",pathDepth=-1,group=DEFAULT],resource=org.apache.spark.ui.JettyUtils$$anon$1-f78bb96@272ed436==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true] to PathMappings[size=1]
22:57:14.599 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - filterNameMap={org.apache.spark.ui.HttpSecurityFilter-253fc61e=org.apache.spark.ui.HttpSecurityFilter-253fc61e@253fc61e==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true}
22:57:14.599 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - pathFilters=[[/*]/[]/[INCLUDE, REQUEST, ASYNC, FORWARD, ERROR]=>org.apache.spark.ui.HttpSecurityFilter-253fc61e]
22:57:14.599 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletFilterMap={}
22:57:14.599 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletPathMap=PathMappings[size=1]
22:57:14.599 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$1-f78bb96=org.apache.spark.ui.JettyUtils$$anon$1-f78bb96@272ed436==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true}
22:57:14.599 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting ServletHandler@25b1a317{STARTING}
22:57:14.599 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11372ms ServletHandler@25b1a317{STARTED}
22:57:14.599 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting org.apache.spark.ui.HttpSecurityFilter-253fc61e@253fc61e==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true
22:57:14.599 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11372ms org.apache.spark.ui.HttpSecurityFilter-253fc61e@253fc61e==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true
22:57:14.599 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.FilterHolder - Filter.init org.apache.spark.ui.HttpSecurityFilter@79e66900
22:57:14.599 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting org.apache.spark.ui.JettyUtils$$anon$1-f78bb96@272ed436==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true
22:57:14.599 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11372ms org.apache.spark.ui.JettyUtils$$anon$1-f78bb96@272ed436==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true
22:57:14.599 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHolder - Servlet.init org.apache.spark.ui.JettyUtils$$anon$1@33bf502c for org.apache.spark.ui.JettyUtils$$anon$1-f78bb96
22:57:14.599 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3cdd11cc{/stages/stage,null,AVAILABLE,@Spark}
22:57:14.599 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11372ms o.s.j.s.ServletContextHandler@3cdd11cc{/stages/stage,null,AVAILABLE,@Spark}
22:57:14.599 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting GzipHandler@31c8e2ae{STOPPED}
22:57:14.599 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting GzipHandler@31c8e2ae{STARTING}
22:57:14.599 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11372ms GzipHandler@31c8e2ae{STARTED}
22:57:14.600 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.http.pathmap.PathMappings - Added MappedResource[pathSpec=ServletPathSpec["*.svgz",pathDepth=0,group=SUFFIX_GLOB],resource=true] to PathMappings[size=1]
22:57:14.600 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.gzip.GzipHandler - GzipHandler@30a40be{STOPPED} mime types IncludeExclude@41d9ee40{i=[],ip=org.sparkproject.jetty.util.IncludeExcludeSet$SetContainsPredicate@727fca03,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=org.sparkproject.jetty.util.IncludeExcludeSet$SetContainsPredicate@44700664}
22:57:14.600 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - GzipHandler@30a40be{STOPPED} added {o.s.j.s.ServletContextHandler@6def4b33{/stages/stage/json,null,UNAVAILABLE,@Spark},MANAGED}
22:57:14.600 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/json->[{GzipHandler@2c462871{STARTED},[o.s.j.s.ServletContextHandler@6b382dd6{/jobs/json,null,AVAILABLE,@Spark}]}]
22:57:14.600 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/job->[{GzipHandler@171f458{STARTED},[o.s.j.s.ServletContextHandler@1fdad46b{/jobs/job,null,AVAILABLE,@Spark}]}]
22:57:14.601 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/stage/json->[{GzipHandler@30a40be{STOPPED},[o.s.j.s.ServletContextHandler@6def4b33{/stages/stage/json,null,UNAVAILABLE,@Spark}]}]
22:57:14.601 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/job/json->[{GzipHandler@469e8d9e{STARTED},[o.s.j.s.ServletContextHandler@56d07c59{/jobs/job/json,null,AVAILABLE,@Spark}]}]
22:57:14.601 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/json->[{GzipHandler@44284db4{STARTED},[o.s.j.s.ServletContextHandler@edcc350{/stages/json,null,AVAILABLE,@Spark}]}]
22:57:14.601 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs->[{GzipHandler@3ae6a9f6{STARTED},[o.s.j.s.ServletContextHandler@5d2d691a{/jobs,null,AVAILABLE,@Spark}]}]
22:57:14.601 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages->[{GzipHandler@4f73ea1c{STARTED},[o.s.j.s.ServletContextHandler@5e077920{/stages,null,AVAILABLE,@Spark}]}]
22:57:14.601 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/stage->[{GzipHandler@31c8e2ae{STARTED},[o.s.j.s.ServletContextHandler@3cdd11cc{/stages/stage,null,AVAILABLE,@Spark}]}]
22:57:14.601 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - ContextHandlerCollection@7d170ea6{STARTED} added {GzipHandler@30a40be{STOPPED},UNMANAGED}
22:57:14.601 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting o.s.j.s.ServletContextHandler@6def4b33{/stages/stage/json,null,UNAVAILABLE,@Spark}
22:57:14.601 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting o.s.j.s.ServletContextHandler@6def4b33{/stages/stage/json,null,STARTING,@Spark}
22:57:14.601 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting ServletHandler@456ee482{STOPPED}
22:57:14.601 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$1-f8053b2[EMBEDDED:null]
22:57:14.601 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.http.pathmap.PathMappings - Added MappedResource[pathSpec=ServletPathSpec["/",pathDepth=-1,group=DEFAULT],resource=org.apache.spark.ui.JettyUtils$$anon$1-f8053b2@285dd09e==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true] to PathMappings[size=1]
22:57:14.601 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - filterNameMap={org.apache.spark.ui.HttpSecurityFilter-2f991841=org.apache.spark.ui.HttpSecurityFilter-2f991841@2f991841==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true}
22:57:14.601 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - pathFilters=[[/*]/[]/[INCLUDE, REQUEST, ASYNC, FORWARD, ERROR]=>org.apache.spark.ui.HttpSecurityFilter-2f991841]
22:57:14.601 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletFilterMap={}
22:57:14.601 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletPathMap=PathMappings[size=1]
22:57:14.601 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$1-f8053b2=org.apache.spark.ui.JettyUtils$$anon$1-f8053b2@285dd09e==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true}
22:57:14.601 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting ServletHandler@456ee482{STARTING}
22:57:14.601 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11374ms ServletHandler@456ee482{STARTED}
22:57:14.601 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting org.apache.spark.ui.HttpSecurityFilter-2f991841@2f991841==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true
22:57:14.602 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11375ms org.apache.spark.ui.HttpSecurityFilter-2f991841@2f991841==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true
22:57:14.602 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.FilterHolder - Filter.init org.apache.spark.ui.HttpSecurityFilter@15aeb1e4
22:57:14.602 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting org.apache.spark.ui.JettyUtils$$anon$1-f8053b2@285dd09e==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true
22:57:14.602 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11375ms org.apache.spark.ui.JettyUtils$$anon$1-f8053b2@285dd09e==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true
22:57:14.602 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHolder - Servlet.init org.apache.spark.ui.JettyUtils$$anon$1@1b8be6ff for org.apache.spark.ui.JettyUtils$$anon$1-f8053b2
22:57:14.603 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6def4b33{/stages/stage/json,null,AVAILABLE,@Spark}
22:57:14.604 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11377ms o.s.j.s.ServletContextHandler@6def4b33{/stages/stage/json,null,AVAILABLE,@Spark}
22:57:14.604 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting GzipHandler@30a40be{STOPPED}
22:57:14.606 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting GzipHandler@30a40be{STARTING}
22:57:14.607 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11380ms GzipHandler@30a40be{STARTED}
22:57:14.607 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.http.pathmap.PathMappings - Added MappedResource[pathSpec=ServletPathSpec["*.svgz",pathDepth=0,group=SUFFIX_GLOB],resource=true] to PathMappings[size=1]
22:57:14.607 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.gzip.GzipHandler - GzipHandler@532f7fe8{STOPPED} mime types IncludeExclude@33eed5f8{i=[],ip=org.sparkproject.jetty.util.IncludeExcludeSet$SetContainsPredicate@77167164,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=org.sparkproject.jetty.util.IncludeExcludeSet$SetContainsPredicate@202bf539}
22:57:14.608 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - GzipHandler@532f7fe8{STOPPED} added {o.s.j.s.ServletContextHandler@50be7065{/stages/pool,null,UNAVAILABLE,@Spark},MANAGED}
22:57:14.610 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/json->[{GzipHandler@2c462871{STARTED},[o.s.j.s.ServletContextHandler@6b382dd6{/jobs/json,null,AVAILABLE,@Spark}]}]
22:57:14.610 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/job->[{GzipHandler@171f458{STARTED},[o.s.j.s.ServletContextHandler@1fdad46b{/jobs/job,null,AVAILABLE,@Spark}]}]
22:57:14.610 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/stage/json->[{GzipHandler@30a40be{STARTED},[o.s.j.s.ServletContextHandler@6def4b33{/stages/stage/json,null,AVAILABLE,@Spark}]}]
22:57:14.610 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/job/json->[{GzipHandler@469e8d9e{STARTED},[o.s.j.s.ServletContextHandler@56d07c59{/jobs/job/json,null,AVAILABLE,@Spark}]}]
22:57:14.610 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/json->[{GzipHandler@44284db4{STARTED},[o.s.j.s.ServletContextHandler@edcc350{/stages/json,null,AVAILABLE,@Spark}]}]
22:57:14.610 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs->[{GzipHandler@3ae6a9f6{STARTED},[o.s.j.s.ServletContextHandler@5d2d691a{/jobs,null,AVAILABLE,@Spark}]}]
22:57:14.610 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages->[{GzipHandler@4f73ea1c{STARTED},[o.s.j.s.ServletContextHandler@5e077920{/stages,null,AVAILABLE,@Spark}]}]
22:57:14.610 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/stage->[{GzipHandler@31c8e2ae{STARTED},[o.s.j.s.ServletContextHandler@3cdd11cc{/stages/stage,null,AVAILABLE,@Spark}]}]
22:57:14.610 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/pool->[{GzipHandler@532f7fe8{STOPPED},[o.s.j.s.ServletContextHandler@50be7065{/stages/pool,null,UNAVAILABLE,@Spark}]}]
22:57:14.610 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - ContextHandlerCollection@7d170ea6{STARTED} added {GzipHandler@532f7fe8{STOPPED},UNMANAGED}
22:57:14.610 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting o.s.j.s.ServletContextHandler@50be7065{/stages/pool,null,UNAVAILABLE,@Spark}
22:57:14.612 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting o.s.j.s.ServletContextHandler@50be7065{/stages/pool,null,STARTING,@Spark}
22:57:14.612 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting ServletHandler@5d2ea403{STOPPED}
22:57:14.612 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$1-15c911b0[EMBEDDED:null]
22:57:14.612 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.http.pathmap.PathMappings - Added MappedResource[pathSpec=ServletPathSpec["/",pathDepth=-1,group=DEFAULT],resource=org.apache.spark.ui.JettyUtils$$anon$1-15c911b0@1a493236==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true] to PathMappings[size=1]
22:57:14.612 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - filterNameMap={org.apache.spark.ui.HttpSecurityFilter-17a8185f=org.apache.spark.ui.HttpSecurityFilter-17a8185f@17a8185f==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true}
22:57:14.612 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - pathFilters=[[/*]/[]/[INCLUDE, REQUEST, ASYNC, FORWARD, ERROR]=>org.apache.spark.ui.HttpSecurityFilter-17a8185f]
22:57:14.612 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletFilterMap={}
22:57:14.612 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletPathMap=PathMappings[size=1]
22:57:14.612 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$1-15c911b0=org.apache.spark.ui.JettyUtils$$anon$1-15c911b0@1a493236==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true}
22:57:14.612 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting ServletHandler@5d2ea403{STARTING}
22:57:14.612 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11385ms ServletHandler@5d2ea403{STARTED}
22:57:14.612 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting org.apache.spark.ui.HttpSecurityFilter-17a8185f@17a8185f==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true
22:57:14.612 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11386ms org.apache.spark.ui.HttpSecurityFilter-17a8185f@17a8185f==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true
22:57:14.613 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.FilterHolder - Filter.init org.apache.spark.ui.HttpSecurityFilter@8531eeb
22:57:14.613 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting org.apache.spark.ui.JettyUtils$$anon$1-15c911b0@1a493236==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true
22:57:14.613 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11386ms org.apache.spark.ui.JettyUtils$$anon$1-15c911b0@1a493236==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true
22:57:14.613 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHolder - Servlet.init org.apache.spark.ui.JettyUtils$$anon$1@4d575b0c for org.apache.spark.ui.JettyUtils$$anon$1-15c911b0
22:57:14.613 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@50be7065{/stages/pool,null,AVAILABLE,@Spark}
22:57:14.613 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11386ms o.s.j.s.ServletContextHandler@50be7065{/stages/pool,null,AVAILABLE,@Spark}
22:57:14.613 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting GzipHandler@532f7fe8{STOPPED}
22:57:14.613 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting GzipHandler@532f7fe8{STARTING}
22:57:14.613 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11386ms GzipHandler@532f7fe8{STARTED}
22:57:14.614 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.http.pathmap.PathMappings - Added MappedResource[pathSpec=ServletPathSpec["*.svgz",pathDepth=0,group=SUFFIX_GLOB],resource=true] to PathMappings[size=1]
22:57:14.614 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.gzip.GzipHandler - GzipHandler@76f330b6{STOPPED} mime types IncludeExclude@6647dcc5{i=[],ip=org.sparkproject.jetty.util.IncludeExcludeSet$SetContainsPredicate@11bf643a,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=org.sparkproject.jetty.util.IncludeExcludeSet$SetContainsPredicate@6cb80a88}
22:57:14.615 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - GzipHandler@76f330b6{STOPPED} added {o.s.j.s.ServletContextHandler@6257b05c{/stages/pool/json,null,UNAVAILABLE,@Spark},MANAGED}
22:57:14.615 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/json->[{GzipHandler@2c462871{STARTED},[o.s.j.s.ServletContextHandler@6b382dd6{/jobs/json,null,AVAILABLE,@Spark}]}]
22:57:14.615 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/job->[{GzipHandler@171f458{STARTED},[o.s.j.s.ServletContextHandler@1fdad46b{/jobs/job,null,AVAILABLE,@Spark}]}]
22:57:14.615 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/stage/json->[{GzipHandler@30a40be{STARTED},[o.s.j.s.ServletContextHandler@6def4b33{/stages/stage/json,null,AVAILABLE,@Spark}]}]
22:57:14.615 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/job/json->[{GzipHandler@469e8d9e{STARTED},[o.s.j.s.ServletContextHandler@56d07c59{/jobs/job/json,null,AVAILABLE,@Spark}]}]
22:57:14.615 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/json->[{GzipHandler@44284db4{STARTED},[o.s.j.s.ServletContextHandler@edcc350{/stages/json,null,AVAILABLE,@Spark}]}]
22:57:14.615 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs->[{GzipHandler@3ae6a9f6{STARTED},[o.s.j.s.ServletContextHandler@5d2d691a{/jobs,null,AVAILABLE,@Spark}]}]
22:57:14.615 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages->[{GzipHandler@4f73ea1c{STARTED},[o.s.j.s.ServletContextHandler@5e077920{/stages,null,AVAILABLE,@Spark}]}]
22:57:14.615 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/stage->[{GzipHandler@31c8e2ae{STARTED},[o.s.j.s.ServletContextHandler@3cdd11cc{/stages/stage,null,AVAILABLE,@Spark}]}]
22:57:14.615 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/pool/json->[{GzipHandler@76f330b6{STOPPED},[o.s.j.s.ServletContextHandler@6257b05c{/stages/pool/json,null,UNAVAILABLE,@Spark}]}]
22:57:14.615 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/pool->[{GzipHandler@532f7fe8{STARTED},[o.s.j.s.ServletContextHandler@50be7065{/stages/pool,null,AVAILABLE,@Spark}]}]
22:57:14.615 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - ContextHandlerCollection@7d170ea6{STARTED} added {GzipHandler@76f330b6{STOPPED},UNMANAGED}
22:57:14.615 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting o.s.j.s.ServletContextHandler@6257b05c{/stages/pool/json,null,UNAVAILABLE,@Spark}
22:57:14.615 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting o.s.j.s.ServletContextHandler@6257b05c{/stages/pool/json,null,STARTING,@Spark}
22:57:14.615 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting ServletHandler@4af11ea9{STOPPED}
22:57:14.616 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$1-1768c2d6[EMBEDDED:null]
22:57:14.616 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.http.pathmap.PathMappings - Added MappedResource[pathSpec=ServletPathSpec["/",pathDepth=-1,group=DEFAULT],resource=org.apache.spark.ui.JettyUtils$$anon$1-1768c2d6@37543bd7==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true] to PathMappings[size=1]
22:57:14.616 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - filterNameMap={org.apache.spark.ui.HttpSecurityFilter-90e8e93=org.apache.spark.ui.HttpSecurityFilter-90e8e93@90e8e93==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true}
22:57:14.616 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - pathFilters=[[/*]/[]/[INCLUDE, REQUEST, ASYNC, FORWARD, ERROR]=>org.apache.spark.ui.HttpSecurityFilter-90e8e93]
22:57:14.616 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletFilterMap={}
22:57:14.616 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletPathMap=PathMappings[size=1]
22:57:14.616 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$1-1768c2d6=org.apache.spark.ui.JettyUtils$$anon$1-1768c2d6@37543bd7==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true}
22:57:14.616 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting ServletHandler@4af11ea9{STARTING}
22:57:14.616 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11389ms ServletHandler@4af11ea9{STARTED}
22:57:14.616 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting org.apache.spark.ui.HttpSecurityFilter-90e8e93@90e8e93==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true
22:57:14.616 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11389ms org.apache.spark.ui.HttpSecurityFilter-90e8e93@90e8e93==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true
22:57:14.616 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.FilterHolder - Filter.init org.apache.spark.ui.HttpSecurityFilter@321f129
22:57:14.616 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting org.apache.spark.ui.JettyUtils$$anon$1-1768c2d6@37543bd7==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true
22:57:14.616 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11389ms org.apache.spark.ui.JettyUtils$$anon$1-1768c2d6@37543bd7==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true
22:57:14.616 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHolder - Servlet.init org.apache.spark.ui.JettyUtils$$anon$1@5bd8b47c for org.apache.spark.ui.JettyUtils$$anon$1-1768c2d6
22:57:14.616 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6257b05c{/stages/pool/json,null,AVAILABLE,@Spark}
22:57:14.617 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11390ms o.s.j.s.ServletContextHandler@6257b05c{/stages/pool/json,null,AVAILABLE,@Spark}
22:57:14.617 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting GzipHandler@76f330b6{STOPPED}
22:57:14.617 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting GzipHandler@76f330b6{STARTING}
22:57:14.617 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11390ms GzipHandler@76f330b6{STARTED}
22:57:14.617 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.http.pathmap.PathMappings - Added MappedResource[pathSpec=ServletPathSpec["*.svgz",pathDepth=0,group=SUFFIX_GLOB],resource=true] to PathMappings[size=1]
22:57:14.617 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.gzip.GzipHandler - GzipHandler@1dec257e{STOPPED} mime types IncludeExclude@11c82753{i=[],ip=org.sparkproject.jetty.util.IncludeExcludeSet$SetContainsPredicate@fad0a42,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=org.sparkproject.jetty.util.IncludeExcludeSet$SetContainsPredicate@5a7846f3}
22:57:14.618 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - GzipHandler@1dec257e{STOPPED} added {o.s.j.s.ServletContextHandler@49d00b2a{/storage,null,UNAVAILABLE,@Spark},MANAGED}
22:57:14.618 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/json->[{GzipHandler@2c462871{STARTED},[o.s.j.s.ServletContextHandler@6b382dd6{/jobs/json,null,AVAILABLE,@Spark}]}]
22:57:14.618 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/job->[{GzipHandler@171f458{STARTED},[o.s.j.s.ServletContextHandler@1fdad46b{/jobs/job,null,AVAILABLE,@Spark}]}]
22:57:14.618 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/stage/json->[{GzipHandler@30a40be{STARTED},[o.s.j.s.ServletContextHandler@6def4b33{/stages/stage/json,null,AVAILABLE,@Spark}]}]
22:57:14.618 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/job/json->[{GzipHandler@469e8d9e{STARTED},[o.s.j.s.ServletContextHandler@56d07c59{/jobs/job/json,null,AVAILABLE,@Spark}]}]
22:57:14.618 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/json->[{GzipHandler@44284db4{STARTED},[o.s.j.s.ServletContextHandler@edcc350{/stages/json,null,AVAILABLE,@Spark}]}]
22:57:14.618 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs->[{GzipHandler@3ae6a9f6{STARTED},[o.s.j.s.ServletContextHandler@5d2d691a{/jobs,null,AVAILABLE,@Spark}]}]
22:57:14.618 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages->[{GzipHandler@4f73ea1c{STARTED},[o.s.j.s.ServletContextHandler@5e077920{/stages,null,AVAILABLE,@Spark}]}]
22:57:14.618 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/stage->[{GzipHandler@31c8e2ae{STARTED},[o.s.j.s.ServletContextHandler@3cdd11cc{/stages/stage,null,AVAILABLE,@Spark}]}]
22:57:14.618 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage->[{GzipHandler@1dec257e{STOPPED},[o.s.j.s.ServletContextHandler@49d00b2a{/storage,null,UNAVAILABLE,@Spark}]}]
22:57:14.618 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/pool/json->[{GzipHandler@76f330b6{STARTED},[o.s.j.s.ServletContextHandler@6257b05c{/stages/pool/json,null,AVAILABLE,@Spark}]}]
22:57:14.619 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/pool->[{GzipHandler@532f7fe8{STARTED},[o.s.j.s.ServletContextHandler@50be7065{/stages/pool,null,AVAILABLE,@Spark}]}]
22:57:14.619 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - ContextHandlerCollection@7d170ea6{STARTED} added {GzipHandler@1dec257e{STOPPED},UNMANAGED}
22:57:14.619 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting o.s.j.s.ServletContextHandler@49d00b2a{/storage,null,UNAVAILABLE,@Spark}
22:57:14.619 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting o.s.j.s.ServletContextHandler@49d00b2a{/storage,null,STARTING,@Spark}
22:57:14.619 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting ServletHandler@6b45f59d{STOPPED}
22:57:14.619 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$1-2cb1d77b[EMBEDDED:null]
22:57:14.619 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.http.pathmap.PathMappings - Added MappedResource[pathSpec=ServletPathSpec["/",pathDepth=-1,group=DEFAULT],resource=org.apache.spark.ui.JettyUtils$$anon$1-2cb1d77b@17ab08c==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true] to PathMappings[size=1]
22:57:14.619 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - filterNameMap={org.apache.spark.ui.HttpSecurityFilter-31ac8589=org.apache.spark.ui.HttpSecurityFilter-31ac8589@31ac8589==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true}
22:57:14.619 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - pathFilters=[[/*]/[]/[INCLUDE, REQUEST, ASYNC, FORWARD, ERROR]=>org.apache.spark.ui.HttpSecurityFilter-31ac8589]
22:57:14.619 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletFilterMap={}
22:57:14.619 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletPathMap=PathMappings[size=1]
22:57:14.619 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$1-2cb1d77b=org.apache.spark.ui.JettyUtils$$anon$1-2cb1d77b@17ab08c==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true}
22:57:14.619 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting ServletHandler@6b45f59d{STARTING}
22:57:14.619 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11392ms ServletHandler@6b45f59d{STARTED}
22:57:14.619 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting org.apache.spark.ui.HttpSecurityFilter-31ac8589@31ac8589==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true
22:57:14.619 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11392ms org.apache.spark.ui.HttpSecurityFilter-31ac8589@31ac8589==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true
22:57:14.619 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.FilterHolder - Filter.init org.apache.spark.ui.HttpSecurityFilter@26435f17
22:57:14.619 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting org.apache.spark.ui.JettyUtils$$anon$1-2cb1d77b@17ab08c==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true
22:57:14.619 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11392ms org.apache.spark.ui.JettyUtils$$anon$1-2cb1d77b@17ab08c==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true
22:57:14.619 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHolder - Servlet.init org.apache.spark.ui.JettyUtils$$anon$1@76e10633 for org.apache.spark.ui.JettyUtils$$anon$1-2cb1d77b
22:57:14.619 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@49d00b2a{/storage,null,AVAILABLE,@Spark}
22:57:14.619 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11392ms o.s.j.s.ServletContextHandler@49d00b2a{/storage,null,AVAILABLE,@Spark}
22:57:14.619 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting GzipHandler@1dec257e{STOPPED}
22:57:14.619 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting GzipHandler@1dec257e{STARTING}
22:57:14.619 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11393ms GzipHandler@1dec257e{STARTED}
22:57:14.621 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.http.pathmap.PathMappings - Added MappedResource[pathSpec=ServletPathSpec["*.svgz",pathDepth=0,group=SUFFIX_GLOB],resource=true] to PathMappings[size=1]
22:57:14.621 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.gzip.GzipHandler - GzipHandler@9ae7eb5{STOPPED} mime types IncludeExclude@35a92370{i=[],ip=org.sparkproject.jetty.util.IncludeExcludeSet$SetContainsPredicate@4b877b68,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=org.sparkproject.jetty.util.IncludeExcludeSet$SetContainsPredicate@154da3ea}
22:57:14.623 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - GzipHandler@9ae7eb5{STOPPED} added {o.s.j.s.ServletContextHandler@5288383e{/storage/json,null,UNAVAILABLE,@Spark},MANAGED}
22:57:14.624 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/json->[{GzipHandler@2c462871{STARTED},[o.s.j.s.ServletContextHandler@6b382dd6{/jobs/json,null,AVAILABLE,@Spark}]}]
22:57:14.624 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/job->[{GzipHandler@171f458{STARTED},[o.s.j.s.ServletContextHandler@1fdad46b{/jobs/job,null,AVAILABLE,@Spark}]}]
22:57:14.624 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/stage/json->[{GzipHandler@30a40be{STARTED},[o.s.j.s.ServletContextHandler@6def4b33{/stages/stage/json,null,AVAILABLE,@Spark}]}]
22:57:14.624 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/job/json->[{GzipHandler@469e8d9e{STARTED},[o.s.j.s.ServletContextHandler@56d07c59{/jobs/job/json,null,AVAILABLE,@Spark}]}]
22:57:14.624 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/json->[{GzipHandler@44284db4{STARTED},[o.s.j.s.ServletContextHandler@edcc350{/stages/json,null,AVAILABLE,@Spark}]}]
22:57:14.624 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs->[{GzipHandler@3ae6a9f6{STARTED},[o.s.j.s.ServletContextHandler@5d2d691a{/jobs,null,AVAILABLE,@Spark}]}]
22:57:14.624 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages->[{GzipHandler@4f73ea1c{STARTED},[o.s.j.s.ServletContextHandler@5e077920{/stages,null,AVAILABLE,@Spark}]}]
22:57:14.624 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/stage->[{GzipHandler@31c8e2ae{STARTED},[o.s.j.s.ServletContextHandler@3cdd11cc{/stages/stage,null,AVAILABLE,@Spark}]}]
22:57:14.624 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage/json->[{GzipHandler@9ae7eb5{STOPPED},[o.s.j.s.ServletContextHandler@5288383e{/storage/json,null,UNAVAILABLE,@Spark}]}]
22:57:14.625 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage->[{GzipHandler@1dec257e{STARTED},[o.s.j.s.ServletContextHandler@49d00b2a{/storage,null,AVAILABLE,@Spark}]}]
22:57:14.625 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/pool/json->[{GzipHandler@76f330b6{STARTED},[o.s.j.s.ServletContextHandler@6257b05c{/stages/pool/json,null,AVAILABLE,@Spark}]}]
22:57:14.625 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/pool->[{GzipHandler@532f7fe8{STARTED},[o.s.j.s.ServletContextHandler@50be7065{/stages/pool,null,AVAILABLE,@Spark}]}]
22:57:14.625 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - ContextHandlerCollection@7d170ea6{STARTED} added {GzipHandler@9ae7eb5{STOPPED},UNMANAGED}
22:57:14.625 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting o.s.j.s.ServletContextHandler@5288383e{/storage/json,null,UNAVAILABLE,@Spark}
22:57:14.625 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting o.s.j.s.ServletContextHandler@5288383e{/storage/json,null,STARTING,@Spark}
22:57:14.626 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting ServletHandler@16a5c6ef{STOPPED}
22:57:14.626 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$1-348dd14f[EMBEDDED:null]
22:57:14.630 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.http.pathmap.PathMappings - Added MappedResource[pathSpec=ServletPathSpec["/",pathDepth=-1,group=DEFAULT],resource=org.apache.spark.ui.JettyUtils$$anon$1-348dd14f@6e3a0a3a==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true] to PathMappings[size=1]
22:57:14.631 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - filterNameMap={org.apache.spark.ui.HttpSecurityFilter-1179fdb7=org.apache.spark.ui.HttpSecurityFilter-1179fdb7@1179fdb7==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true}
22:57:14.632 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - pathFilters=[[/*]/[]/[INCLUDE, REQUEST, ASYNC, FORWARD, ERROR]=>org.apache.spark.ui.HttpSecurityFilter-1179fdb7]
22:57:14.632 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletFilterMap={}
22:57:14.632 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletPathMap=PathMappings[size=1]
22:57:14.632 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$1-348dd14f=org.apache.spark.ui.JettyUtils$$anon$1-348dd14f@6e3a0a3a==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true}
22:57:14.632 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting ServletHandler@16a5c6ef{STARTING}
22:57:14.632 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11405ms ServletHandler@16a5c6ef{STARTED}
22:57:14.632 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting org.apache.spark.ui.HttpSecurityFilter-1179fdb7@1179fdb7==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true
22:57:14.632 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11406ms org.apache.spark.ui.HttpSecurityFilter-1179fdb7@1179fdb7==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true
22:57:14.632 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.FilterHolder - Filter.init org.apache.spark.ui.HttpSecurityFilter@6f6c2c20
22:57:14.633 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting org.apache.spark.ui.JettyUtils$$anon$1-348dd14f@6e3a0a3a==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true
22:57:14.633 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11406ms org.apache.spark.ui.JettyUtils$$anon$1-348dd14f@6e3a0a3a==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true
22:57:14.633 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHolder - Servlet.init org.apache.spark.ui.JettyUtils$$anon$1@1322301e for org.apache.spark.ui.JettyUtils$$anon$1-348dd14f
22:57:14.633 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5288383e{/storage/json,null,AVAILABLE,@Spark}
22:57:14.633 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11406ms o.s.j.s.ServletContextHandler@5288383e{/storage/json,null,AVAILABLE,@Spark}
22:57:14.633 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting GzipHandler@9ae7eb5{STOPPED}
22:57:14.633 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting GzipHandler@9ae7eb5{STARTING}
22:57:14.633 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11406ms GzipHandler@9ae7eb5{STARTED}
22:57:14.633 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.http.pathmap.PathMappings - Added MappedResource[pathSpec=ServletPathSpec["*.svgz",pathDepth=0,group=SUFFIX_GLOB],resource=true] to PathMappings[size=1]
22:57:14.636 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.gzip.GzipHandler - GzipHandler@1b24f851{STOPPED} mime types IncludeExclude@1527ca39{i=[],ip=org.sparkproject.jetty.util.IncludeExcludeSet$SetContainsPredicate@1707ba43,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=org.sparkproject.jetty.util.IncludeExcludeSet$SetContainsPredicate@6b19287}
22:57:14.639 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - GzipHandler@1b24f851{STOPPED} added {o.s.j.s.ServletContextHandler@4320206d{/storage/rdd,null,UNAVAILABLE,@Spark},MANAGED}
22:57:14.643 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/json->[{GzipHandler@2c462871{STARTED},[o.s.j.s.ServletContextHandler@6b382dd6{/jobs/json,null,AVAILABLE,@Spark}]}]
22:57:14.644 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/stage/json->[{GzipHandler@30a40be{STARTED},[o.s.j.s.ServletContextHandler@6def4b33{/stages/stage/json,null,AVAILABLE,@Spark}]}]
22:57:14.644 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage/rdd->[{GzipHandler@1b24f851{STOPPED},[o.s.j.s.ServletContextHandler@4320206d{/storage/rdd,null,UNAVAILABLE,@Spark}]}]
22:57:14.644 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/job/json->[{GzipHandler@469e8d9e{STARTED},[o.s.j.s.ServletContextHandler@56d07c59{/jobs/job/json,null,AVAILABLE,@Spark}]}]
22:57:14.644 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs->[{GzipHandler@3ae6a9f6{STARTED},[o.s.j.s.ServletContextHandler@5d2d691a{/jobs,null,AVAILABLE,@Spark}]}]
22:57:14.644 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/json->[{GzipHandler@44284db4{STARTED},[o.s.j.s.ServletContextHandler@edcc350{/stages/json,null,AVAILABLE,@Spark}]}]
22:57:14.644 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/stage->[{GzipHandler@31c8e2ae{STARTED},[o.s.j.s.ServletContextHandler@3cdd11cc{/stages/stage,null,AVAILABLE,@Spark}]}]
22:57:14.644 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage/json->[{GzipHandler@9ae7eb5{STARTED},[o.s.j.s.ServletContextHandler@5288383e{/storage/json,null,AVAILABLE,@Spark}]}]
22:57:14.644 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage->[{GzipHandler@1dec257e{STARTED},[o.s.j.s.ServletContextHandler@49d00b2a{/storage,null,AVAILABLE,@Spark}]}]
22:57:14.644 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/job->[{GzipHandler@171f458{STARTED},[o.s.j.s.ServletContextHandler@1fdad46b{/jobs/job,null,AVAILABLE,@Spark}]}]
22:57:14.644 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages->[{GzipHandler@4f73ea1c{STARTED},[o.s.j.s.ServletContextHandler@5e077920{/stages,null,AVAILABLE,@Spark}]}]
22:57:14.644 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/pool/json->[{GzipHandler@76f330b6{STARTED},[o.s.j.s.ServletContextHandler@6257b05c{/stages/pool/json,null,AVAILABLE,@Spark}]}]
22:57:14.644 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/pool->[{GzipHandler@532f7fe8{STARTED},[o.s.j.s.ServletContextHandler@50be7065{/stages/pool,null,AVAILABLE,@Spark}]}]
22:57:14.644 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - ContextHandlerCollection@7d170ea6{STARTED} added {GzipHandler@1b24f851{STOPPED},UNMANAGED}
22:57:14.644 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting o.s.j.s.ServletContextHandler@4320206d{/storage/rdd,null,UNAVAILABLE,@Spark}
22:57:14.644 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting o.s.j.s.ServletContextHandler@4320206d{/storage/rdd,null,STARTING,@Spark}
22:57:14.644 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting ServletHandler@19fd96ef{STOPPED}
22:57:14.645 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$1-753c3ff6[EMBEDDED:null]
22:57:14.645 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.http.pathmap.PathMappings - Added MappedResource[pathSpec=ServletPathSpec["/",pathDepth=-1,group=DEFAULT],resource=org.apache.spark.ui.JettyUtils$$anon$1-753c3ff6@39f92cff==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true] to PathMappings[size=1]
22:57:14.646 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - filterNameMap={org.apache.spark.ui.HttpSecurityFilter-630e4529=org.apache.spark.ui.HttpSecurityFilter-630e4529@630e4529==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true}
22:57:14.647 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - pathFilters=[[/*]/[]/[INCLUDE, REQUEST, ASYNC, FORWARD, ERROR]=>org.apache.spark.ui.HttpSecurityFilter-630e4529]
22:57:14.647 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletFilterMap={}
22:57:14.648 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletPathMap=PathMappings[size=1]
22:57:14.648 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$1-753c3ff6=org.apache.spark.ui.JettyUtils$$anon$1-753c3ff6@39f92cff==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true}
22:57:14.648 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting ServletHandler@19fd96ef{STARTING}
22:57:14.648 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11421ms ServletHandler@19fd96ef{STARTED}
22:57:14.648 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting org.apache.spark.ui.HttpSecurityFilter-630e4529@630e4529==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true
22:57:14.648 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11421ms org.apache.spark.ui.HttpSecurityFilter-630e4529@630e4529==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true
22:57:14.648 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.FilterHolder - Filter.init org.apache.spark.ui.HttpSecurityFilter@3aeb1470
22:57:14.648 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting org.apache.spark.ui.JettyUtils$$anon$1-753c3ff6@39f92cff==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true
22:57:14.648 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11421ms org.apache.spark.ui.JettyUtils$$anon$1-753c3ff6@39f92cff==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true
22:57:14.648 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHolder - Servlet.init org.apache.spark.ui.JettyUtils$$anon$1@11366716 for org.apache.spark.ui.JettyUtils$$anon$1-753c3ff6
22:57:14.648 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@4320206d{/storage/rdd,null,AVAILABLE,@Spark}
22:57:14.648 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11421ms o.s.j.s.ServletContextHandler@4320206d{/storage/rdd,null,AVAILABLE,@Spark}
22:57:14.648 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting GzipHandler@1b24f851{STOPPED}
22:57:14.648 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting GzipHandler@1b24f851{STARTING}
22:57:14.648 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11422ms GzipHandler@1b24f851{STARTED}
22:57:14.650 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.http.pathmap.PathMappings - Added MappedResource[pathSpec=ServletPathSpec["*.svgz",pathDepth=0,group=SUFFIX_GLOB],resource=true] to PathMappings[size=1]
22:57:14.650 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.gzip.GzipHandler - GzipHandler@31e69a85{STOPPED} mime types IncludeExclude@53cc505d{i=[],ip=org.sparkproject.jetty.util.IncludeExcludeSet$SetContainsPredicate@71e093e5,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=org.sparkproject.jetty.util.IncludeExcludeSet$SetContainsPredicate@2ccd882}
22:57:14.650 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - GzipHandler@31e69a85{STOPPED} added {o.s.j.s.ServletContextHandler@1ce6aa02{/storage/rdd/json,null,UNAVAILABLE,@Spark},MANAGED}
22:57:14.650 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/json->[{GzipHandler@2c462871{STARTED},[o.s.j.s.ServletContextHandler@6b382dd6{/jobs/json,null,AVAILABLE,@Spark}]}]
22:57:14.650 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/stage/json->[{GzipHandler@30a40be{STARTED},[o.s.j.s.ServletContextHandler@6def4b33{/stages/stage/json,null,AVAILABLE,@Spark}]}]
22:57:14.650 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage/rdd->[{GzipHandler@1b24f851{STARTED},[o.s.j.s.ServletContextHandler@4320206d{/storage/rdd,null,AVAILABLE,@Spark}]}]
22:57:14.650 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/job/json->[{GzipHandler@469e8d9e{STARTED},[o.s.j.s.ServletContextHandler@56d07c59{/jobs/job/json,null,AVAILABLE,@Spark}]}]
22:57:14.650 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs->[{GzipHandler@3ae6a9f6{STARTED},[o.s.j.s.ServletContextHandler@5d2d691a{/jobs,null,AVAILABLE,@Spark}]}]
22:57:14.650 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/json->[{GzipHandler@44284db4{STARTED},[o.s.j.s.ServletContextHandler@edcc350{/stages/json,null,AVAILABLE,@Spark}]}]
22:57:14.651 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/stage->[{GzipHandler@31c8e2ae{STARTED},[o.s.j.s.ServletContextHandler@3cdd11cc{/stages/stage,null,AVAILABLE,@Spark}]}]
22:57:14.651 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage/json->[{GzipHandler@9ae7eb5{STARTED},[o.s.j.s.ServletContextHandler@5288383e{/storage/json,null,AVAILABLE,@Spark}]}]
22:57:14.651 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage->[{GzipHandler@1dec257e{STARTED},[o.s.j.s.ServletContextHandler@49d00b2a{/storage,null,AVAILABLE,@Spark}]}]
22:57:14.651 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage/rdd/json->[{GzipHandler@31e69a85{STOPPED},[o.s.j.s.ServletContextHandler@1ce6aa02{/storage/rdd/json,null,UNAVAILABLE,@Spark}]}]
22:57:14.651 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/job->[{GzipHandler@171f458{STARTED},[o.s.j.s.ServletContextHandler@1fdad46b{/jobs/job,null,AVAILABLE,@Spark}]}]
22:57:14.651 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages->[{GzipHandler@4f73ea1c{STARTED},[o.s.j.s.ServletContextHandler@5e077920{/stages,null,AVAILABLE,@Spark}]}]
22:57:14.651 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/pool/json->[{GzipHandler@76f330b6{STARTED},[o.s.j.s.ServletContextHandler@6257b05c{/stages/pool/json,null,AVAILABLE,@Spark}]}]
22:57:14.651 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/pool->[{GzipHandler@532f7fe8{STARTED},[o.s.j.s.ServletContextHandler@50be7065{/stages/pool,null,AVAILABLE,@Spark}]}]
22:57:14.651 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - ContextHandlerCollection@7d170ea6{STARTED} added {GzipHandler@31e69a85{STOPPED},UNMANAGED}
22:57:14.651 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting o.s.j.s.ServletContextHandler@1ce6aa02{/storage/rdd/json,null,UNAVAILABLE,@Spark}
22:57:14.651 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting o.s.j.s.ServletContextHandler@1ce6aa02{/storage/rdd/json,null,STARTING,@Spark}
22:57:14.651 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting ServletHandler@69590d9f{STOPPED}
22:57:14.651 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$1-7d8e5ccd[EMBEDDED:null]
22:57:14.651 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.http.pathmap.PathMappings - Added MappedResource[pathSpec=ServletPathSpec["/",pathDepth=-1,group=DEFAULT],resource=org.apache.spark.ui.JettyUtils$$anon$1-7d8e5ccd@f8e35557==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true] to PathMappings[size=1]
22:57:14.651 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - filterNameMap={org.apache.spark.ui.HttpSecurityFilter-3bc902d1=org.apache.spark.ui.HttpSecurityFilter-3bc902d1@3bc902d1==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true}
22:57:14.651 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - pathFilters=[[/*]/[]/[INCLUDE, REQUEST, ASYNC, FORWARD, ERROR]=>org.apache.spark.ui.HttpSecurityFilter-3bc902d1]
22:57:14.651 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletFilterMap={}
22:57:14.651 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletPathMap=PathMappings[size=1]
22:57:14.651 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$1-7d8e5ccd=org.apache.spark.ui.JettyUtils$$anon$1-7d8e5ccd@f8e35557==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true}
22:57:14.651 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting ServletHandler@69590d9f{STARTING}
22:57:14.651 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11424ms ServletHandler@69590d9f{STARTED}
22:57:14.651 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting org.apache.spark.ui.HttpSecurityFilter-3bc902d1@3bc902d1==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true
22:57:14.651 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11424ms org.apache.spark.ui.HttpSecurityFilter-3bc902d1@3bc902d1==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true
22:57:14.651 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.FilterHolder - Filter.init org.apache.spark.ui.HttpSecurityFilter@77b58ec
22:57:14.651 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting org.apache.spark.ui.JettyUtils$$anon$1-7d8e5ccd@f8e35557==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true
22:57:14.651 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11425ms org.apache.spark.ui.JettyUtils$$anon$1-7d8e5ccd@f8e35557==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true
22:57:14.651 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHolder - Servlet.init org.apache.spark.ui.JettyUtils$$anon$1@3684bee6 for org.apache.spark.ui.JettyUtils$$anon$1-7d8e5ccd
22:57:14.652 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1ce6aa02{/storage/rdd/json,null,AVAILABLE,@Spark}
22:57:14.652 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11425ms o.s.j.s.ServletContextHandler@1ce6aa02{/storage/rdd/json,null,AVAILABLE,@Spark}
22:57:14.652 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting GzipHandler@31e69a85{STOPPED}
22:57:14.652 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting GzipHandler@31e69a85{STARTING}
22:57:14.652 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11425ms GzipHandler@31e69a85{STARTED}
22:57:14.652 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.http.pathmap.PathMappings - Added MappedResource[pathSpec=ServletPathSpec["*.svgz",pathDepth=0,group=SUFFIX_GLOB],resource=true] to PathMappings[size=1]
22:57:14.652 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.gzip.GzipHandler - GzipHandler@2762a443{STOPPED} mime types IncludeExclude@7a83347b{i=[],ip=org.sparkproject.jetty.util.IncludeExcludeSet$SetContainsPredicate@69e3d643,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=org.sparkproject.jetty.util.IncludeExcludeSet$SetContainsPredicate@59be1a2b}
22:57:14.652 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - GzipHandler@2762a443{STOPPED} added {o.s.j.s.ServletContextHandler@73adebf8{/environment,null,UNAVAILABLE,@Spark},MANAGED}
22:57:14.652 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/json->[{GzipHandler@2c462871{STARTED},[o.s.j.s.ServletContextHandler@6b382dd6{/jobs/json,null,AVAILABLE,@Spark}]}]
22:57:14.652 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/stage/json->[{GzipHandler@30a40be{STARTED},[o.s.j.s.ServletContextHandler@6def4b33{/stages/stage/json,null,AVAILABLE,@Spark}]}]
22:57:14.652 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage/rdd->[{GzipHandler@1b24f851{STARTED},[o.s.j.s.ServletContextHandler@4320206d{/storage/rdd,null,AVAILABLE,@Spark}]}]
22:57:14.652 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/job/json->[{GzipHandler@469e8d9e{STARTED},[o.s.j.s.ServletContextHandler@56d07c59{/jobs/job/json,null,AVAILABLE,@Spark}]}]
22:57:14.652 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs->[{GzipHandler@3ae6a9f6{STARTED},[o.s.j.s.ServletContextHandler@5d2d691a{/jobs,null,AVAILABLE,@Spark}]}]
22:57:14.652 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/json->[{GzipHandler@44284db4{STARTED},[o.s.j.s.ServletContextHandler@edcc350{/stages/json,null,AVAILABLE,@Spark}]}]
22:57:14.652 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/stage->[{GzipHandler@31c8e2ae{STARTED},[o.s.j.s.ServletContextHandler@3cdd11cc{/stages/stage,null,AVAILABLE,@Spark}]}]
22:57:14.652 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage/json->[{GzipHandler@9ae7eb5{STARTED},[o.s.j.s.ServletContextHandler@5288383e{/storage/json,null,AVAILABLE,@Spark}]}]
22:57:14.652 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage->[{GzipHandler@1dec257e{STARTED},[o.s.j.s.ServletContextHandler@49d00b2a{/storage,null,AVAILABLE,@Spark}]}]
22:57:14.652 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage/rdd/json->[{GzipHandler@31e69a85{STARTED},[o.s.j.s.ServletContextHandler@1ce6aa02{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
22:57:14.653 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/job->[{GzipHandler@171f458{STARTED},[o.s.j.s.ServletContextHandler@1fdad46b{/jobs/job,null,AVAILABLE,@Spark}]}]
22:57:14.653 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - environment->[{GzipHandler@2762a443{STOPPED},[o.s.j.s.ServletContextHandler@73adebf8{/environment,null,UNAVAILABLE,@Spark}]}]
22:57:14.653 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages->[{GzipHandler@4f73ea1c{STARTED},[o.s.j.s.ServletContextHandler@5e077920{/stages,null,AVAILABLE,@Spark}]}]
22:57:14.653 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/pool/json->[{GzipHandler@76f330b6{STARTED},[o.s.j.s.ServletContextHandler@6257b05c{/stages/pool/json,null,AVAILABLE,@Spark}]}]
22:57:14.653 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/pool->[{GzipHandler@532f7fe8{STARTED},[o.s.j.s.ServletContextHandler@50be7065{/stages/pool,null,AVAILABLE,@Spark}]}]
22:57:14.653 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - ContextHandlerCollection@7d170ea6{STARTED} added {GzipHandler@2762a443{STOPPED},UNMANAGED}
22:57:14.653 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting o.s.j.s.ServletContextHandler@73adebf8{/environment,null,UNAVAILABLE,@Spark}
22:57:14.653 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting o.s.j.s.ServletContextHandler@73adebf8{/environment,null,STARTING,@Spark}
22:57:14.653 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting ServletHandler@7e44cf94{STOPPED}
22:57:14.653 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$1-41b88290[EMBEDDED:null]
22:57:14.653 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.http.pathmap.PathMappings - Added MappedResource[pathSpec=ServletPathSpec["/",pathDepth=-1,group=DEFAULT],resource=org.apache.spark.ui.JettyUtils$$anon$1-41b88290@7c940912==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true] to PathMappings[size=1]
22:57:14.653 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - filterNameMap={org.apache.spark.ui.HttpSecurityFilter-430e7c68=org.apache.spark.ui.HttpSecurityFilter-430e7c68@430e7c68==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true}
22:57:14.653 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - pathFilters=[[/*]/[]/[INCLUDE, REQUEST, ASYNC, FORWARD, ERROR]=>org.apache.spark.ui.HttpSecurityFilter-430e7c68]
22:57:14.653 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletFilterMap={}
22:57:14.653 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletPathMap=PathMappings[size=1]
22:57:14.653 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$1-41b88290=org.apache.spark.ui.JettyUtils$$anon$1-41b88290@7c940912==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true}
22:57:14.653 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting ServletHandler@7e44cf94{STARTING}
22:57:14.653 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11426ms ServletHandler@7e44cf94{STARTED}
22:57:14.653 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting org.apache.spark.ui.HttpSecurityFilter-430e7c68@430e7c68==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true
22:57:14.653 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11426ms org.apache.spark.ui.HttpSecurityFilter-430e7c68@430e7c68==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true
22:57:14.653 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.FilterHolder - Filter.init org.apache.spark.ui.HttpSecurityFilter@6b4cede3
22:57:14.653 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting org.apache.spark.ui.JettyUtils$$anon$1-41b88290@7c940912==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true
22:57:14.653 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11426ms org.apache.spark.ui.JettyUtils$$anon$1-41b88290@7c940912==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true
22:57:14.653 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHolder - Servlet.init org.apache.spark.ui.JettyUtils$$anon$1@88f3c1d for org.apache.spark.ui.JettyUtils$$anon$1-41b88290
22:57:14.653 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@73adebf8{/environment,null,AVAILABLE,@Spark}
22:57:14.653 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11426ms o.s.j.s.ServletContextHandler@73adebf8{/environment,null,AVAILABLE,@Spark}
22:57:14.654 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting GzipHandler@2762a443{STOPPED}
22:57:14.654 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting GzipHandler@2762a443{STARTING}
22:57:14.654 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11427ms GzipHandler@2762a443{STARTED}
22:57:14.654 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.http.pathmap.PathMappings - Added MappedResource[pathSpec=ServletPathSpec["*.svgz",pathDepth=0,group=SUFFIX_GLOB],resource=true] to PathMappings[size=1]
22:57:14.654 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.gzip.GzipHandler - GzipHandler@56ef900d{STOPPED} mime types IncludeExclude@5a404cc6{i=[],ip=org.sparkproject.jetty.util.IncludeExcludeSet$SetContainsPredicate@2b5eec47,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=org.sparkproject.jetty.util.IncludeExcludeSet$SetContainsPredicate@19d16e07}
22:57:14.655 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - GzipHandler@56ef900d{STOPPED} added {o.s.j.s.ServletContextHandler@3501a276{/environment/json,null,UNAVAILABLE,@Spark},MANAGED}
22:57:14.655 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/json->[{GzipHandler@2c462871{STARTED},[o.s.j.s.ServletContextHandler@6b382dd6{/jobs/json,null,AVAILABLE,@Spark}]}]
22:57:14.655 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/stage/json->[{GzipHandler@30a40be{STARTED},[o.s.j.s.ServletContextHandler@6def4b33{/stages/stage/json,null,AVAILABLE,@Spark}]}]
22:57:14.655 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage/rdd->[{GzipHandler@1b24f851{STARTED},[o.s.j.s.ServletContextHandler@4320206d{/storage/rdd,null,AVAILABLE,@Spark}]}]
22:57:14.655 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - environment/json->[{GzipHandler@56ef900d{STOPPED},[o.s.j.s.ServletContextHandler@3501a276{/environment/json,null,UNAVAILABLE,@Spark}]}]
22:57:14.655 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/job/json->[{GzipHandler@469e8d9e{STARTED},[o.s.j.s.ServletContextHandler@56d07c59{/jobs/job/json,null,AVAILABLE,@Spark}]}]
22:57:14.655 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs->[{GzipHandler@3ae6a9f6{STARTED},[o.s.j.s.ServletContextHandler@5d2d691a{/jobs,null,AVAILABLE,@Spark}]}]
22:57:14.655 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/json->[{GzipHandler@44284db4{STARTED},[o.s.j.s.ServletContextHandler@edcc350{/stages/json,null,AVAILABLE,@Spark}]}]
22:57:14.655 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/stage->[{GzipHandler@31c8e2ae{STARTED},[o.s.j.s.ServletContextHandler@3cdd11cc{/stages/stage,null,AVAILABLE,@Spark}]}]
22:57:14.655 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage/json->[{GzipHandler@9ae7eb5{STARTED},[o.s.j.s.ServletContextHandler@5288383e{/storage/json,null,AVAILABLE,@Spark}]}]
22:57:14.655 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage->[{GzipHandler@1dec257e{STARTED},[o.s.j.s.ServletContextHandler@49d00b2a{/storage,null,AVAILABLE,@Spark}]}]
22:57:14.655 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage/rdd/json->[{GzipHandler@31e69a85{STARTED},[o.s.j.s.ServletContextHandler@1ce6aa02{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
22:57:14.655 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - environment->[{GzipHandler@2762a443{STARTED},[o.s.j.s.ServletContextHandler@73adebf8{/environment,null,AVAILABLE,@Spark}]}]
22:57:14.655 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/job->[{GzipHandler@171f458{STARTED},[o.s.j.s.ServletContextHandler@1fdad46b{/jobs/job,null,AVAILABLE,@Spark}]}]
22:57:14.655 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages->[{GzipHandler@4f73ea1c{STARTED},[o.s.j.s.ServletContextHandler@5e077920{/stages,null,AVAILABLE,@Spark}]}]
22:57:14.655 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/pool/json->[{GzipHandler@76f330b6{STARTED},[o.s.j.s.ServletContextHandler@6257b05c{/stages/pool/json,null,AVAILABLE,@Spark}]}]
22:57:14.655 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/pool->[{GzipHandler@532f7fe8{STARTED},[o.s.j.s.ServletContextHandler@50be7065{/stages/pool,null,AVAILABLE,@Spark}]}]
22:57:14.655 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - ContextHandlerCollection@7d170ea6{STARTED} added {GzipHandler@56ef900d{STOPPED},UNMANAGED}
22:57:14.656 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting o.s.j.s.ServletContextHandler@3501a276{/environment/json,null,UNAVAILABLE,@Spark}
22:57:14.656 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting o.s.j.s.ServletContextHandler@3501a276{/environment/json,null,STARTING,@Spark}
22:57:14.656 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting ServletHandler@6de1f32d{STOPPED}
22:57:14.656 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$1-68484003[EMBEDDED:null]
22:57:14.656 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.http.pathmap.PathMappings - Added MappedResource[pathSpec=ServletPathSpec["/",pathDepth=-1,group=DEFAULT],resource=org.apache.spark.ui.JettyUtils$$anon$1-68484003@70213af3==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true] to PathMappings[size=1]
22:57:14.656 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - filterNameMap={org.apache.spark.ui.HttpSecurityFilter-7f29a281=org.apache.spark.ui.HttpSecurityFilter-7f29a281@7f29a281==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true}
22:57:14.656 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - pathFilters=[[/*]/[]/[INCLUDE, REQUEST, ASYNC, FORWARD, ERROR]=>org.apache.spark.ui.HttpSecurityFilter-7f29a281]
22:57:14.656 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletFilterMap={}
22:57:14.656 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletPathMap=PathMappings[size=1]
22:57:14.656 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$1-68484003=org.apache.spark.ui.JettyUtils$$anon$1-68484003@70213af3==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true}
22:57:14.656 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting ServletHandler@6de1f32d{STARTING}
22:57:14.656 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11430ms ServletHandler@6de1f32d{STARTED}
22:57:14.657 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting org.apache.spark.ui.HttpSecurityFilter-7f29a281@7f29a281==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true
22:57:14.657 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11430ms org.apache.spark.ui.HttpSecurityFilter-7f29a281@7f29a281==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true
22:57:14.657 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.FilterHolder - Filter.init org.apache.spark.ui.HttpSecurityFilter@3394fac8
22:57:14.657 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting org.apache.spark.ui.JettyUtils$$anon$1-68484003@70213af3==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true
22:57:14.657 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11430ms org.apache.spark.ui.JettyUtils$$anon$1-68484003@70213af3==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true
22:57:14.657 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHolder - Servlet.init org.apache.spark.ui.JettyUtils$$anon$1@6b6aa600 for org.apache.spark.ui.JettyUtils$$anon$1-68484003
22:57:14.657 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@3501a276{/environment/json,null,AVAILABLE,@Spark}
22:57:14.657 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11430ms o.s.j.s.ServletContextHandler@3501a276{/environment/json,null,AVAILABLE,@Spark}
22:57:14.657 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting GzipHandler@56ef900d{STOPPED}
22:57:14.657 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting GzipHandler@56ef900d{STARTING}
22:57:14.657 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11430ms GzipHandler@56ef900d{STARTED}
22:57:14.658 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.http.pathmap.PathMappings - Added MappedResource[pathSpec=ServletPathSpec["*.svgz",pathDepth=0,group=SUFFIX_GLOB],resource=true] to PathMappings[size=1]
22:57:14.658 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.gzip.GzipHandler - GzipHandler@2539b24e{STOPPED} mime types IncludeExclude@31ef4e9b{i=[],ip=org.sparkproject.jetty.util.IncludeExcludeSet$SetContainsPredicate@f0ba400,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=org.sparkproject.jetty.util.IncludeExcludeSet$SetContainsPredicate@6a7b93bc}
22:57:14.658 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - GzipHandler@2539b24e{STOPPED} added {o.s.j.s.ServletContextHandler@328c8921{/executors,null,UNAVAILABLE,@Spark},MANAGED}
22:57:14.659 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/json->[{GzipHandler@2c462871{STARTED},[o.s.j.s.ServletContextHandler@6b382dd6{/jobs/json,null,AVAILABLE,@Spark}]}]
22:57:14.659 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/stage/json->[{GzipHandler@30a40be{STARTED},[o.s.j.s.ServletContextHandler@6def4b33{/stages/stage/json,null,AVAILABLE,@Spark}]}]
22:57:14.659 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage/rdd->[{GzipHandler@1b24f851{STARTED},[o.s.j.s.ServletContextHandler@4320206d{/storage/rdd,null,AVAILABLE,@Spark}]}]
22:57:14.659 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - environment/json->[{GzipHandler@56ef900d{STARTED},[o.s.j.s.ServletContextHandler@3501a276{/environment/json,null,AVAILABLE,@Spark}]}]
22:57:14.659 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/job/json->[{GzipHandler@469e8d9e{STARTED},[o.s.j.s.ServletContextHandler@56d07c59{/jobs/job/json,null,AVAILABLE,@Spark}]}]
22:57:14.659 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs->[{GzipHandler@3ae6a9f6{STARTED},[o.s.j.s.ServletContextHandler@5d2d691a{/jobs,null,AVAILABLE,@Spark}]}]
22:57:14.659 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/json->[{GzipHandler@44284db4{STARTED},[o.s.j.s.ServletContextHandler@edcc350{/stages/json,null,AVAILABLE,@Spark}]}]
22:57:14.659 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/stage->[{GzipHandler@31c8e2ae{STARTED},[o.s.j.s.ServletContextHandler@3cdd11cc{/stages/stage,null,AVAILABLE,@Spark}]}]
22:57:14.659 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage/json->[{GzipHandler@9ae7eb5{STARTED},[o.s.j.s.ServletContextHandler@5288383e{/storage/json,null,AVAILABLE,@Spark}]}]
22:57:14.660 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage->[{GzipHandler@1dec257e{STARTED},[o.s.j.s.ServletContextHandler@49d00b2a{/storage,null,AVAILABLE,@Spark}]}]
22:57:14.660 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage/rdd/json->[{GzipHandler@31e69a85{STARTED},[o.s.j.s.ServletContextHandler@1ce6aa02{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
22:57:14.660 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - environment->[{GzipHandler@2762a443{STARTED},[o.s.j.s.ServletContextHandler@73adebf8{/environment,null,AVAILABLE,@Spark}]}]
22:57:14.660 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/job->[{GzipHandler@171f458{STARTED},[o.s.j.s.ServletContextHandler@1fdad46b{/jobs/job,null,AVAILABLE,@Spark}]}]
22:57:14.660 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages->[{GzipHandler@4f73ea1c{STARTED},[o.s.j.s.ServletContextHandler@5e077920{/stages,null,AVAILABLE,@Spark}]}]
22:57:14.660 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - executors->[{GzipHandler@2539b24e{STOPPED},[o.s.j.s.ServletContextHandler@328c8921{/executors,null,UNAVAILABLE,@Spark}]}]
22:57:14.660 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/pool/json->[{GzipHandler@76f330b6{STARTED},[o.s.j.s.ServletContextHandler@6257b05c{/stages/pool/json,null,AVAILABLE,@Spark}]}]
22:57:14.660 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/pool->[{GzipHandler@532f7fe8{STARTED},[o.s.j.s.ServletContextHandler@50be7065{/stages/pool,null,AVAILABLE,@Spark}]}]
22:57:14.660 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - ContextHandlerCollection@7d170ea6{STARTED} added {GzipHandler@2539b24e{STOPPED},UNMANAGED}
22:57:14.660 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting o.s.j.s.ServletContextHandler@328c8921{/executors,null,UNAVAILABLE,@Spark}
22:57:14.660 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting o.s.j.s.ServletContextHandler@328c8921{/executors,null,STARTING,@Spark}
22:57:14.660 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting ServletHandler@4d623639{STOPPED}
22:57:14.660 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$1-472783d3[EMBEDDED:null]
22:57:14.660 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.http.pathmap.PathMappings - Added MappedResource[pathSpec=ServletPathSpec["/",pathDepth=-1,group=DEFAULT],resource=org.apache.spark.ui.JettyUtils$$anon$1-472783d3@6802c040==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true] to PathMappings[size=1]
22:57:14.660 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - filterNameMap={org.apache.spark.ui.HttpSecurityFilter-370a53fe=org.apache.spark.ui.HttpSecurityFilter-370a53fe@370a53fe==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true}
22:57:14.661 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - pathFilters=[[/*]/[]/[INCLUDE, REQUEST, ASYNC, FORWARD, ERROR]=>org.apache.spark.ui.HttpSecurityFilter-370a53fe]
22:57:14.661 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletFilterMap={}
22:57:14.661 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletPathMap=PathMappings[size=1]
22:57:14.661 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$1-472783d3=org.apache.spark.ui.JettyUtils$$anon$1-472783d3@6802c040==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true}
22:57:14.661 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting ServletHandler@4d623639{STARTING}
22:57:14.661 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11434ms ServletHandler@4d623639{STARTED}
22:57:14.661 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting org.apache.spark.ui.HttpSecurityFilter-370a53fe@370a53fe==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true
22:57:14.661 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11434ms org.apache.spark.ui.HttpSecurityFilter-370a53fe@370a53fe==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true
22:57:14.661 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.FilterHolder - Filter.init org.apache.spark.ui.HttpSecurityFilter@238e77f
22:57:14.661 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting org.apache.spark.ui.JettyUtils$$anon$1-472783d3@6802c040==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true
22:57:14.661 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11434ms org.apache.spark.ui.JettyUtils$$anon$1-472783d3@6802c040==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true
22:57:14.661 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHolder - Servlet.init org.apache.spark.ui.JettyUtils$$anon$1@4968ea35 for org.apache.spark.ui.JettyUtils$$anon$1-472783d3
22:57:14.662 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@328c8921{/executors,null,AVAILABLE,@Spark}
22:57:14.662 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11435ms o.s.j.s.ServletContextHandler@328c8921{/executors,null,AVAILABLE,@Spark}
22:57:14.662 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting GzipHandler@2539b24e{STOPPED}
22:57:14.662 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting GzipHandler@2539b24e{STARTING}
22:57:14.662 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11435ms GzipHandler@2539b24e{STARTED}
22:57:14.663 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.http.pathmap.PathMappings - Added MappedResource[pathSpec=ServletPathSpec["*.svgz",pathDepth=0,group=SUFFIX_GLOB],resource=true] to PathMappings[size=1]
22:57:14.663 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.gzip.GzipHandler - GzipHandler@2dc1fa11{STOPPED} mime types IncludeExclude@6807f3c9{i=[],ip=org.sparkproject.jetty.util.IncludeExcludeSet$SetContainsPredicate@66b085be,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=org.sparkproject.jetty.util.IncludeExcludeSet$SetContainsPredicate@7654f148}
22:57:14.668 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - GzipHandler@2dc1fa11{STOPPED} added {o.s.j.s.ServletContextHandler@5e87276a{/executors/json,null,UNAVAILABLE,@Spark},MANAGED}
22:57:14.672 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/json->[{GzipHandler@2c462871{STARTED},[o.s.j.s.ServletContextHandler@6b382dd6{/jobs/json,null,AVAILABLE,@Spark}]}]
22:57:14.672 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - executors/json->[{GzipHandler@2dc1fa11{STOPPED},[o.s.j.s.ServletContextHandler@5e87276a{/executors/json,null,UNAVAILABLE,@Spark}]}]
22:57:14.672 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/stage/json->[{GzipHandler@30a40be{STARTED},[o.s.j.s.ServletContextHandler@6def4b33{/stages/stage/json,null,AVAILABLE,@Spark}]}]
22:57:14.672 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage/rdd->[{GzipHandler@1b24f851{STARTED},[o.s.j.s.ServletContextHandler@4320206d{/storage/rdd,null,AVAILABLE,@Spark}]}]
22:57:14.672 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - environment/json->[{GzipHandler@56ef900d{STARTED},[o.s.j.s.ServletContextHandler@3501a276{/environment/json,null,AVAILABLE,@Spark}]}]
22:57:14.672 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/job/json->[{GzipHandler@469e8d9e{STARTED},[o.s.j.s.ServletContextHandler@56d07c59{/jobs/job/json,null,AVAILABLE,@Spark}]}]
22:57:14.672 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs->[{GzipHandler@3ae6a9f6{STARTED},[o.s.j.s.ServletContextHandler@5d2d691a{/jobs,null,AVAILABLE,@Spark}]}]
22:57:14.672 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/json->[{GzipHandler@44284db4{STARTED},[o.s.j.s.ServletContextHandler@edcc350{/stages/json,null,AVAILABLE,@Spark}]}]
22:57:14.672 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/stage->[{GzipHandler@31c8e2ae{STARTED},[o.s.j.s.ServletContextHandler@3cdd11cc{/stages/stage,null,AVAILABLE,@Spark}]}]
22:57:14.672 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage/json->[{GzipHandler@9ae7eb5{STARTED},[o.s.j.s.ServletContextHandler@5288383e{/storage/json,null,AVAILABLE,@Spark}]}]
22:57:14.672 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage->[{GzipHandler@1dec257e{STARTED},[o.s.j.s.ServletContextHandler@49d00b2a{/storage,null,AVAILABLE,@Spark}]}]
22:57:14.672 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage/rdd/json->[{GzipHandler@31e69a85{STARTED},[o.s.j.s.ServletContextHandler@1ce6aa02{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
22:57:14.673 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - environment->[{GzipHandler@2762a443{STARTED},[o.s.j.s.ServletContextHandler@73adebf8{/environment,null,AVAILABLE,@Spark}]}]
22:57:14.673 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/job->[{GzipHandler@171f458{STARTED},[o.s.j.s.ServletContextHandler@1fdad46b{/jobs/job,null,AVAILABLE,@Spark}]}]
22:57:14.673 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages->[{GzipHandler@4f73ea1c{STARTED},[o.s.j.s.ServletContextHandler@5e077920{/stages,null,AVAILABLE,@Spark}]}]
22:57:14.673 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - executors->[{GzipHandler@2539b24e{STARTED},[o.s.j.s.ServletContextHandler@328c8921{/executors,null,AVAILABLE,@Spark}]}]
22:57:14.673 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/pool/json->[{GzipHandler@76f330b6{STARTED},[o.s.j.s.ServletContextHandler@6257b05c{/stages/pool/json,null,AVAILABLE,@Spark}]}]
22:57:14.673 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/pool->[{GzipHandler@532f7fe8{STARTED},[o.s.j.s.ServletContextHandler@50be7065{/stages/pool,null,AVAILABLE,@Spark}]}]
22:57:14.673 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - ContextHandlerCollection@7d170ea6{STARTED} added {GzipHandler@2dc1fa11{STOPPED},UNMANAGED}
22:57:14.673 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting o.s.j.s.ServletContextHandler@5e87276a{/executors/json,null,UNAVAILABLE,@Spark}
22:57:14.673 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting o.s.j.s.ServletContextHandler@5e87276a{/executors/json,null,STARTING,@Spark}
22:57:14.673 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting ServletHandler@208117d{STOPPED}
22:57:14.673 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$1-794a23f4[EMBEDDED:null]
22:57:14.674 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.http.pathmap.PathMappings - Added MappedResource[pathSpec=ServletPathSpec["/",pathDepth=-1,group=DEFAULT],resource=org.apache.spark.ui.JettyUtils$$anon$1-794a23f4@f298fcc==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true] to PathMappings[size=1]
22:57:14.674 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - filterNameMap={org.apache.spark.ui.HttpSecurityFilter-1b117cbc=org.apache.spark.ui.HttpSecurityFilter-1b117cbc@1b117cbc==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true}
22:57:14.674 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - pathFilters=[[/*]/[]/[INCLUDE, REQUEST, ASYNC, FORWARD, ERROR]=>org.apache.spark.ui.HttpSecurityFilter-1b117cbc]
22:57:14.674 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletFilterMap={}
22:57:14.674 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletPathMap=PathMappings[size=1]
22:57:14.674 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$1-794a23f4=org.apache.spark.ui.JettyUtils$$anon$1-794a23f4@f298fcc==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true}
22:57:14.674 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting ServletHandler@208117d{STARTING}
22:57:14.674 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11447ms ServletHandler@208117d{STARTED}
22:57:14.674 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting org.apache.spark.ui.HttpSecurityFilter-1b117cbc@1b117cbc==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true
22:57:14.674 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11447ms org.apache.spark.ui.HttpSecurityFilter-1b117cbc@1b117cbc==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true
22:57:14.674 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.FilterHolder - Filter.init org.apache.spark.ui.HttpSecurityFilter@54e78ce1
22:57:14.674 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting org.apache.spark.ui.JettyUtils$$anon$1-794a23f4@f298fcc==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true
22:57:14.674 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11447ms org.apache.spark.ui.JettyUtils$$anon$1-794a23f4@f298fcc==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true
22:57:14.674 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHolder - Servlet.init org.apache.spark.ui.JettyUtils$$anon$1@329a40f for org.apache.spark.ui.JettyUtils$$anon$1-794a23f4
22:57:14.674 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5e87276a{/executors/json,null,AVAILABLE,@Spark}
22:57:14.674 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11447ms o.s.j.s.ServletContextHandler@5e87276a{/executors/json,null,AVAILABLE,@Spark}
22:57:14.674 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting GzipHandler@2dc1fa11{STOPPED}
22:57:14.674 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting GzipHandler@2dc1fa11{STARTING}
22:57:14.674 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11447ms GzipHandler@2dc1fa11{STARTED}
22:57:14.675 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.http.pathmap.PathMappings - Added MappedResource[pathSpec=ServletPathSpec["*.svgz",pathDepth=0,group=SUFFIX_GLOB],resource=true] to PathMappings[size=1]
22:57:14.675 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.gzip.GzipHandler - GzipHandler@30592f62{STOPPED} mime types IncludeExclude@58b12d0c{i=[],ip=org.sparkproject.jetty.util.IncludeExcludeSet$SetContainsPredicate@16f6f3c9,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=org.sparkproject.jetty.util.IncludeExcludeSet$SetContainsPredicate@459606f2}
22:57:14.675 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - GzipHandler@30592f62{STOPPED} added {o.s.j.s.ServletContextHandler@7779b1d5{/executors/threadDump,null,UNAVAILABLE,@Spark},MANAGED}
22:57:14.676 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/json->[{GzipHandler@2c462871{STARTED},[o.s.j.s.ServletContextHandler@6b382dd6{/jobs/json,null,AVAILABLE,@Spark}]}]
22:57:14.676 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - executors/json->[{GzipHandler@2dc1fa11{STARTED},[o.s.j.s.ServletContextHandler@5e87276a{/executors/json,null,AVAILABLE,@Spark}]}]
22:57:14.676 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/stage/json->[{GzipHandler@30a40be{STARTED},[o.s.j.s.ServletContextHandler@6def4b33{/stages/stage/json,null,AVAILABLE,@Spark}]}]
22:57:14.676 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage/rdd->[{GzipHandler@1b24f851{STARTED},[o.s.j.s.ServletContextHandler@4320206d{/storage/rdd,null,AVAILABLE,@Spark}]}]
22:57:14.676 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - environment/json->[{GzipHandler@56ef900d{STARTED},[o.s.j.s.ServletContextHandler@3501a276{/environment/json,null,AVAILABLE,@Spark}]}]
22:57:14.676 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/job/json->[{GzipHandler@469e8d9e{STARTED},[o.s.j.s.ServletContextHandler@56d07c59{/jobs/job/json,null,AVAILABLE,@Spark}]}]
22:57:14.676 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs->[{GzipHandler@3ae6a9f6{STARTED},[o.s.j.s.ServletContextHandler@5d2d691a{/jobs,null,AVAILABLE,@Spark}]}]
22:57:14.676 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/json->[{GzipHandler@44284db4{STARTED},[o.s.j.s.ServletContextHandler@edcc350{/stages/json,null,AVAILABLE,@Spark}]}]
22:57:14.676 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/stage->[{GzipHandler@31c8e2ae{STARTED},[o.s.j.s.ServletContextHandler@3cdd11cc{/stages/stage,null,AVAILABLE,@Spark}]}]
22:57:14.676 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage/json->[{GzipHandler@9ae7eb5{STARTED},[o.s.j.s.ServletContextHandler@5288383e{/storage/json,null,AVAILABLE,@Spark}]}]
22:57:14.676 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage->[{GzipHandler@1dec257e{STARTED},[o.s.j.s.ServletContextHandler@49d00b2a{/storage,null,AVAILABLE,@Spark}]}]
22:57:14.676 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage/rdd/json->[{GzipHandler@31e69a85{STARTED},[o.s.j.s.ServletContextHandler@1ce6aa02{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
22:57:14.676 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - environment->[{GzipHandler@2762a443{STARTED},[o.s.j.s.ServletContextHandler@73adebf8{/environment,null,AVAILABLE,@Spark}]}]
22:57:14.676 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/job->[{GzipHandler@171f458{STARTED},[o.s.j.s.ServletContextHandler@1fdad46b{/jobs/job,null,AVAILABLE,@Spark}]}]
22:57:14.676 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages->[{GzipHandler@4f73ea1c{STARTED},[o.s.j.s.ServletContextHandler@5e077920{/stages,null,AVAILABLE,@Spark}]}]
22:57:14.676 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - executors->[{GzipHandler@2539b24e{STARTED},[o.s.j.s.ServletContextHandler@328c8921{/executors,null,AVAILABLE,@Spark}]}]
22:57:14.676 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/pool/json->[{GzipHandler@76f330b6{STARTED},[o.s.j.s.ServletContextHandler@6257b05c{/stages/pool/json,null,AVAILABLE,@Spark}]}]
22:57:14.676 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/pool->[{GzipHandler@532f7fe8{STARTED},[o.s.j.s.ServletContextHandler@50be7065{/stages/pool,null,AVAILABLE,@Spark}]}]
22:57:14.676 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - executors/threadDump->[{GzipHandler@30592f62{STOPPED},[o.s.j.s.ServletContextHandler@7779b1d5{/executors/threadDump,null,UNAVAILABLE,@Spark}]}]
22:57:14.677 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - ContextHandlerCollection@7d170ea6{STARTED} added {GzipHandler@30592f62{STOPPED},UNMANAGED}
22:57:14.677 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting o.s.j.s.ServletContextHandler@7779b1d5{/executors/threadDump,null,UNAVAILABLE,@Spark}
22:57:14.677 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting o.s.j.s.ServletContextHandler@7779b1d5{/executors/threadDump,null,STARTING,@Spark}
22:57:14.677 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting ServletHandler@5f84694d{STOPPED}
22:57:14.677 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$1-1548d03e[EMBEDDED:null]
22:57:14.677 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.http.pathmap.PathMappings - Added MappedResource[pathSpec=ServletPathSpec["/",pathDepth=-1,group=DEFAULT],resource=org.apache.spark.ui.JettyUtils$$anon$1-1548d03e@ca1e7b34==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true] to PathMappings[size=1]
22:57:14.677 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - filterNameMap={org.apache.spark.ui.HttpSecurityFilter-61d64ca1=org.apache.spark.ui.HttpSecurityFilter-61d64ca1@61d64ca1==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true}
22:57:14.677 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - pathFilters=[[/*]/[]/[INCLUDE, REQUEST, ASYNC, FORWARD, ERROR]=>org.apache.spark.ui.HttpSecurityFilter-61d64ca1]
22:57:14.677 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletFilterMap={}
22:57:14.677 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletPathMap=PathMappings[size=1]
22:57:14.677 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$1-1548d03e=org.apache.spark.ui.JettyUtils$$anon$1-1548d03e@ca1e7b34==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true}
22:57:14.677 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting ServletHandler@5f84694d{STARTING}
22:57:14.677 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11450ms ServletHandler@5f84694d{STARTED}
22:57:14.677 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting org.apache.spark.ui.HttpSecurityFilter-61d64ca1@61d64ca1==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true
22:57:14.677 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11450ms org.apache.spark.ui.HttpSecurityFilter-61d64ca1@61d64ca1==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true
22:57:14.677 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.FilterHolder - Filter.init org.apache.spark.ui.HttpSecurityFilter@3c4d031d
22:57:14.677 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting org.apache.spark.ui.JettyUtils$$anon$1-1548d03e@ca1e7b34==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true
22:57:14.677 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11450ms org.apache.spark.ui.JettyUtils$$anon$1-1548d03e@ca1e7b34==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true
22:57:14.677 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHolder - Servlet.init org.apache.spark.ui.JettyUtils$$anon$1@6d4590d5 for org.apache.spark.ui.JettyUtils$$anon$1-1548d03e
22:57:14.677 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7779b1d5{/executors/threadDump,null,AVAILABLE,@Spark}
22:57:14.678 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11451ms o.s.j.s.ServletContextHandler@7779b1d5{/executors/threadDump,null,AVAILABLE,@Spark}
22:57:14.680 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting GzipHandler@30592f62{STOPPED}
22:57:14.681 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting GzipHandler@30592f62{STARTING}
22:57:14.681 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11454ms GzipHandler@30592f62{STARTED}
22:57:14.683 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.http.pathmap.PathMappings - Added MappedResource[pathSpec=ServletPathSpec["*.svgz",pathDepth=0,group=SUFFIX_GLOB],resource=true] to PathMappings[size=1]
22:57:14.685 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.gzip.GzipHandler - GzipHandler@395860fa{STOPPED} mime types IncludeExclude@6d59d927{i=[],ip=org.sparkproject.jetty.util.IncludeExcludeSet$SetContainsPredicate@2c5b2f61,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=org.sparkproject.jetty.util.IncludeExcludeSet$SetContainsPredicate@20960c3}
22:57:14.685 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - GzipHandler@395860fa{STOPPED} added {o.s.j.s.ServletContextHandler@5ca18a1d{/executors/threadDump/json,null,UNAVAILABLE,@Spark},MANAGED}
22:57:14.685 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/json->[{GzipHandler@2c462871{STARTED},[o.s.j.s.ServletContextHandler@6b382dd6{/jobs/json,null,AVAILABLE,@Spark}]}]
22:57:14.685 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - executors/json->[{GzipHandler@2dc1fa11{STARTED},[o.s.j.s.ServletContextHandler@5e87276a{/executors/json,null,AVAILABLE,@Spark}]}]
22:57:14.685 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/stage/json->[{GzipHandler@30a40be{STARTED},[o.s.j.s.ServletContextHandler@6def4b33{/stages/stage/json,null,AVAILABLE,@Spark}]}]
22:57:14.685 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage/rdd->[{GzipHandler@1b24f851{STARTED},[o.s.j.s.ServletContextHandler@4320206d{/storage/rdd,null,AVAILABLE,@Spark}]}]
22:57:14.686 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - executors/threadDump/json->[{GzipHandler@395860fa{STOPPED},[o.s.j.s.ServletContextHandler@5ca18a1d{/executors/threadDump/json,null,UNAVAILABLE,@Spark}]}]
22:57:14.686 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - environment/json->[{GzipHandler@56ef900d{STARTED},[o.s.j.s.ServletContextHandler@3501a276{/environment/json,null,AVAILABLE,@Spark}]}]
22:57:14.686 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/job/json->[{GzipHandler@469e8d9e{STARTED},[o.s.j.s.ServletContextHandler@56d07c59{/jobs/job/json,null,AVAILABLE,@Spark}]}]
22:57:14.686 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs->[{GzipHandler@3ae6a9f6{STARTED},[o.s.j.s.ServletContextHandler@5d2d691a{/jobs,null,AVAILABLE,@Spark}]}]
22:57:14.686 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/json->[{GzipHandler@44284db4{STARTED},[o.s.j.s.ServletContextHandler@edcc350{/stages/json,null,AVAILABLE,@Spark}]}]
22:57:14.686 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/stage->[{GzipHandler@31c8e2ae{STARTED},[o.s.j.s.ServletContextHandler@3cdd11cc{/stages/stage,null,AVAILABLE,@Spark}]}]
22:57:14.686 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage/json->[{GzipHandler@9ae7eb5{STARTED},[o.s.j.s.ServletContextHandler@5288383e{/storage/json,null,AVAILABLE,@Spark}]}]
22:57:14.686 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage->[{GzipHandler@1dec257e{STARTED},[o.s.j.s.ServletContextHandler@49d00b2a{/storage,null,AVAILABLE,@Spark}]}]
22:57:14.686 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage/rdd/json->[{GzipHandler@31e69a85{STARTED},[o.s.j.s.ServletContextHandler@1ce6aa02{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
22:57:14.686 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - environment->[{GzipHandler@2762a443{STARTED},[o.s.j.s.ServletContextHandler@73adebf8{/environment,null,AVAILABLE,@Spark}]}]
22:57:14.686 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/job->[{GzipHandler@171f458{STARTED},[o.s.j.s.ServletContextHandler@1fdad46b{/jobs/job,null,AVAILABLE,@Spark}]}]
22:57:14.686 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages->[{GzipHandler@4f73ea1c{STARTED},[o.s.j.s.ServletContextHandler@5e077920{/stages,null,AVAILABLE,@Spark}]}]
22:57:14.686 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - executors->[{GzipHandler@2539b24e{STARTED},[o.s.j.s.ServletContextHandler@328c8921{/executors,null,AVAILABLE,@Spark}]}]
22:57:14.686 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/pool/json->[{GzipHandler@76f330b6{STARTED},[o.s.j.s.ServletContextHandler@6257b05c{/stages/pool/json,null,AVAILABLE,@Spark}]}]
22:57:14.686 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/pool->[{GzipHandler@532f7fe8{STARTED},[o.s.j.s.ServletContextHandler@50be7065{/stages/pool,null,AVAILABLE,@Spark}]}]
22:57:14.686 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - executors/threadDump->[{GzipHandler@30592f62{STARTED},[o.s.j.s.ServletContextHandler@7779b1d5{/executors/threadDump,null,AVAILABLE,@Spark}]}]
22:57:14.686 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - ContextHandlerCollection@7d170ea6{STARTED} added {GzipHandler@395860fa{STOPPED},UNMANAGED}
22:57:14.686 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting o.s.j.s.ServletContextHandler@5ca18a1d{/executors/threadDump/json,null,UNAVAILABLE,@Spark}
22:57:14.686 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting o.s.j.s.ServletContextHandler@5ca18a1d{/executors/threadDump/json,null,STARTING,@Spark}
22:57:14.686 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting ServletHandler@20934ec5{STOPPED}
22:57:14.686 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$1-7389b3c9[EMBEDDED:null]
22:57:14.687 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.http.pathmap.PathMappings - Added MappedResource[pathSpec=ServletPathSpec["/",pathDepth=-1,group=DEFAULT],resource=org.apache.spark.ui.JettyUtils$$anon$1-7389b3c9@d679b412==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true] to PathMappings[size=1]
22:57:14.687 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - filterNameMap={org.apache.spark.ui.HttpSecurityFilter-4c8c5e0f=org.apache.spark.ui.HttpSecurityFilter-4c8c5e0f@4c8c5e0f==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true}
22:57:14.687 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - pathFilters=[[/*]/[]/[INCLUDE, REQUEST, ASYNC, FORWARD, ERROR]=>org.apache.spark.ui.HttpSecurityFilter-4c8c5e0f]
22:57:14.687 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletFilterMap={}
22:57:14.687 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletPathMap=PathMappings[size=1]
22:57:14.687 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$1-7389b3c9=org.apache.spark.ui.JettyUtils$$anon$1-7389b3c9@d679b412==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true}
22:57:14.687 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting ServletHandler@20934ec5{STARTING}
22:57:14.687 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11460ms ServletHandler@20934ec5{STARTED}
22:57:14.687 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting org.apache.spark.ui.HttpSecurityFilter-4c8c5e0f@4c8c5e0f==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true
22:57:14.687 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11460ms org.apache.spark.ui.HttpSecurityFilter-4c8c5e0f@4c8c5e0f==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true
22:57:14.687 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.FilterHolder - Filter.init org.apache.spark.ui.HttpSecurityFilter@78ac9fc8
22:57:14.687 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting org.apache.spark.ui.JettyUtils$$anon$1-7389b3c9@d679b412==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true
22:57:14.687 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11460ms org.apache.spark.ui.JettyUtils$$anon$1-7389b3c9@d679b412==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true
22:57:14.687 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHolder - Servlet.init org.apache.spark.ui.JettyUtils$$anon$1@5b9fe11b for org.apache.spark.ui.JettyUtils$$anon$1-7389b3c9
22:57:14.687 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5ca18a1d{/executors/threadDump/json,null,AVAILABLE,@Spark}
22:57:14.687 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11460ms o.s.j.s.ServletContextHandler@5ca18a1d{/executors/threadDump/json,null,AVAILABLE,@Spark}
22:57:14.687 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting GzipHandler@395860fa{STOPPED}
22:57:14.687 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting GzipHandler@395860fa{STARTING}
22:57:14.687 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11460ms GzipHandler@395860fa{STARTED}
22:57:14.688 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.http.pathmap.PathMappings - Added MappedResource[pathSpec=ServletPathSpec["*.svgz",pathDepth=0,group=SUFFIX_GLOB],resource=true] to PathMappings[size=1]
22:57:14.688 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.gzip.GzipHandler - GzipHandler@670c9f4c{STOPPED} mime types IncludeExclude@7b421eba{i=[],ip=org.sparkproject.jetty.util.IncludeExcludeSet$SetContainsPredicate@5ff1bff0,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=org.sparkproject.jetty.util.IncludeExcludeSet$SetContainsPredicate@262b1aad}
22:57:14.688 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - GzipHandler@670c9f4c{STOPPED} added {o.s.j.s.ServletContextHandler@1c5e9364{/static,null,UNAVAILABLE,@Spark},MANAGED}
22:57:14.688 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/json->[{GzipHandler@2c462871{STARTED},[o.s.j.s.ServletContextHandler@6b382dd6{/jobs/json,null,AVAILABLE,@Spark}]}]
22:57:14.688 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - static->[{GzipHandler@670c9f4c{STOPPED},[o.s.j.s.ServletContextHandler@1c5e9364{/static,null,UNAVAILABLE,@Spark}]}]
22:57:14.688 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - executors/json->[{GzipHandler@2dc1fa11{STARTED},[o.s.j.s.ServletContextHandler@5e87276a{/executors/json,null,AVAILABLE,@Spark}]}]
22:57:14.688 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/stage/json->[{GzipHandler@30a40be{STARTED},[o.s.j.s.ServletContextHandler@6def4b33{/stages/stage/json,null,AVAILABLE,@Spark}]}]
22:57:14.688 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage/rdd->[{GzipHandler@1b24f851{STARTED},[o.s.j.s.ServletContextHandler@4320206d{/storage/rdd,null,AVAILABLE,@Spark}]}]
22:57:14.688 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - executors/threadDump/json->[{GzipHandler@395860fa{STARTED},[o.s.j.s.ServletContextHandler@5ca18a1d{/executors/threadDump/json,null,AVAILABLE,@Spark}]}]
22:57:14.688 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - environment/json->[{GzipHandler@56ef900d{STARTED},[o.s.j.s.ServletContextHandler@3501a276{/environment/json,null,AVAILABLE,@Spark}]}]
22:57:14.688 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/job/json->[{GzipHandler@469e8d9e{STARTED},[o.s.j.s.ServletContextHandler@56d07c59{/jobs/job/json,null,AVAILABLE,@Spark}]}]
22:57:14.688 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs->[{GzipHandler@3ae6a9f6{STARTED},[o.s.j.s.ServletContextHandler@5d2d691a{/jobs,null,AVAILABLE,@Spark}]}]
22:57:14.688 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/json->[{GzipHandler@44284db4{STARTED},[o.s.j.s.ServletContextHandler@edcc350{/stages/json,null,AVAILABLE,@Spark}]}]
22:57:14.688 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/stage->[{GzipHandler@31c8e2ae{STARTED},[o.s.j.s.ServletContextHandler@3cdd11cc{/stages/stage,null,AVAILABLE,@Spark}]}]
22:57:14.688 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage/json->[{GzipHandler@9ae7eb5{STARTED},[o.s.j.s.ServletContextHandler@5288383e{/storage/json,null,AVAILABLE,@Spark}]}]
22:57:14.688 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage->[{GzipHandler@1dec257e{STARTED},[o.s.j.s.ServletContextHandler@49d00b2a{/storage,null,AVAILABLE,@Spark}]}]
22:57:14.688 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage/rdd/json->[{GzipHandler@31e69a85{STARTED},[o.s.j.s.ServletContextHandler@1ce6aa02{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
22:57:14.688 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - environment->[{GzipHandler@2762a443{STARTED},[o.s.j.s.ServletContextHandler@73adebf8{/environment,null,AVAILABLE,@Spark}]}]
22:57:14.688 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/job->[{GzipHandler@171f458{STARTED},[o.s.j.s.ServletContextHandler@1fdad46b{/jobs/job,null,AVAILABLE,@Spark}]}]
22:57:14.688 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages->[{GzipHandler@4f73ea1c{STARTED},[o.s.j.s.ServletContextHandler@5e077920{/stages,null,AVAILABLE,@Spark}]}]
22:57:14.689 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - executors->[{GzipHandler@2539b24e{STARTED},[o.s.j.s.ServletContextHandler@328c8921{/executors,null,AVAILABLE,@Spark}]}]
22:57:14.689 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/pool/json->[{GzipHandler@76f330b6{STARTED},[o.s.j.s.ServletContextHandler@6257b05c{/stages/pool/json,null,AVAILABLE,@Spark}]}]
22:57:14.689 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/pool->[{GzipHandler@532f7fe8{STARTED},[o.s.j.s.ServletContextHandler@50be7065{/stages/pool,null,AVAILABLE,@Spark}]}]
22:57:14.689 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - executors/threadDump->[{GzipHandler@30592f62{STARTED},[o.s.j.s.ServletContextHandler@7779b1d5{/executors/threadDump,null,AVAILABLE,@Spark}]}]
22:57:14.689 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - ContextHandlerCollection@7d170ea6{STARTED} added {GzipHandler@670c9f4c{STOPPED},UNMANAGED}
22:57:14.689 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting o.s.j.s.ServletContextHandler@1c5e9364{/static,null,UNAVAILABLE,@Spark}
22:57:14.689 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting o.s.j.s.ServletContextHandler@1c5e9364{/static,null,STARTING,@Spark}
22:57:14.689 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting ServletHandler@22776cdd{STOPPED}
22:57:14.689 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - Path=/[EMBEDDED:null] mapped to servlet=org.sparkproject.jetty.servlet.DefaultServlet-1bedff88[EMBEDDED:null]
22:57:14.689 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.http.pathmap.PathMappings - Added MappedResource[pathSpec=ServletPathSpec["/",pathDepth=-1,group=DEFAULT],resource=org.sparkproject.jetty.servlet.DefaultServlet-1bedff88@a87701f8==org.sparkproject.jetty.servlet.DefaultServlet,jsp=null,order=-1,inst=true,async=true] to PathMappings[size=1]
22:57:14.689 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - filterNameMap={org.apache.spark.ui.HttpSecurityFilter-2ecafd2e=org.apache.spark.ui.HttpSecurityFilter-2ecafd2e@2ecafd2e==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true}
22:57:14.689 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - pathFilters=[[/*]/[]/[INCLUDE, REQUEST, ASYNC, FORWARD, ERROR]=>org.apache.spark.ui.HttpSecurityFilter-2ecafd2e]
22:57:14.689 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletFilterMap={}
22:57:14.689 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletPathMap=PathMappings[size=1]
22:57:14.689 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletNameMap={org.sparkproject.jetty.servlet.DefaultServlet-1bedff88=org.sparkproject.jetty.servlet.DefaultServlet-1bedff88@a87701f8==org.sparkproject.jetty.servlet.DefaultServlet,jsp=null,order=-1,inst=true,async=true}
22:57:14.689 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting ServletHandler@22776cdd{STARTING}
22:57:14.689 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11462ms ServletHandler@22776cdd{STARTED}
22:57:14.689 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting org.apache.spark.ui.HttpSecurityFilter-2ecafd2e@2ecafd2e==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true
22:57:14.689 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11462ms org.apache.spark.ui.HttpSecurityFilter-2ecafd2e@2ecafd2e==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true
22:57:14.689 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.FilterHolder - Filter.init org.apache.spark.ui.HttpSecurityFilter@25de07db
22:57:14.689 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting org.sparkproject.jetty.servlet.DefaultServlet-1bedff88@a87701f8==org.sparkproject.jetty.servlet.DefaultServlet,jsp=null,order=-1,inst=true,async=true
22:57:14.689 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11463ms org.sparkproject.jetty.servlet.DefaultServlet-1bedff88@a87701f8==org.sparkproject.jetty.servlet.DefaultServlet,jsp=null,order=-1,inst=true,async=true
22:57:14.690 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHolder - Servlet.init org.sparkproject.jetty.servlet.DefaultServlet@4e026a14 for org.sparkproject.jetty.servlet.DefaultServlet-1bedff88
22:57:14.702 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.DefaultServlet - resource base = jar:file:/Users/sue/.m2/repository/org/apache/spark/spark-core_2.12/3.0.1/spark-core_2.12-3.0.1.jar!/org/apache/spark/ui/static
22:57:14.702 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1c5e9364{/static,null,AVAILABLE,@Spark}
22:57:14.702 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11475ms o.s.j.s.ServletContextHandler@1c5e9364{/static,null,AVAILABLE,@Spark}
22:57:14.702 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting GzipHandler@670c9f4c{STOPPED}
22:57:14.702 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting GzipHandler@670c9f4c{STARTING}
22:57:14.702 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11475ms GzipHandler@670c9f4c{STARTED}
22:57:14.702 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.http.pathmap.PathMappings - Added MappedResource[pathSpec=ServletPathSpec["*.svgz",pathDepth=0,group=SUFFIX_GLOB],resource=true] to PathMappings[size=1]
22:57:14.703 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.gzip.GzipHandler - GzipHandler@47b4fed9{STOPPED} mime types IncludeExclude@229566b5{i=[],ip=org.sparkproject.jetty.util.IncludeExcludeSet$SetContainsPredicate@72d9bf38,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=org.sparkproject.jetty.util.IncludeExcludeSet$SetContainsPredicate@5ed7473c}
22:57:14.703 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - GzipHandler@47b4fed9{STOPPED} added {o.s.j.s.ServletContextHandler@19db68e6{/,null,UNAVAILABLE,@Spark},MANAGED}
22:57:14.703 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - ->[{GzipHandler@47b4fed9{STOPPED},[o.s.j.s.ServletContextHandler@19db68e6{/,null,UNAVAILABLE,@Spark}]}]
22:57:14.703 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/json->[{GzipHandler@2c462871{STARTED},[o.s.j.s.ServletContextHandler@6b382dd6{/jobs/json,null,AVAILABLE,@Spark}]}]
22:57:14.703 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - static->[{GzipHandler@670c9f4c{STARTED},[o.s.j.s.ServletContextHandler@1c5e9364{/static,null,AVAILABLE,@Spark}]}]
22:57:14.703 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - executors/json->[{GzipHandler@2dc1fa11{STARTED},[o.s.j.s.ServletContextHandler@5e87276a{/executors/json,null,AVAILABLE,@Spark}]}]
22:57:14.703 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/stage/json->[{GzipHandler@30a40be{STARTED},[o.s.j.s.ServletContextHandler@6def4b33{/stages/stage/json,null,AVAILABLE,@Spark}]}]
22:57:14.703 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage/rdd->[{GzipHandler@1b24f851{STARTED},[o.s.j.s.ServletContextHandler@4320206d{/storage/rdd,null,AVAILABLE,@Spark}]}]
22:57:14.703 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - executors/threadDump/json->[{GzipHandler@395860fa{STARTED},[o.s.j.s.ServletContextHandler@5ca18a1d{/executors/threadDump/json,null,AVAILABLE,@Spark}]}]
22:57:14.703 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - environment/json->[{GzipHandler@56ef900d{STARTED},[o.s.j.s.ServletContextHandler@3501a276{/environment/json,null,AVAILABLE,@Spark}]}]
22:57:14.703 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/job/json->[{GzipHandler@469e8d9e{STARTED},[o.s.j.s.ServletContextHandler@56d07c59{/jobs/job/json,null,AVAILABLE,@Spark}]}]
22:57:14.703 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs->[{GzipHandler@3ae6a9f6{STARTED},[o.s.j.s.ServletContextHandler@5d2d691a{/jobs,null,AVAILABLE,@Spark}]}]
22:57:14.703 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/json->[{GzipHandler@44284db4{STARTED},[o.s.j.s.ServletContextHandler@edcc350{/stages/json,null,AVAILABLE,@Spark}]}]
22:57:14.703 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/stage->[{GzipHandler@31c8e2ae{STARTED},[o.s.j.s.ServletContextHandler@3cdd11cc{/stages/stage,null,AVAILABLE,@Spark}]}]
22:57:14.703 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage/json->[{GzipHandler@9ae7eb5{STARTED},[o.s.j.s.ServletContextHandler@5288383e{/storage/json,null,AVAILABLE,@Spark}]}]
22:57:14.703 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage->[{GzipHandler@1dec257e{STARTED},[o.s.j.s.ServletContextHandler@49d00b2a{/storage,null,AVAILABLE,@Spark}]}]
22:57:14.703 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage/rdd/json->[{GzipHandler@31e69a85{STARTED},[o.s.j.s.ServletContextHandler@1ce6aa02{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
22:57:14.703 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - environment->[{GzipHandler@2762a443{STARTED},[o.s.j.s.ServletContextHandler@73adebf8{/environment,null,AVAILABLE,@Spark}]}]
22:57:14.703 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/job->[{GzipHandler@171f458{STARTED},[o.s.j.s.ServletContextHandler@1fdad46b{/jobs/job,null,AVAILABLE,@Spark}]}]
22:57:14.703 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages->[{GzipHandler@4f73ea1c{STARTED},[o.s.j.s.ServletContextHandler@5e077920{/stages,null,AVAILABLE,@Spark}]}]
22:57:14.703 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - executors->[{GzipHandler@2539b24e{STARTED},[o.s.j.s.ServletContextHandler@328c8921{/executors,null,AVAILABLE,@Spark}]}]
22:57:14.704 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/pool/json->[{GzipHandler@76f330b6{STARTED},[o.s.j.s.ServletContextHandler@6257b05c{/stages/pool/json,null,AVAILABLE,@Spark}]}]
22:57:14.704 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/pool->[{GzipHandler@532f7fe8{STARTED},[o.s.j.s.ServletContextHandler@50be7065{/stages/pool,null,AVAILABLE,@Spark}]}]
22:57:14.704 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - executors/threadDump->[{GzipHandler@30592f62{STARTED},[o.s.j.s.ServletContextHandler@7779b1d5{/executors/threadDump,null,AVAILABLE,@Spark}]}]
22:57:14.704 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - ContextHandlerCollection@7d170ea6{STARTED} added {GzipHandler@47b4fed9{STOPPED},UNMANAGED}
22:57:14.704 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting o.s.j.s.ServletContextHandler@19db68e6{/,null,UNAVAILABLE,@Spark}
22:57:14.704 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting o.s.j.s.ServletContextHandler@19db68e6{/,null,STARTING,@Spark}
22:57:14.704 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting ServletHandler@6113941b{STOPPED}
22:57:14.704 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$2-6f37d90a[EMBEDDED:null]
22:57:14.704 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.http.pathmap.PathMappings - Added MappedResource[pathSpec=ServletPathSpec["/",pathDepth=-1,group=DEFAULT],resource=org.apache.spark.ui.JettyUtils$$anon$2-6f37d90a@e41c5567==org.apache.spark.ui.JettyUtils$$anon$2,jsp=null,order=-1,inst=true,async=true] to PathMappings[size=1]
22:57:14.704 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - filterNameMap={org.apache.spark.ui.HttpSecurityFilter-1cc61c9b=org.apache.spark.ui.HttpSecurityFilter-1cc61c9b@1cc61c9b==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true}
22:57:14.704 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - pathFilters=[[/*]/[]/[INCLUDE, REQUEST, ASYNC, FORWARD, ERROR]=>org.apache.spark.ui.HttpSecurityFilter-1cc61c9b]
22:57:14.704 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletFilterMap={}
22:57:14.704 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletPathMap=PathMappings[size=1]
22:57:14.704 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-6f37d90a=org.apache.spark.ui.JettyUtils$$anon$2-6f37d90a@e41c5567==org.apache.spark.ui.JettyUtils$$anon$2,jsp=null,order=-1,inst=true,async=true}
22:57:14.704 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting ServletHandler@6113941b{STARTING}
22:57:14.704 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11477ms ServletHandler@6113941b{STARTED}
22:57:14.704 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting org.apache.spark.ui.HttpSecurityFilter-1cc61c9b@1cc61c9b==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true
22:57:14.704 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11477ms org.apache.spark.ui.HttpSecurityFilter-1cc61c9b@1cc61c9b==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true
22:57:14.704 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.FilterHolder - Filter.init org.apache.spark.ui.HttpSecurityFilter@3e752809
22:57:14.704 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting org.apache.spark.ui.JettyUtils$$anon$2-6f37d90a@e41c5567==org.apache.spark.ui.JettyUtils$$anon$2,jsp=null,order=-1,inst=true,async=true
22:57:14.704 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11477ms org.apache.spark.ui.JettyUtils$$anon$2-6f37d90a@e41c5567==org.apache.spark.ui.JettyUtils$$anon$2,jsp=null,order=-1,inst=true,async=true
22:57:14.704 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHolder - Servlet.init org.apache.spark.ui.JettyUtils$$anon$2@37dc41bc for org.apache.spark.ui.JettyUtils$$anon$2-6f37d90a
22:57:14.704 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@19db68e6{/,null,AVAILABLE,@Spark}
22:57:14.704 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11477ms o.s.j.s.ServletContextHandler@19db68e6{/,null,AVAILABLE,@Spark}
22:57:14.704 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting GzipHandler@47b4fed9{STOPPED}
22:57:14.704 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting GzipHandler@47b4fed9{STARTING}
22:57:14.704 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11477ms GzipHandler@47b4fed9{STARTED}
22:57:14.705 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.http.pathmap.PathMappings - Added MappedResource[pathSpec=ServletPathSpec["*.svgz",pathDepth=0,group=SUFFIX_GLOB],resource=true] to PathMappings[size=1]
22:57:14.705 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.gzip.GzipHandler - GzipHandler@6c495083{STOPPED} mime types IncludeExclude@7beb0863{i=[],ip=org.sparkproject.jetty.util.IncludeExcludeSet$SetContainsPredicate@2eef6703,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=org.sparkproject.jetty.util.IncludeExcludeSet$SetContainsPredicate@3efc79c6}
22:57:14.705 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - GzipHandler@6c495083{STOPPED} added {o.s.j.s.ServletContextHandler@24d35d46{/api,null,UNAVAILABLE,@Spark},MANAGED}
22:57:14.705 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - ->[{GzipHandler@47b4fed9{STARTED},[o.s.j.s.ServletContextHandler@19db68e6{/,null,AVAILABLE,@Spark}]}]
22:57:14.706 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/json->[{GzipHandler@2c462871{STARTED},[o.s.j.s.ServletContextHandler@6b382dd6{/jobs/json,null,AVAILABLE,@Spark}]}]
22:57:14.706 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - static->[{GzipHandler@670c9f4c{STARTED},[o.s.j.s.ServletContextHandler@1c5e9364{/static,null,AVAILABLE,@Spark}]}]
22:57:14.706 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - executors/json->[{GzipHandler@2dc1fa11{STARTED},[o.s.j.s.ServletContextHandler@5e87276a{/executors/json,null,AVAILABLE,@Spark}]}]
22:57:14.706 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/stage/json->[{GzipHandler@30a40be{STARTED},[o.s.j.s.ServletContextHandler@6def4b33{/stages/stage/json,null,AVAILABLE,@Spark}]}]
22:57:14.706 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage/rdd->[{GzipHandler@1b24f851{STARTED},[o.s.j.s.ServletContextHandler@4320206d{/storage/rdd,null,AVAILABLE,@Spark}]}]
22:57:14.706 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - executors/threadDump/json->[{GzipHandler@395860fa{STARTED},[o.s.j.s.ServletContextHandler@5ca18a1d{/executors/threadDump/json,null,AVAILABLE,@Spark}]}]
22:57:14.706 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - environment/json->[{GzipHandler@56ef900d{STARTED},[o.s.j.s.ServletContextHandler@3501a276{/environment/json,null,AVAILABLE,@Spark}]}]
22:57:14.706 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/job/json->[{GzipHandler@469e8d9e{STARTED},[o.s.j.s.ServletContextHandler@56d07c59{/jobs/job/json,null,AVAILABLE,@Spark}]}]
22:57:14.706 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs->[{GzipHandler@3ae6a9f6{STARTED},[o.s.j.s.ServletContextHandler@5d2d691a{/jobs,null,AVAILABLE,@Spark}]}]
22:57:14.706 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/json->[{GzipHandler@44284db4{STARTED},[o.s.j.s.ServletContextHandler@edcc350{/stages/json,null,AVAILABLE,@Spark}]}]
22:57:14.706 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/stage->[{GzipHandler@31c8e2ae{STARTED},[o.s.j.s.ServletContextHandler@3cdd11cc{/stages/stage,null,AVAILABLE,@Spark}]}]
22:57:14.706 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage/json->[{GzipHandler@9ae7eb5{STARTED},[o.s.j.s.ServletContextHandler@5288383e{/storage/json,null,AVAILABLE,@Spark}]}]
22:57:14.706 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage->[{GzipHandler@1dec257e{STARTED},[o.s.j.s.ServletContextHandler@49d00b2a{/storage,null,AVAILABLE,@Spark}]}]
22:57:14.706 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage/rdd/json->[{GzipHandler@31e69a85{STARTED},[o.s.j.s.ServletContextHandler@1ce6aa02{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
22:57:14.706 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - environment->[{GzipHandler@2762a443{STARTED},[o.s.j.s.ServletContextHandler@73adebf8{/environment,null,AVAILABLE,@Spark}]}]
22:57:14.706 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/job->[{GzipHandler@171f458{STARTED},[o.s.j.s.ServletContextHandler@1fdad46b{/jobs/job,null,AVAILABLE,@Spark}]}]
22:57:14.706 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages->[{GzipHandler@4f73ea1c{STARTED},[o.s.j.s.ServletContextHandler@5e077920{/stages,null,AVAILABLE,@Spark}]}]
22:57:14.707 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - executors->[{GzipHandler@2539b24e{STARTED},[o.s.j.s.ServletContextHandler@328c8921{/executors,null,AVAILABLE,@Spark}]}]
22:57:14.707 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - api->[{GzipHandler@6c495083{STOPPED},[o.s.j.s.ServletContextHandler@24d35d46{/api,null,UNAVAILABLE,@Spark}]}]
22:57:14.707 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/pool/json->[{GzipHandler@76f330b6{STARTED},[o.s.j.s.ServletContextHandler@6257b05c{/stages/pool/json,null,AVAILABLE,@Spark}]}]
22:57:14.707 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/pool->[{GzipHandler@532f7fe8{STARTED},[o.s.j.s.ServletContextHandler@50be7065{/stages/pool,null,AVAILABLE,@Spark}]}]
22:57:14.707 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - executors/threadDump->[{GzipHandler@30592f62{STARTED},[o.s.j.s.ServletContextHandler@7779b1d5{/executors/threadDump,null,AVAILABLE,@Spark}]}]
22:57:14.707 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - ContextHandlerCollection@7d170ea6{STARTED} added {GzipHandler@6c495083{STOPPED},UNMANAGED}
22:57:14.707 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting o.s.j.s.ServletContextHandler@24d35d46{/api,null,UNAVAILABLE,@Spark}
22:57:14.707 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting o.s.j.s.ServletContextHandler@24d35d46{/api,null,STARTING,@Spark}
22:57:14.707 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting ServletHandler@2ecc7caa{STOPPED}
22:57:14.707 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - Path=/*[EMBEDDED:null] mapped to servlet=org.glassfish.jersey.servlet.ServletContainer-51bdf739[EMBEDDED:null]
22:57:14.707 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.http.pathmap.PathMappings - Added MappedResource[pathSpec=ServletPathSpec["/*",pathDepth=1,group=PREFIX_GLOB],resource=org.glassfish.jersey.servlet.ServletContainer-51bdf739@27957c23==org.glassfish.jersey.servlet.ServletContainer,jsp=null,order=-1,inst=false,async=true] to PathMappings[size=1]
22:57:14.707 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - filterNameMap={org.apache.spark.ui.HttpSecurityFilter-8c31800=org.apache.spark.ui.HttpSecurityFilter-8c31800@8c31800==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true}
22:57:14.707 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - pathFilters=[[/*]/[]/[INCLUDE, REQUEST, ASYNC, FORWARD, ERROR]=>org.apache.spark.ui.HttpSecurityFilter-8c31800]
22:57:14.707 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletFilterMap={}
22:57:14.707 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletPathMap=PathMappings[size=1]
22:57:14.707 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletNameMap={org.glassfish.jersey.servlet.ServletContainer-51bdf739=org.glassfish.jersey.servlet.ServletContainer-51bdf739@27957c23==org.glassfish.jersey.servlet.ServletContainer,jsp=null,order=-1,inst=false,async=true}
22:57:14.707 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - Adding Default404Servlet to ServletHandler@2ecc7caa{STARTING}
22:57:14.708 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - Path=/*[EMBEDDED:null] mapped to servlet=org.glassfish.jersey.servlet.ServletContainer-51bdf739[EMBEDDED:null]
22:57:14.708 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.http.pathmap.PathMappings - Added MappedResource[pathSpec=ServletPathSpec["/*",pathDepth=1,group=PREFIX_GLOB],resource=org.glassfish.jersey.servlet.ServletContainer-51bdf739@27957c23==org.glassfish.jersey.servlet.ServletContainer,jsp=null,order=-1,inst=false,async=true] to PathMappings[size=1]
22:57:14.708 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - Path=/[EMBEDDED:null] mapped to servlet=org.sparkproject.jetty.servlet.ServletHandler$Default404Servlet-6e9effbc[EMBEDDED:null]
22:57:14.708 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.http.pathmap.PathMappings - Added MappedResource[pathSpec=ServletPathSpec["/",pathDepth=-1,group=DEFAULT],resource=org.sparkproject.jetty.servlet.ServletHandler$Default404Servlet-6e9effbc@4e707b9==org.sparkproject.jetty.servlet.ServletHandler$Default404Servlet,jsp=null,order=-1,inst=false,async=true] to PathMappings[size=2]
22:57:14.708 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - filterNameMap={org.apache.spark.ui.HttpSecurityFilter-8c31800=org.apache.spark.ui.HttpSecurityFilter-8c31800@8c31800==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true}
22:57:14.708 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - pathFilters=[[/*]/[]/[INCLUDE, REQUEST, ASYNC, FORWARD, ERROR]=>org.apache.spark.ui.HttpSecurityFilter-8c31800]
22:57:14.708 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletFilterMap={}
22:57:14.708 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletPathMap=PathMappings[size=2]
22:57:14.708 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletNameMap={org.glassfish.jersey.servlet.ServletContainer-51bdf739=org.glassfish.jersey.servlet.ServletContainer-51bdf739@27957c23==org.glassfish.jersey.servlet.ServletContainer,jsp=null,order=-1,inst=false,async=true, org.sparkproject.jetty.servlet.ServletHandler$Default404Servlet-6e9effbc=org.sparkproject.jetty.servlet.ServletHandler$Default404Servlet-6e9effbc@4e707b9==org.sparkproject.jetty.servlet.ServletHandler$Default404Servlet,jsp=null,order=-1,inst=false,async=true}
22:57:14.708 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting ServletHandler@2ecc7caa{STARTING}
22:57:14.708 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11481ms ServletHandler@2ecc7caa{STARTED}
22:57:14.708 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting org.apache.spark.ui.HttpSecurityFilter-8c31800@8c31800==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true
22:57:14.708 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11481ms org.apache.spark.ui.HttpSecurityFilter-8c31800@8c31800==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true
22:57:14.708 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.FilterHolder - Filter.init org.apache.spark.ui.HttpSecurityFilter@337199e0
22:57:14.708 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting org.glassfish.jersey.servlet.ServletContainer-51bdf739@27957c23==org.glassfish.jersey.servlet.ServletContainer,jsp=null,order=-1,inst=false,async=true
22:57:14.708 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11481ms org.glassfish.jersey.servlet.ServletContainer-51bdf739@27957c23==org.glassfish.jersey.servlet.ServletContainer,jsp=null,order=-1,inst=false,async=true
22:57:14.708 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting org.sparkproject.jetty.servlet.ServletHandler$Default404Servlet-6e9effbc@4e707b9==org.sparkproject.jetty.servlet.ServletHandler$Default404Servlet,jsp=null,order=-1,inst=false,async=true
22:57:14.708 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11481ms org.sparkproject.jetty.servlet.ServletHandler$Default404Servlet-6e9effbc@4e707b9==org.sparkproject.jetty.servlet.ServletHandler$Default404Servlet,jsp=null,order=-1,inst=false,async=true
22:57:14.708 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@24d35d46{/api,null,AVAILABLE,@Spark}
22:57:14.708 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11481ms o.s.j.s.ServletContextHandler@24d35d46{/api,null,AVAILABLE,@Spark}
22:57:14.708 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting GzipHandler@6c495083{STOPPED}
22:57:14.708 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting GzipHandler@6c495083{STARTING}
22:57:14.708 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11481ms GzipHandler@6c495083{STARTED}
22:57:14.709 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.http.pathmap.PathMappings - Added MappedResource[pathSpec=ServletPathSpec["*.svgz",pathDepth=0,group=SUFFIX_GLOB],resource=true] to PathMappings[size=1]
22:57:14.709 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.gzip.GzipHandler - GzipHandler@4d5f6e4f{STOPPED} mime types IncludeExclude@54250b47{i=[],ip=org.sparkproject.jetty.util.IncludeExcludeSet$SetContainsPredicate@6b2436ba,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=org.sparkproject.jetty.util.IncludeExcludeSet$SetContainsPredicate@6bf486c}
22:57:14.710 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - GzipHandler@4d5f6e4f{STOPPED} added {o.s.j.s.ServletContextHandler@6cd59fdc{/jobs/job/kill,null,UNAVAILABLE,@Spark},MANAGED}
22:57:14.713 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - ->[{GzipHandler@47b4fed9{STARTED},[o.s.j.s.ServletContextHandler@19db68e6{/,null,AVAILABLE,@Spark}]}]
22:57:14.713 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/json->[{GzipHandler@2c462871{STARTED},[o.s.j.s.ServletContextHandler@6b382dd6{/jobs/json,null,AVAILABLE,@Spark}]}]
22:57:14.714 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - static->[{GzipHandler@670c9f4c{STARTED},[o.s.j.s.ServletContextHandler@1c5e9364{/static,null,AVAILABLE,@Spark}]}]
22:57:14.714 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - executors/json->[{GzipHandler@2dc1fa11{STARTED},[o.s.j.s.ServletContextHandler@5e87276a{/executors/json,null,AVAILABLE,@Spark}]}]
22:57:14.714 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/stage/json->[{GzipHandler@30a40be{STARTED},[o.s.j.s.ServletContextHandler@6def4b33{/stages/stage/json,null,AVAILABLE,@Spark}]}]
22:57:14.714 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage/rdd->[{GzipHandler@1b24f851{STARTED},[o.s.j.s.ServletContextHandler@4320206d{/storage/rdd,null,AVAILABLE,@Spark}]}]
22:57:14.714 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - executors/threadDump/json->[{GzipHandler@395860fa{STARTED},[o.s.j.s.ServletContextHandler@5ca18a1d{/executors/threadDump/json,null,AVAILABLE,@Spark}]}]
22:57:14.714 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - environment/json->[{GzipHandler@56ef900d{STARTED},[o.s.j.s.ServletContextHandler@3501a276{/environment/json,null,AVAILABLE,@Spark}]}]
22:57:14.714 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/job/json->[{GzipHandler@469e8d9e{STARTED},[o.s.j.s.ServletContextHandler@56d07c59{/jobs/job/json,null,AVAILABLE,@Spark}]}]
22:57:14.714 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs->[{GzipHandler@3ae6a9f6{STARTED},[o.s.j.s.ServletContextHandler@5d2d691a{/jobs,null,AVAILABLE,@Spark}]}]
22:57:14.714 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/json->[{GzipHandler@44284db4{STARTED},[o.s.j.s.ServletContextHandler@edcc350{/stages/json,null,AVAILABLE,@Spark}]}]
22:57:14.714 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/stage->[{GzipHandler@31c8e2ae{STARTED},[o.s.j.s.ServletContextHandler@3cdd11cc{/stages/stage,null,AVAILABLE,@Spark}]}]
22:57:14.714 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage/json->[{GzipHandler@9ae7eb5{STARTED},[o.s.j.s.ServletContextHandler@5288383e{/storage/json,null,AVAILABLE,@Spark}]}]
22:57:14.714 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage->[{GzipHandler@1dec257e{STARTED},[o.s.j.s.ServletContextHandler@49d00b2a{/storage,null,AVAILABLE,@Spark}]}]
22:57:14.714 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage/rdd/json->[{GzipHandler@31e69a85{STARTED},[o.s.j.s.ServletContextHandler@1ce6aa02{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
22:57:14.714 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - environment->[{GzipHandler@2762a443{STARTED},[o.s.j.s.ServletContextHandler@73adebf8{/environment,null,AVAILABLE,@Spark}]}]
22:57:14.714 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/job->[{GzipHandler@171f458{STARTED},[o.s.j.s.ServletContextHandler@1fdad46b{/jobs/job,null,AVAILABLE,@Spark}]}]
22:57:14.714 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages->[{GzipHandler@4f73ea1c{STARTED},[o.s.j.s.ServletContextHandler@5e077920{/stages,null,AVAILABLE,@Spark}]}]
22:57:14.714 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - executors->[{GzipHandler@2539b24e{STARTED},[o.s.j.s.ServletContextHandler@328c8921{/executors,null,AVAILABLE,@Spark}]}]
22:57:14.714 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - api->[{GzipHandler@6c495083{STARTED},[o.s.j.s.ServletContextHandler@24d35d46{/api,null,AVAILABLE,@Spark}]}]
22:57:14.714 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/job/kill->[{GzipHandler@4d5f6e4f{STOPPED},[o.s.j.s.ServletContextHandler@6cd59fdc{/jobs/job/kill,null,UNAVAILABLE,@Spark}]}]
22:57:14.714 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/pool/json->[{GzipHandler@76f330b6{STARTED},[o.s.j.s.ServletContextHandler@6257b05c{/stages/pool/json,null,AVAILABLE,@Spark}]}]
22:57:14.714 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/pool->[{GzipHandler@532f7fe8{STARTED},[o.s.j.s.ServletContextHandler@50be7065{/stages/pool,null,AVAILABLE,@Spark}]}]
22:57:14.714 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - executors/threadDump->[{GzipHandler@30592f62{STARTED},[o.s.j.s.ServletContextHandler@7779b1d5{/executors/threadDump,null,AVAILABLE,@Spark}]}]
22:57:14.715 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - ContextHandlerCollection@7d170ea6{STARTED} added {GzipHandler@4d5f6e4f{STOPPED},UNMANAGED}
22:57:14.716 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting o.s.j.s.ServletContextHandler@6cd59fdc{/jobs/job/kill,null,UNAVAILABLE,@Spark}
22:57:14.716 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting o.s.j.s.ServletContextHandler@6cd59fdc{/jobs/job/kill,null,STARTING,@Spark}
22:57:14.717 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting ServletHandler@53b2b1c0{STOPPED}
22:57:14.717 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$2-79be7534[EMBEDDED:null]
22:57:14.717 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.http.pathmap.PathMappings - Added MappedResource[pathSpec=ServletPathSpec["/",pathDepth=-1,group=DEFAULT],resource=org.apache.spark.ui.JettyUtils$$anon$2-79be7534@522c92b1==org.apache.spark.ui.JettyUtils$$anon$2,jsp=null,order=-1,inst=true,async=true] to PathMappings[size=1]
22:57:14.717 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - filterNameMap={org.apache.spark.ui.HttpSecurityFilter-141f3d37=org.apache.spark.ui.HttpSecurityFilter-141f3d37@141f3d37==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true}
22:57:14.717 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - pathFilters=[[/*]/[]/[INCLUDE, REQUEST, ASYNC, FORWARD, ERROR]=>org.apache.spark.ui.HttpSecurityFilter-141f3d37]
22:57:14.717 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletFilterMap={}
22:57:14.717 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletPathMap=PathMappings[size=1]
22:57:14.717 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-79be7534=org.apache.spark.ui.JettyUtils$$anon$2-79be7534@522c92b1==org.apache.spark.ui.JettyUtils$$anon$2,jsp=null,order=-1,inst=true,async=true}
22:57:14.717 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting ServletHandler@53b2b1c0{STARTING}
22:57:14.717 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11490ms ServletHandler@53b2b1c0{STARTED}
22:57:14.717 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting org.apache.spark.ui.HttpSecurityFilter-141f3d37@141f3d37==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true
22:57:14.717 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11490ms org.apache.spark.ui.HttpSecurityFilter-141f3d37@141f3d37==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true
22:57:14.717 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.FilterHolder - Filter.init org.apache.spark.ui.HttpSecurityFilter@30a473e2
22:57:14.717 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting org.apache.spark.ui.JettyUtils$$anon$2-79be7534@522c92b1==org.apache.spark.ui.JettyUtils$$anon$2,jsp=null,order=-1,inst=true,async=true
22:57:14.717 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11490ms org.apache.spark.ui.JettyUtils$$anon$2-79be7534@522c92b1==org.apache.spark.ui.JettyUtils$$anon$2,jsp=null,order=-1,inst=true,async=true
22:57:14.717 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHolder - Servlet.init org.apache.spark.ui.JettyUtils$$anon$2@4c273982 for org.apache.spark.ui.JettyUtils$$anon$2-79be7534
22:57:14.717 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6cd59fdc{/jobs/job/kill,null,AVAILABLE,@Spark}
22:57:14.717 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11490ms o.s.j.s.ServletContextHandler@6cd59fdc{/jobs/job/kill,null,AVAILABLE,@Spark}
22:57:14.717 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting GzipHandler@4d5f6e4f{STOPPED}
22:57:14.717 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting GzipHandler@4d5f6e4f{STARTING}
22:57:14.717 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11490ms GzipHandler@4d5f6e4f{STARTED}
22:57:14.718 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.http.pathmap.PathMappings - Added MappedResource[pathSpec=ServletPathSpec["*.svgz",pathDepth=0,group=SUFFIX_GLOB],resource=true] to PathMappings[size=1]
22:57:14.718 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.gzip.GzipHandler - GzipHandler@7afc93c1{STOPPED} mime types IncludeExclude@69d4c41f{i=[],ip=org.sparkproject.jetty.util.IncludeExcludeSet$SetContainsPredicate@1cca0b8e,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=org.sparkproject.jetty.util.IncludeExcludeSet$SetContainsPredicate@78e8d9ca}
22:57:14.718 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - GzipHandler@7afc93c1{STOPPED} added {o.s.j.s.ServletContextHandler@7052620d{/stages/stage/kill,null,UNAVAILABLE,@Spark},MANAGED}
22:57:14.719 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - ->[{GzipHandler@47b4fed9{STARTED},[o.s.j.s.ServletContextHandler@19db68e6{/,null,AVAILABLE,@Spark}]}]
22:57:14.719 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage/rdd->[{GzipHandler@1b24f851{STARTED},[o.s.j.s.ServletContextHandler@4320206d{/storage/rdd,null,AVAILABLE,@Spark}]}]
22:57:14.719 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage->[{GzipHandler@1dec257e{STARTED},[o.s.j.s.ServletContextHandler@49d00b2a{/storage,null,AVAILABLE,@Spark}]}]
22:57:14.719 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage/rdd/json->[{GzipHandler@31e69a85{STARTED},[o.s.j.s.ServletContextHandler@1ce6aa02{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
22:57:14.719 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - api->[{GzipHandler@6c495083{STARTED},[o.s.j.s.ServletContextHandler@24d35d46{/api,null,AVAILABLE,@Spark}]}]
22:57:14.719 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/pool/json->[{GzipHandler@76f330b6{STARTED},[o.s.j.s.ServletContextHandler@6257b05c{/stages/pool/json,null,AVAILABLE,@Spark}]}]
22:57:14.719 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/pool->[{GzipHandler@532f7fe8{STARTED},[o.s.j.s.ServletContextHandler@50be7065{/stages/pool,null,AVAILABLE,@Spark}]}]
22:57:14.719 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/json->[{GzipHandler@2c462871{STARTED},[o.s.j.s.ServletContextHandler@6b382dd6{/jobs/json,null,AVAILABLE,@Spark}]}]
22:57:14.719 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - static->[{GzipHandler@670c9f4c{STARTED},[o.s.j.s.ServletContextHandler@1c5e9364{/static,null,AVAILABLE,@Spark}]}]
22:57:14.719 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - executors/json->[{GzipHandler@2dc1fa11{STARTED},[o.s.j.s.ServletContextHandler@5e87276a{/executors/json,null,AVAILABLE,@Spark}]}]
22:57:14.719 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/stage/json->[{GzipHandler@30a40be{STARTED},[o.s.j.s.ServletContextHandler@6def4b33{/stages/stage/json,null,AVAILABLE,@Spark}]}]
22:57:14.719 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - executors/threadDump/json->[{GzipHandler@395860fa{STARTED},[o.s.j.s.ServletContextHandler@5ca18a1d{/executors/threadDump/json,null,AVAILABLE,@Spark}]}]
22:57:14.719 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - environment/json->[{GzipHandler@56ef900d{STARTED},[o.s.j.s.ServletContextHandler@3501a276{/environment/json,null,AVAILABLE,@Spark}]}]
22:57:14.719 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/job/json->[{GzipHandler@469e8d9e{STARTED},[o.s.j.s.ServletContextHandler@56d07c59{/jobs/job/json,null,AVAILABLE,@Spark}]}]
22:57:14.719 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs->[{GzipHandler@3ae6a9f6{STARTED},[o.s.j.s.ServletContextHandler@5d2d691a{/jobs,null,AVAILABLE,@Spark}]}]
22:57:14.719 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/json->[{GzipHandler@44284db4{STARTED},[o.s.j.s.ServletContextHandler@edcc350{/stages/json,null,AVAILABLE,@Spark}]}]
22:57:14.719 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/stage->[{GzipHandler@31c8e2ae{STARTED},[o.s.j.s.ServletContextHandler@3cdd11cc{/stages/stage,null,AVAILABLE,@Spark}]}]
22:57:14.719 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage/json->[{GzipHandler@9ae7eb5{STARTED},[o.s.j.s.ServletContextHandler@5288383e{/storage/json,null,AVAILABLE,@Spark}]}]
22:57:14.719 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/stage/kill->[{GzipHandler@7afc93c1{STOPPED},[o.s.j.s.ServletContextHandler@7052620d{/stages/stage/kill,null,UNAVAILABLE,@Spark}]}]
22:57:14.719 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/job->[{GzipHandler@171f458{STARTED},[o.s.j.s.ServletContextHandler@1fdad46b{/jobs/job,null,AVAILABLE,@Spark}]}]
22:57:14.719 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - environment->[{GzipHandler@2762a443{STARTED},[o.s.j.s.ServletContextHandler@73adebf8{/environment,null,AVAILABLE,@Spark}]}]
22:57:14.719 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages->[{GzipHandler@4f73ea1c{STARTED},[o.s.j.s.ServletContextHandler@5e077920{/stages,null,AVAILABLE,@Spark}]}]
22:57:14.719 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - executors->[{GzipHandler@2539b24e{STARTED},[o.s.j.s.ServletContextHandler@328c8921{/executors,null,AVAILABLE,@Spark}]}]
22:57:14.719 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/job/kill->[{GzipHandler@4d5f6e4f{STARTED},[o.s.j.s.ServletContextHandler@6cd59fdc{/jobs/job/kill,null,AVAILABLE,@Spark}]}]
22:57:14.719 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - executors/threadDump->[{GzipHandler@30592f62{STARTED},[o.s.j.s.ServletContextHandler@7779b1d5{/executors/threadDump,null,AVAILABLE,@Spark}]}]
22:57:14.719 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - ContextHandlerCollection@7d170ea6{STARTED} added {GzipHandler@7afc93c1{STOPPED},UNMANAGED}
22:57:14.720 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting o.s.j.s.ServletContextHandler@7052620d{/stages/stage/kill,null,UNAVAILABLE,@Spark}
22:57:14.720 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting o.s.j.s.ServletContextHandler@7052620d{/stages/stage/kill,null,STARTING,@Spark}
22:57:14.720 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting ServletHandler@7a4fbee6{STOPPED}
22:57:14.720 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$2-11f9cfde[EMBEDDED:null]
22:57:14.720 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.http.pathmap.PathMappings - Added MappedResource[pathSpec=ServletPathSpec["/",pathDepth=-1,group=DEFAULT],resource=org.apache.spark.ui.JettyUtils$$anon$2-11f9cfde@402fc224==org.apache.spark.ui.JettyUtils$$anon$2,jsp=null,order=-1,inst=true,async=true] to PathMappings[size=1]
22:57:14.720 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - filterNameMap={org.apache.spark.ui.HttpSecurityFilter-6fc575a=org.apache.spark.ui.HttpSecurityFilter-6fc575a@6fc575a==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true}
22:57:14.720 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - pathFilters=[[/*]/[]/[INCLUDE, REQUEST, ASYNC, FORWARD, ERROR]=>org.apache.spark.ui.HttpSecurityFilter-6fc575a]
22:57:14.720 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletFilterMap={}
22:57:14.720 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletPathMap=PathMappings[size=1]
22:57:14.720 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$2-11f9cfde=org.apache.spark.ui.JettyUtils$$anon$2-11f9cfde@402fc224==org.apache.spark.ui.JettyUtils$$anon$2,jsp=null,order=-1,inst=true,async=true}
22:57:14.720 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting ServletHandler@7a4fbee6{STARTING}
22:57:14.720 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11493ms ServletHandler@7a4fbee6{STARTED}
22:57:14.720 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting org.apache.spark.ui.HttpSecurityFilter-6fc575a@6fc575a==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true
22:57:14.720 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11493ms org.apache.spark.ui.HttpSecurityFilter-6fc575a@6fc575a==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true
22:57:14.720 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.FilterHolder - Filter.init org.apache.spark.ui.HttpSecurityFilter@66143f8d
22:57:14.720 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting org.apache.spark.ui.JettyUtils$$anon$2-11f9cfde@402fc224==org.apache.spark.ui.JettyUtils$$anon$2,jsp=null,order=-1,inst=true,async=true
22:57:14.720 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11493ms org.apache.spark.ui.JettyUtils$$anon$2-11f9cfde@402fc224==org.apache.spark.ui.JettyUtils$$anon$2,jsp=null,order=-1,inst=true,async=true
22:57:14.720 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHolder - Servlet.init org.apache.spark.ui.JettyUtils$$anon$2@292ffb6c for org.apache.spark.ui.JettyUtils$$anon$2-11f9cfde
22:57:14.720 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@7052620d{/stages/stage/kill,null,AVAILABLE,@Spark}
22:57:14.720 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11493ms o.s.j.s.ServletContextHandler@7052620d{/stages/stage/kill,null,AVAILABLE,@Spark}
22:57:14.720 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting GzipHandler@7afc93c1{STOPPED}
22:57:14.720 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting GzipHandler@7afc93c1{STARTING}
22:57:14.720 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @11493ms GzipHandler@7afc93c1{STARTED}
22:57:14.722 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.ui.SparkUI - Bound SparkUI to 0.0.0.0, and started at http://saras-air:4040
22:57:15.157 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.executor.Executor - Starting executor ID driver on host saras-air
22:57:15.197 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.network.server.TransportServer - Shuffle server started on port: 55957
22:57:15.199 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.util.Utils - Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 55957.
22:57:15.200 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.network.netty.NettyBlockTransferService - Server created on saras-air:55957
22:57:15.203 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.storage.BlockManager - Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
22:57:15.212 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.storage.BlockManagerMaster - Registering BlockManager BlockManagerId(driver, saras-air, 55957, None)
22:57:15.215 [dispatcher-BlockManagerMaster] DEBUG org.apache.spark.storage.DefaultTopologyMapper - Got a request for saras-air
22:57:15.220 [dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerMasterEndpoint - Registering block manager saras-air:55957 with 1048.8 MiB RAM, BlockManagerId(driver, saras-air, 55957, None)
22:57:15.225 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.storage.BlockManagerMaster - Registered BlockManager BlockManagerId(driver, saras-air, 55957, None)
22:57:15.227 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.storage.BlockManager - Initialized BlockManager: BlockManagerId(driver, saras-air, 55957, None)
22:57:15.432 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.DecoratedObjectFactory - Adding Decorator: org.sparkproject.jetty.util.DeprecationWarning@631e092
22:57:15.433 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - o.s.j.s.ServletContextHandler@6156560{/,null,UNAVAILABLE} added {ServletHandler@ca8c5cf{STOPPED},MANAGED}
22:57:15.438 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.http.pathmap.PathMappings - Added MappedResource[pathSpec=ServletPathSpec["*.svgz",pathDepth=0,group=SUFFIX_GLOB],resource=true] to PathMappings[size=1]
22:57:15.439 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.gzip.GzipHandler - GzipHandler@684f5877{STOPPED} mime types IncludeExclude@9a87141{i=[],ip=org.sparkproject.jetty.util.IncludeExcludeSet$SetContainsPredicate@5b97cda9,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=org.sparkproject.jetty.util.IncludeExcludeSet$SetContainsPredicate@12042d57}
22:57:15.439 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - GzipHandler@684f5877{STOPPED} added {o.s.j.s.ServletContextHandler@6156560{/metrics/json,null,UNAVAILABLE,@Spark},MANAGED}
22:57:15.440 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - ->[{GzipHandler@47b4fed9{STARTED},[o.s.j.s.ServletContextHandler@19db68e6{/,null,AVAILABLE,@Spark}]}]
22:57:15.440 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage/rdd->[{GzipHandler@1b24f851{STARTED},[o.s.j.s.ServletContextHandler@4320206d{/storage/rdd,null,AVAILABLE,@Spark}]}]
22:57:15.440 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage->[{GzipHandler@1dec257e{STARTED},[o.s.j.s.ServletContextHandler@49d00b2a{/storage,null,AVAILABLE,@Spark}]}]
22:57:15.440 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage/rdd/json->[{GzipHandler@31e69a85{STARTED},[o.s.j.s.ServletContextHandler@1ce6aa02{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
22:57:15.440 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - api->[{GzipHandler@6c495083{STARTED},[o.s.j.s.ServletContextHandler@24d35d46{/api,null,AVAILABLE,@Spark}]}]
22:57:15.440 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/pool/json->[{GzipHandler@76f330b6{STARTED},[o.s.j.s.ServletContextHandler@6257b05c{/stages/pool/json,null,AVAILABLE,@Spark}]}]
22:57:15.440 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/pool->[{GzipHandler@532f7fe8{STARTED},[o.s.j.s.ServletContextHandler@50be7065{/stages/pool,null,AVAILABLE,@Spark}]}]
22:57:15.440 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/json->[{GzipHandler@2c462871{STARTED},[o.s.j.s.ServletContextHandler@6b382dd6{/jobs/json,null,AVAILABLE,@Spark}]}]
22:57:15.440 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - static->[{GzipHandler@670c9f4c{STARTED},[o.s.j.s.ServletContextHandler@1c5e9364{/static,null,AVAILABLE,@Spark}]}]
22:57:15.440 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - executors/json->[{GzipHandler@2dc1fa11{STARTED},[o.s.j.s.ServletContextHandler@5e87276a{/executors/json,null,AVAILABLE,@Spark}]}]
22:57:15.440 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/stage/json->[{GzipHandler@30a40be{STARTED},[o.s.j.s.ServletContextHandler@6def4b33{/stages/stage/json,null,AVAILABLE,@Spark}]}]
22:57:15.440 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - executors/threadDump/json->[{GzipHandler@395860fa{STARTED},[o.s.j.s.ServletContextHandler@5ca18a1d{/executors/threadDump/json,null,AVAILABLE,@Spark}]}]
22:57:15.440 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - environment/json->[{GzipHandler@56ef900d{STARTED},[o.s.j.s.ServletContextHandler@3501a276{/environment/json,null,AVAILABLE,@Spark}]}]
22:57:15.440 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/job/json->[{GzipHandler@469e8d9e{STARTED},[o.s.j.s.ServletContextHandler@56d07c59{/jobs/job/json,null,AVAILABLE,@Spark}]}]
22:57:15.440 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs->[{GzipHandler@3ae6a9f6{STARTED},[o.s.j.s.ServletContextHandler@5d2d691a{/jobs,null,AVAILABLE,@Spark}]}]
22:57:15.440 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/json->[{GzipHandler@44284db4{STARTED},[o.s.j.s.ServletContextHandler@edcc350{/stages/json,null,AVAILABLE,@Spark}]}]
22:57:15.441 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/stage->[{GzipHandler@31c8e2ae{STARTED},[o.s.j.s.ServletContextHandler@3cdd11cc{/stages/stage,null,AVAILABLE,@Spark}]}]
22:57:15.441 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage/json->[{GzipHandler@9ae7eb5{STARTED},[o.s.j.s.ServletContextHandler@5288383e{/storage/json,null,AVAILABLE,@Spark}]}]
22:57:15.441 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/stage/kill->[{GzipHandler@7afc93c1{STARTED},[o.s.j.s.ServletContextHandler@7052620d{/stages/stage/kill,null,AVAILABLE,@Spark}]}]
22:57:15.441 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/job->[{GzipHandler@171f458{STARTED},[o.s.j.s.ServletContextHandler@1fdad46b{/jobs/job,null,AVAILABLE,@Spark}]}]
22:57:15.441 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - environment->[{GzipHandler@2762a443{STARTED},[o.s.j.s.ServletContextHandler@73adebf8{/environment,null,AVAILABLE,@Spark}]}]
22:57:15.441 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages->[{GzipHandler@4f73ea1c{STARTED},[o.s.j.s.ServletContextHandler@5e077920{/stages,null,AVAILABLE,@Spark}]}]
22:57:15.441 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - executors->[{GzipHandler@2539b24e{STARTED},[o.s.j.s.ServletContextHandler@328c8921{/executors,null,AVAILABLE,@Spark}]}]
22:57:15.441 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/job/kill->[{GzipHandler@4d5f6e4f{STARTED},[o.s.j.s.ServletContextHandler@6cd59fdc{/jobs/job/kill,null,AVAILABLE,@Spark}]}]
22:57:15.441 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - metrics/json->[{GzipHandler@684f5877{STOPPED},[o.s.j.s.ServletContextHandler@6156560{/metrics/json,null,UNAVAILABLE,@Spark}]}]
22:57:15.441 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - executors/threadDump->[{GzipHandler@30592f62{STARTED},[o.s.j.s.ServletContextHandler@7779b1d5{/executors/threadDump,null,AVAILABLE,@Spark}]}]
22:57:15.441 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - ContextHandlerCollection@7d170ea6{STARTED} added {GzipHandler@684f5877{STOPPED},UNMANAGED}
22:57:15.441 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting o.s.j.s.ServletContextHandler@6156560{/metrics/json,null,UNAVAILABLE,@Spark}
22:57:15.441 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting o.s.j.s.ServletContextHandler@6156560{/metrics/json,null,STARTING,@Spark}
22:57:15.441 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting ServletHandler@ca8c5cf{STOPPED}
22:57:15.441 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$1-338397e1[EMBEDDED:null]
22:57:15.442 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.http.pathmap.PathMappings - Added MappedResource[pathSpec=ServletPathSpec["/",pathDepth=-1,group=DEFAULT],resource=org.apache.spark.ui.JettyUtils$$anon$1-338397e1@368ddf93==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true] to PathMappings[size=1]
22:57:15.442 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - filterNameMap={org.apache.spark.ui.HttpSecurityFilter-7717d9bd=org.apache.spark.ui.HttpSecurityFilter-7717d9bd@7717d9bd==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true}
22:57:15.442 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - pathFilters=[[/*]/[]/[INCLUDE, REQUEST, ASYNC, FORWARD, ERROR]=>org.apache.spark.ui.HttpSecurityFilter-7717d9bd]
22:57:15.442 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletFilterMap={}
22:57:15.442 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletPathMap=PathMappings[size=1]
22:57:15.442 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$1-338397e1=org.apache.spark.ui.JettyUtils$$anon$1-338397e1@368ddf93==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true}
22:57:15.442 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting ServletHandler@ca8c5cf{STARTING}
22:57:15.442 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @12215ms ServletHandler@ca8c5cf{STARTED}
22:57:15.442 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting org.apache.spark.ui.HttpSecurityFilter-7717d9bd@7717d9bd==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true
22:57:15.442 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @12215ms org.apache.spark.ui.HttpSecurityFilter-7717d9bd@7717d9bd==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true
22:57:15.442 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.FilterHolder - Filter.init org.apache.spark.ui.HttpSecurityFilter@1a4380
22:57:15.442 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting org.apache.spark.ui.JettyUtils$$anon$1-338397e1@368ddf93==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true
22:57:15.442 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @12215ms org.apache.spark.ui.JettyUtils$$anon$1-338397e1@368ddf93==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true
22:57:15.442 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHolder - Servlet.init org.apache.spark.ui.JettyUtils$$anon$1@4af208bb for org.apache.spark.ui.JettyUtils$$anon$1-338397e1
22:57:15.442 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6156560{/metrics/json,null,AVAILABLE,@Spark}
22:57:15.442 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @12216ms o.s.j.s.ServletContextHandler@6156560{/metrics/json,null,AVAILABLE,@Spark}
22:57:15.442 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting GzipHandler@684f5877{STOPPED}
22:57:15.442 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting GzipHandler@684f5877{STARTING}
22:57:15.442 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @12216ms GzipHandler@684f5877{STARTED}
22:57:15.642 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.SparkContext - Adding shutdown hook
22:57:16.226 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir ('file:/Users/sue/Documents/ITI/java-for-ml/final-project/WuzzufJobsAnalysis/spark-warehouse/').
22:57:16.231 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.sql.internal.SharedState - Warehouse path is 'file:/Users/sue/Documents/ITI/java-for-ml/final-project/WuzzufJobsAnalysis/spark-warehouse/'.
22:57:16.233 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.sql.internal.SharedState - Applying initial SparkSession options to SparkConf/HadoopConf: spark.app.name -> Java Spark ML project
22:57:16.233 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.sql.internal.SharedState - Applying initial SparkSession options to SparkConf/HadoopConf: spark.master -> local
22:57:16.263 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.DecoratedObjectFactory - Adding Decorator: org.sparkproject.jetty.util.DeprecationWarning@d57006a
22:57:16.270 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - o.s.j.s.ServletContextHandler@6ec01c04{/,null,UNAVAILABLE} added {ServletHandler@388e756f{STOPPED},MANAGED}
22:57:16.270 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.DecoratedObjectFactory - Adding Decorator: org.sparkproject.jetty.util.DeprecationWarning@3dffcc7d
22:57:16.271 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - o.s.j.s.ServletContextHandler@5331c7fa{/,null,UNAVAILABLE} added {ServletHandler@18dfe5fe{STOPPED},MANAGED}
22:57:16.274 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.http.pathmap.PathMappings - Added MappedResource[pathSpec=ServletPathSpec["*.svgz",pathDepth=0,group=SUFFIX_GLOB],resource=true] to PathMappings[size=1]
22:57:16.275 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.gzip.GzipHandler - GzipHandler@78f97207{STOPPED} mime types IncludeExclude@1f6e98f3{i=[],ip=org.sparkproject.jetty.util.IncludeExcludeSet$SetContainsPredicate@570d2b4d,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=org.sparkproject.jetty.util.IncludeExcludeSet$SetContainsPredicate@212bf6e4}
22:57:16.275 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - GzipHandler@78f97207{STOPPED} added {o.s.j.s.ServletContextHandler@6ec01c04{/SQL,null,UNAVAILABLE,@Spark},MANAGED}
22:57:16.277 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - ->[{GzipHandler@47b4fed9{STARTED},[o.s.j.s.ServletContextHandler@19db68e6{/,null,AVAILABLE,@Spark}]}]
22:57:16.278 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage/rdd->[{GzipHandler@1b24f851{STARTED},[o.s.j.s.ServletContextHandler@4320206d{/storage/rdd,null,AVAILABLE,@Spark}]}]
22:57:16.278 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage->[{GzipHandler@1dec257e{STARTED},[o.s.j.s.ServletContextHandler@49d00b2a{/storage,null,AVAILABLE,@Spark}]}]
22:57:16.278 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage/rdd/json->[{GzipHandler@31e69a85{STARTED},[o.s.j.s.ServletContextHandler@1ce6aa02{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
22:57:16.278 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - api->[{GzipHandler@6c495083{STARTED},[o.s.j.s.ServletContextHandler@24d35d46{/api,null,AVAILABLE,@Spark}]}]
22:57:16.278 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/pool/json->[{GzipHandler@76f330b6{STARTED},[o.s.j.s.ServletContextHandler@6257b05c{/stages/pool/json,null,AVAILABLE,@Spark}]}]
22:57:16.278 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/pool->[{GzipHandler@532f7fe8{STARTED},[o.s.j.s.ServletContextHandler@50be7065{/stages/pool,null,AVAILABLE,@Spark}]}]
22:57:16.278 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/json->[{GzipHandler@2c462871{STARTED},[o.s.j.s.ServletContextHandler@6b382dd6{/jobs/json,null,AVAILABLE,@Spark}]}]
22:57:16.278 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - static->[{GzipHandler@670c9f4c{STARTED},[o.s.j.s.ServletContextHandler@1c5e9364{/static,null,AVAILABLE,@Spark}]}]
22:57:16.278 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - executors/json->[{GzipHandler@2dc1fa11{STARTED},[o.s.j.s.ServletContextHandler@5e87276a{/executors/json,null,AVAILABLE,@Spark}]}]
22:57:16.278 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/stage/json->[{GzipHandler@30a40be{STARTED},[o.s.j.s.ServletContextHandler@6def4b33{/stages/stage/json,null,AVAILABLE,@Spark}]}]
22:57:16.278 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - executors/threadDump/json->[{GzipHandler@395860fa{STARTED},[o.s.j.s.ServletContextHandler@5ca18a1d{/executors/threadDump/json,null,AVAILABLE,@Spark}]}]
22:57:16.278 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - environment/json->[{GzipHandler@56ef900d{STARTED},[o.s.j.s.ServletContextHandler@3501a276{/environment/json,null,AVAILABLE,@Spark}]}]
22:57:16.278 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/job/json->[{GzipHandler@469e8d9e{STARTED},[o.s.j.s.ServletContextHandler@56d07c59{/jobs/job/json,null,AVAILABLE,@Spark}]}]
22:57:16.278 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs->[{GzipHandler@3ae6a9f6{STARTED},[o.s.j.s.ServletContextHandler@5d2d691a{/jobs,null,AVAILABLE,@Spark}]}]
22:57:16.279 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/json->[{GzipHandler@44284db4{STARTED},[o.s.j.s.ServletContextHandler@edcc350{/stages/json,null,AVAILABLE,@Spark}]}]
22:57:16.279 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/stage->[{GzipHandler@31c8e2ae{STARTED},[o.s.j.s.ServletContextHandler@3cdd11cc{/stages/stage,null,AVAILABLE,@Spark}]}]
22:57:16.279 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage/json->[{GzipHandler@9ae7eb5{STARTED},[o.s.j.s.ServletContextHandler@5288383e{/storage/json,null,AVAILABLE,@Spark}]}]
22:57:16.279 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - SQL->[{GzipHandler@78f97207{STOPPED},[o.s.j.s.ServletContextHandler@6ec01c04{/SQL,null,UNAVAILABLE,@Spark}]}]
22:57:16.279 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/stage/kill->[{GzipHandler@7afc93c1{STARTED},[o.s.j.s.ServletContextHandler@7052620d{/stages/stage/kill,null,AVAILABLE,@Spark}]}]
22:57:16.279 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/job->[{GzipHandler@171f458{STARTED},[o.s.j.s.ServletContextHandler@1fdad46b{/jobs/job,null,AVAILABLE,@Spark}]}]
22:57:16.279 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - environment->[{GzipHandler@2762a443{STARTED},[o.s.j.s.ServletContextHandler@73adebf8{/environment,null,AVAILABLE,@Spark}]}]
22:57:16.279 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages->[{GzipHandler@4f73ea1c{STARTED},[o.s.j.s.ServletContextHandler@5e077920{/stages,null,AVAILABLE,@Spark}]}]
22:57:16.279 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - executors->[{GzipHandler@2539b24e{STARTED},[o.s.j.s.ServletContextHandler@328c8921{/executors,null,AVAILABLE,@Spark}]}]
22:57:16.279 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/job/kill->[{GzipHandler@4d5f6e4f{STARTED},[o.s.j.s.ServletContextHandler@6cd59fdc{/jobs/job/kill,null,AVAILABLE,@Spark}]}]
22:57:16.279 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - metrics/json->[{GzipHandler@684f5877{STARTED},[o.s.j.s.ServletContextHandler@6156560{/metrics/json,null,AVAILABLE,@Spark}]}]
22:57:16.279 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - executors/threadDump->[{GzipHandler@30592f62{STARTED},[o.s.j.s.ServletContextHandler@7779b1d5{/executors/threadDump,null,AVAILABLE,@Spark}]}]
22:57:16.279 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - ContextHandlerCollection@7d170ea6{STARTED} added {GzipHandler@78f97207{STOPPED},UNMANAGED}
22:57:16.279 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting o.s.j.s.ServletContextHandler@6ec01c04{/SQL,null,UNAVAILABLE,@Spark}
22:57:16.280 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting o.s.j.s.ServletContextHandler@6ec01c04{/SQL,null,STARTING,@Spark}
22:57:16.281 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting ServletHandler@388e756f{STOPPED}
22:57:16.281 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$1-89ed954[EMBEDDED:null]
22:57:16.281 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.http.pathmap.PathMappings - Added MappedResource[pathSpec=ServletPathSpec["/",pathDepth=-1,group=DEFAULT],resource=org.apache.spark.ui.JettyUtils$$anon$1-89ed954@abb44fca==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true] to PathMappings[size=1]
22:57:16.281 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - filterNameMap={org.apache.spark.ui.HttpSecurityFilter-252a7f4f=org.apache.spark.ui.HttpSecurityFilter-252a7f4f@252a7f4f==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true}
22:57:16.282 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - pathFilters=[[/*]/[]/[INCLUDE, REQUEST, ASYNC, FORWARD, ERROR]=>org.apache.spark.ui.HttpSecurityFilter-252a7f4f]
22:57:16.282 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletFilterMap={}
22:57:16.282 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletPathMap=PathMappings[size=1]
22:57:16.282 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$1-89ed954=org.apache.spark.ui.JettyUtils$$anon$1-89ed954@abb44fca==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true}
22:57:16.282 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting ServletHandler@388e756f{STARTING}
22:57:16.282 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @13055ms ServletHandler@388e756f{STARTED}
22:57:16.282 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting org.apache.spark.ui.HttpSecurityFilter-252a7f4f@252a7f4f==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true
22:57:16.282 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @13055ms org.apache.spark.ui.HttpSecurityFilter-252a7f4f@252a7f4f==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true
22:57:16.282 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.FilterHolder - Filter.init org.apache.spark.ui.HttpSecurityFilter@12f1c18c
22:57:16.282 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting org.apache.spark.ui.JettyUtils$$anon$1-89ed954@abb44fca==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true
22:57:16.282 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @13055ms org.apache.spark.ui.JettyUtils$$anon$1-89ed954@abb44fca==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true
22:57:16.282 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHolder - Servlet.init org.apache.spark.ui.JettyUtils$$anon$1@79336a21 for org.apache.spark.ui.JettyUtils$$anon$1-89ed954
22:57:16.282 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@6ec01c04{/SQL,null,AVAILABLE,@Spark}
22:57:16.282 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @13055ms o.s.j.s.ServletContextHandler@6ec01c04{/SQL,null,AVAILABLE,@Spark}
22:57:16.282 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting GzipHandler@78f97207{STOPPED}
22:57:16.282 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting GzipHandler@78f97207{STARTING}
22:57:16.282 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @13056ms GzipHandler@78f97207{STARTED}
22:57:16.283 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.http.pathmap.PathMappings - Added MappedResource[pathSpec=ServletPathSpec["*.svgz",pathDepth=0,group=SUFFIX_GLOB],resource=true] to PathMappings[size=1]
22:57:16.283 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.gzip.GzipHandler - GzipHandler@393833d0{STOPPED} mime types IncludeExclude@255978da{i=[],ip=org.sparkproject.jetty.util.IncludeExcludeSet$SetContainsPredicate@47d97c32,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=org.sparkproject.jetty.util.IncludeExcludeSet$SetContainsPredicate@539f8ff9}
22:57:16.283 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - GzipHandler@393833d0{STOPPED} added {o.s.j.s.ServletContextHandler@5331c7fa{/SQL/json,null,UNAVAILABLE,@Spark},MANAGED}
22:57:16.283 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - ->[{GzipHandler@47b4fed9{STARTED},[o.s.j.s.ServletContextHandler@19db68e6{/,null,AVAILABLE,@Spark}]}]
22:57:16.283 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage/rdd->[{GzipHandler@1b24f851{STARTED},[o.s.j.s.ServletContextHandler@4320206d{/storage/rdd,null,AVAILABLE,@Spark}]}]
22:57:16.283 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage->[{GzipHandler@1dec257e{STARTED},[o.s.j.s.ServletContextHandler@49d00b2a{/storage,null,AVAILABLE,@Spark}]}]
22:57:16.283 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage/rdd/json->[{GzipHandler@31e69a85{STARTED},[o.s.j.s.ServletContextHandler@1ce6aa02{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
22:57:16.283 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - api->[{GzipHandler@6c495083{STARTED},[o.s.j.s.ServletContextHandler@24d35d46{/api,null,AVAILABLE,@Spark}]}]
22:57:16.283 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/pool/json->[{GzipHandler@76f330b6{STARTED},[o.s.j.s.ServletContextHandler@6257b05c{/stages/pool/json,null,AVAILABLE,@Spark}]}]
22:57:16.283 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/pool->[{GzipHandler@532f7fe8{STARTED},[o.s.j.s.ServletContextHandler@50be7065{/stages/pool,null,AVAILABLE,@Spark}]}]
22:57:16.283 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/json->[{GzipHandler@2c462871{STARTED},[o.s.j.s.ServletContextHandler@6b382dd6{/jobs/json,null,AVAILABLE,@Spark}]}]
22:57:16.284 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - static->[{GzipHandler@670c9f4c{STARTED},[o.s.j.s.ServletContextHandler@1c5e9364{/static,null,AVAILABLE,@Spark}]}]
22:57:16.284 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - executors/json->[{GzipHandler@2dc1fa11{STARTED},[o.s.j.s.ServletContextHandler@5e87276a{/executors/json,null,AVAILABLE,@Spark}]}]
22:57:16.284 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/stage/json->[{GzipHandler@30a40be{STARTED},[o.s.j.s.ServletContextHandler@6def4b33{/stages/stage/json,null,AVAILABLE,@Spark}]}]
22:57:16.284 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - executors/threadDump/json->[{GzipHandler@395860fa{STARTED},[o.s.j.s.ServletContextHandler@5ca18a1d{/executors/threadDump/json,null,AVAILABLE,@Spark}]}]
22:57:16.284 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - environment/json->[{GzipHandler@56ef900d{STARTED},[o.s.j.s.ServletContextHandler@3501a276{/environment/json,null,AVAILABLE,@Spark}]}]
22:57:16.284 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/job/json->[{GzipHandler@469e8d9e{STARTED},[o.s.j.s.ServletContextHandler@56d07c59{/jobs/job/json,null,AVAILABLE,@Spark}]}]
22:57:16.284 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs->[{GzipHandler@3ae6a9f6{STARTED},[o.s.j.s.ServletContextHandler@5d2d691a{/jobs,null,AVAILABLE,@Spark}]}]
22:57:16.284 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/json->[{GzipHandler@44284db4{STARTED},[o.s.j.s.ServletContextHandler@edcc350{/stages/json,null,AVAILABLE,@Spark}]}]
22:57:16.284 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/stage->[{GzipHandler@31c8e2ae{STARTED},[o.s.j.s.ServletContextHandler@3cdd11cc{/stages/stage,null,AVAILABLE,@Spark}]}]
22:57:16.284 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage/json->[{GzipHandler@9ae7eb5{STARTED},[o.s.j.s.ServletContextHandler@5288383e{/storage/json,null,AVAILABLE,@Spark}]}]
22:57:16.284 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - SQL->[{GzipHandler@78f97207{STARTED},[o.s.j.s.ServletContextHandler@6ec01c04{/SQL,null,AVAILABLE,@Spark}]}]
22:57:16.284 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/stage/kill->[{GzipHandler@7afc93c1{STARTED},[o.s.j.s.ServletContextHandler@7052620d{/stages/stage/kill,null,AVAILABLE,@Spark}]}]
22:57:16.284 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/job->[{GzipHandler@171f458{STARTED},[o.s.j.s.ServletContextHandler@1fdad46b{/jobs/job,null,AVAILABLE,@Spark}]}]
22:57:16.284 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - environment->[{GzipHandler@2762a443{STARTED},[o.s.j.s.ServletContextHandler@73adebf8{/environment,null,AVAILABLE,@Spark}]}]
22:57:16.284 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages->[{GzipHandler@4f73ea1c{STARTED},[o.s.j.s.ServletContextHandler@5e077920{/stages,null,AVAILABLE,@Spark}]}]
22:57:16.284 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - executors->[{GzipHandler@2539b24e{STARTED},[o.s.j.s.ServletContextHandler@328c8921{/executors,null,AVAILABLE,@Spark}]}]
22:57:16.284 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - SQL/json->[{GzipHandler@393833d0{STOPPED},[o.s.j.s.ServletContextHandler@5331c7fa{/SQL/json,null,UNAVAILABLE,@Spark}]}]
22:57:16.284 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/job/kill->[{GzipHandler@4d5f6e4f{STARTED},[o.s.j.s.ServletContextHandler@6cd59fdc{/jobs/job/kill,null,AVAILABLE,@Spark}]}]
22:57:16.284 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - metrics/json->[{GzipHandler@684f5877{STARTED},[o.s.j.s.ServletContextHandler@6156560{/metrics/json,null,AVAILABLE,@Spark}]}]
22:57:16.284 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - executors/threadDump->[{GzipHandler@30592f62{STARTED},[o.s.j.s.ServletContextHandler@7779b1d5{/executors/threadDump,null,AVAILABLE,@Spark}]}]
22:57:16.284 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - ContextHandlerCollection@7d170ea6{STARTED} added {GzipHandler@393833d0{STOPPED},UNMANAGED}
22:57:16.284 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting o.s.j.s.ServletContextHandler@5331c7fa{/SQL/json,null,UNAVAILABLE,@Spark}
22:57:16.285 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting o.s.j.s.ServletContextHandler@5331c7fa{/SQL/json,null,STARTING,@Spark}
22:57:16.285 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting ServletHandler@18dfe5fe{STOPPED}
22:57:16.285 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$1-66f5dbc8[EMBEDDED:null]
22:57:16.286 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.http.pathmap.PathMappings - Added MappedResource[pathSpec=ServletPathSpec["/",pathDepth=-1,group=DEFAULT],resource=org.apache.spark.ui.JettyUtils$$anon$1-66f5dbc8@5b9360b0==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true] to PathMappings[size=1]
22:57:16.286 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - filterNameMap={org.apache.spark.ui.HttpSecurityFilter-475a7109=org.apache.spark.ui.HttpSecurityFilter-475a7109@475a7109==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true}
22:57:16.286 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - pathFilters=[[/*]/[]/[INCLUDE, REQUEST, ASYNC, FORWARD, ERROR]=>org.apache.spark.ui.HttpSecurityFilter-475a7109]
22:57:16.286 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletFilterMap={}
22:57:16.286 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletPathMap=PathMappings[size=1]
22:57:16.286 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$1-66f5dbc8=org.apache.spark.ui.JettyUtils$$anon$1-66f5dbc8@5b9360b0==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true}
22:57:16.286 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting ServletHandler@18dfe5fe{STARTING}
22:57:16.286 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @13059ms ServletHandler@18dfe5fe{STARTED}
22:57:16.286 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting org.apache.spark.ui.HttpSecurityFilter-475a7109@475a7109==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true
22:57:16.286 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @13059ms org.apache.spark.ui.HttpSecurityFilter-475a7109@475a7109==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true
22:57:16.286 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.FilterHolder - Filter.init org.apache.spark.ui.HttpSecurityFilter@2b4190e9
22:57:16.286 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting org.apache.spark.ui.JettyUtils$$anon$1-66f5dbc8@5b9360b0==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true
22:57:16.286 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @13059ms org.apache.spark.ui.JettyUtils$$anon$1-66f5dbc8@5b9360b0==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true
22:57:16.286 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHolder - Servlet.init org.apache.spark.ui.JettyUtils$$anon$1@4530a2f3 for org.apache.spark.ui.JettyUtils$$anon$1-66f5dbc8
22:57:16.286 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@5331c7fa{/SQL/json,null,AVAILABLE,@Spark}
22:57:16.286 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @13059ms o.s.j.s.ServletContextHandler@5331c7fa{/SQL/json,null,AVAILABLE,@Spark}
22:57:16.286 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting GzipHandler@393833d0{STOPPED}
22:57:16.286 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting GzipHandler@393833d0{STARTING}
22:57:16.286 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @13060ms GzipHandler@393833d0{STARTED}
22:57:16.287 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.DecoratedObjectFactory - Adding Decorator: org.sparkproject.jetty.util.DeprecationWarning@664bd015
22:57:16.287 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - o.s.j.s.ServletContextHandler@41aac2ed{/,null,UNAVAILABLE} added {ServletHandler@1cdd711{STOPPED},MANAGED}
22:57:16.287 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.DecoratedObjectFactory - Adding Decorator: org.sparkproject.jetty.util.DeprecationWarning@53a43f92
22:57:16.287 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - o.s.j.s.ServletContextHandler@440e32a1{/,null,UNAVAILABLE} added {ServletHandler@2b14006e{STOPPED},MANAGED}
22:57:16.287 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.http.pathmap.PathMappings - Added MappedResource[pathSpec=ServletPathSpec["*.svgz",pathDepth=0,group=SUFFIX_GLOB],resource=true] to PathMappings[size=1]
22:57:16.287 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.gzip.GzipHandler - GzipHandler@3b1cbf53{STOPPED} mime types IncludeExclude@3000cf2e{i=[],ip=org.sparkproject.jetty.util.IncludeExcludeSet$SetContainsPredicate@b1e0b79,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=org.sparkproject.jetty.util.IncludeExcludeSet$SetContainsPredicate@154abf7b}
22:57:16.288 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - GzipHandler@3b1cbf53{STOPPED} added {o.s.j.s.ServletContextHandler@41aac2ed{/SQL/execution,null,UNAVAILABLE,@Spark},MANAGED}
22:57:16.289 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - ->[{GzipHandler@47b4fed9{STARTED},[o.s.j.s.ServletContextHandler@19db68e6{/,null,AVAILABLE,@Spark}]}]
22:57:16.289 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage/rdd->[{GzipHandler@1b24f851{STARTED},[o.s.j.s.ServletContextHandler@4320206d{/storage/rdd,null,AVAILABLE,@Spark}]}]
22:57:16.289 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage->[{GzipHandler@1dec257e{STARTED},[o.s.j.s.ServletContextHandler@49d00b2a{/storage,null,AVAILABLE,@Spark}]}]
22:57:16.289 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage/rdd/json->[{GzipHandler@31e69a85{STARTED},[o.s.j.s.ServletContextHandler@1ce6aa02{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
22:57:16.289 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - api->[{GzipHandler@6c495083{STARTED},[o.s.j.s.ServletContextHandler@24d35d46{/api,null,AVAILABLE,@Spark}]}]
22:57:16.289 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/pool/json->[{GzipHandler@76f330b6{STARTED},[o.s.j.s.ServletContextHandler@6257b05c{/stages/pool/json,null,AVAILABLE,@Spark}]}]
22:57:16.289 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/pool->[{GzipHandler@532f7fe8{STARTED},[o.s.j.s.ServletContextHandler@50be7065{/stages/pool,null,AVAILABLE,@Spark}]}]
22:57:16.289 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/json->[{GzipHandler@2c462871{STARTED},[o.s.j.s.ServletContextHandler@6b382dd6{/jobs/json,null,AVAILABLE,@Spark}]}]
22:57:16.289 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - static->[{GzipHandler@670c9f4c{STARTED},[o.s.j.s.ServletContextHandler@1c5e9364{/static,null,AVAILABLE,@Spark}]}]
22:57:16.289 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - executors/json->[{GzipHandler@2dc1fa11{STARTED},[o.s.j.s.ServletContextHandler@5e87276a{/executors/json,null,AVAILABLE,@Spark}]}]
22:57:16.289 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/stage/json->[{GzipHandler@30a40be{STARTED},[o.s.j.s.ServletContextHandler@6def4b33{/stages/stage/json,null,AVAILABLE,@Spark}]}]
22:57:16.289 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - executors/threadDump/json->[{GzipHandler@395860fa{STARTED},[o.s.j.s.ServletContextHandler@5ca18a1d{/executors/threadDump/json,null,AVAILABLE,@Spark}]}]
22:57:16.289 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - environment/json->[{GzipHandler@56ef900d{STARTED},[o.s.j.s.ServletContextHandler@3501a276{/environment/json,null,AVAILABLE,@Spark}]}]
22:57:16.289 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/job/json->[{GzipHandler@469e8d9e{STARTED},[o.s.j.s.ServletContextHandler@56d07c59{/jobs/job/json,null,AVAILABLE,@Spark}]}]
22:57:16.289 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs->[{GzipHandler@3ae6a9f6{STARTED},[o.s.j.s.ServletContextHandler@5d2d691a{/jobs,null,AVAILABLE,@Spark}]}]
22:57:16.289 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/json->[{GzipHandler@44284db4{STARTED},[o.s.j.s.ServletContextHandler@edcc350{/stages/json,null,AVAILABLE,@Spark}]}]
22:57:16.289 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/stage->[{GzipHandler@31c8e2ae{STARTED},[o.s.j.s.ServletContextHandler@3cdd11cc{/stages/stage,null,AVAILABLE,@Spark}]}]
22:57:16.290 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage/json->[{GzipHandler@9ae7eb5{STARTED},[o.s.j.s.ServletContextHandler@5288383e{/storage/json,null,AVAILABLE,@Spark}]}]
22:57:16.290 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - SQL->[{GzipHandler@78f97207{STARTED},[o.s.j.s.ServletContextHandler@6ec01c04{/SQL,null,AVAILABLE,@Spark}]}]
22:57:16.290 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/stage/kill->[{GzipHandler@7afc93c1{STARTED},[o.s.j.s.ServletContextHandler@7052620d{/stages/stage/kill,null,AVAILABLE,@Spark}]}]
22:57:16.290 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/job->[{GzipHandler@171f458{STARTED},[o.s.j.s.ServletContextHandler@1fdad46b{/jobs/job,null,AVAILABLE,@Spark}]}]
22:57:16.290 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - environment->[{GzipHandler@2762a443{STARTED},[o.s.j.s.ServletContextHandler@73adebf8{/environment,null,AVAILABLE,@Spark}]}]
22:57:16.290 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages->[{GzipHandler@4f73ea1c{STARTED},[o.s.j.s.ServletContextHandler@5e077920{/stages,null,AVAILABLE,@Spark}]}]
22:57:16.290 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - executors->[{GzipHandler@2539b24e{STARTED},[o.s.j.s.ServletContextHandler@328c8921{/executors,null,AVAILABLE,@Spark}]}]
22:57:16.290 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - SQL/json->[{GzipHandler@393833d0{STARTED},[o.s.j.s.ServletContextHandler@5331c7fa{/SQL/json,null,AVAILABLE,@Spark}]}]
22:57:16.290 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/job/kill->[{GzipHandler@4d5f6e4f{STARTED},[o.s.j.s.ServletContextHandler@6cd59fdc{/jobs/job/kill,null,AVAILABLE,@Spark}]}]
22:57:16.290 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - metrics/json->[{GzipHandler@684f5877{STARTED},[o.s.j.s.ServletContextHandler@6156560{/metrics/json,null,AVAILABLE,@Spark}]}]
22:57:16.290 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - SQL/execution->[{GzipHandler@3b1cbf53{STOPPED},[o.s.j.s.ServletContextHandler@41aac2ed{/SQL/execution,null,UNAVAILABLE,@Spark}]}]
22:57:16.290 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - executors/threadDump->[{GzipHandler@30592f62{STARTED},[o.s.j.s.ServletContextHandler@7779b1d5{/executors/threadDump,null,AVAILABLE,@Spark}]}]
22:57:16.290 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - ContextHandlerCollection@7d170ea6{STARTED} added {GzipHandler@3b1cbf53{STOPPED},UNMANAGED}
22:57:16.290 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting o.s.j.s.ServletContextHandler@41aac2ed{/SQL/execution,null,UNAVAILABLE,@Spark}
22:57:16.290 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting o.s.j.s.ServletContextHandler@41aac2ed{/SQL/execution,null,STARTING,@Spark}
22:57:16.290 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting ServletHandler@1cdd711{STOPPED}
22:57:16.291 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$1-6b579195[EMBEDDED:null]
22:57:16.291 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.http.pathmap.PathMappings - Added MappedResource[pathSpec=ServletPathSpec["/",pathDepth=-1,group=DEFAULT],resource=org.apache.spark.ui.JettyUtils$$anon$1-6b579195@1f8fff70==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true] to PathMappings[size=1]
22:57:16.291 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - filterNameMap={org.apache.spark.ui.HttpSecurityFilter-e321f0e=org.apache.spark.ui.HttpSecurityFilter-e321f0e@e321f0e==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true}
22:57:16.291 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - pathFilters=[[/*]/[]/[INCLUDE, REQUEST, ASYNC, FORWARD, ERROR]=>org.apache.spark.ui.HttpSecurityFilter-e321f0e]
22:57:16.291 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletFilterMap={}
22:57:16.291 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletPathMap=PathMappings[size=1]
22:57:16.291 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$1-6b579195=org.apache.spark.ui.JettyUtils$$anon$1-6b579195@1f8fff70==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true}
22:57:16.291 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting ServletHandler@1cdd711{STARTING}
22:57:16.291 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @13064ms ServletHandler@1cdd711{STARTED}
22:57:16.291 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting org.apache.spark.ui.HttpSecurityFilter-e321f0e@e321f0e==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true
22:57:16.291 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @13064ms org.apache.spark.ui.HttpSecurityFilter-e321f0e@e321f0e==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true
22:57:16.291 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.FilterHolder - Filter.init org.apache.spark.ui.HttpSecurityFilter@4bf27ca6
22:57:16.291 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting org.apache.spark.ui.JettyUtils$$anon$1-6b579195@1f8fff70==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true
22:57:16.291 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @13064ms org.apache.spark.ui.JettyUtils$$anon$1-6b579195@1f8fff70==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true
22:57:16.291 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHolder - Servlet.init org.apache.spark.ui.JettyUtils$$anon$1@7d3db349 for org.apache.spark.ui.JettyUtils$$anon$1-6b579195
22:57:16.291 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@41aac2ed{/SQL/execution,null,AVAILABLE,@Spark}
22:57:16.291 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @13064ms o.s.j.s.ServletContextHandler@41aac2ed{/SQL/execution,null,AVAILABLE,@Spark}
22:57:16.291 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting GzipHandler@3b1cbf53{STOPPED}
22:57:16.291 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting GzipHandler@3b1cbf53{STARTING}
22:57:16.291 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @13064ms GzipHandler@3b1cbf53{STARTED}
22:57:16.293 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.http.pathmap.PathMappings - Added MappedResource[pathSpec=ServletPathSpec["*.svgz",pathDepth=0,group=SUFFIX_GLOB],resource=true] to PathMappings[size=1]
22:57:16.293 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.gzip.GzipHandler - GzipHandler@46f2ec88{STOPPED} mime types IncludeExclude@1e3dafbc{i=[],ip=org.sparkproject.jetty.util.IncludeExcludeSet$SetContainsPredicate@e8af0c9,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=org.sparkproject.jetty.util.IncludeExcludeSet$SetContainsPredicate@11442ac6}
22:57:16.294 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - GzipHandler@46f2ec88{STOPPED} added {o.s.j.s.ServletContextHandler@440e32a1{/SQL/execution/json,null,UNAVAILABLE,@Spark},MANAGED}
22:57:16.294 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - ->[{GzipHandler@47b4fed9{STARTED},[o.s.j.s.ServletContextHandler@19db68e6{/,null,AVAILABLE,@Spark}]}]
22:57:16.294 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage/rdd->[{GzipHandler@1b24f851{STARTED},[o.s.j.s.ServletContextHandler@4320206d{/storage/rdd,null,AVAILABLE,@Spark}]}]
22:57:16.294 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage->[{GzipHandler@1dec257e{STARTED},[o.s.j.s.ServletContextHandler@49d00b2a{/storage,null,AVAILABLE,@Spark}]}]
22:57:16.294 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage/rdd/json->[{GzipHandler@31e69a85{STARTED},[o.s.j.s.ServletContextHandler@1ce6aa02{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
22:57:16.294 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - SQL/execution/json->[{GzipHandler@46f2ec88{STOPPED},[o.s.j.s.ServletContextHandler@440e32a1{/SQL/execution/json,null,UNAVAILABLE,@Spark}]}]
22:57:16.294 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - api->[{GzipHandler@6c495083{STARTED},[o.s.j.s.ServletContextHandler@24d35d46{/api,null,AVAILABLE,@Spark}]}]
22:57:16.294 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/pool/json->[{GzipHandler@76f330b6{STARTED},[o.s.j.s.ServletContextHandler@6257b05c{/stages/pool/json,null,AVAILABLE,@Spark}]}]
22:57:16.294 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/pool->[{GzipHandler@532f7fe8{STARTED},[o.s.j.s.ServletContextHandler@50be7065{/stages/pool,null,AVAILABLE,@Spark}]}]
22:57:16.294 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/json->[{GzipHandler@2c462871{STARTED},[o.s.j.s.ServletContextHandler@6b382dd6{/jobs/json,null,AVAILABLE,@Spark}]}]
22:57:16.294 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - static->[{GzipHandler@670c9f4c{STARTED},[o.s.j.s.ServletContextHandler@1c5e9364{/static,null,AVAILABLE,@Spark}]}]
22:57:16.294 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - executors/json->[{GzipHandler@2dc1fa11{STARTED},[o.s.j.s.ServletContextHandler@5e87276a{/executors/json,null,AVAILABLE,@Spark}]}]
22:57:16.294 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/stage/json->[{GzipHandler@30a40be{STARTED},[o.s.j.s.ServletContextHandler@6def4b33{/stages/stage/json,null,AVAILABLE,@Spark}]}]
22:57:16.295 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - executors/threadDump/json->[{GzipHandler@395860fa{STARTED},[o.s.j.s.ServletContextHandler@5ca18a1d{/executors/threadDump/json,null,AVAILABLE,@Spark}]}]
22:57:16.295 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - environment/json->[{GzipHandler@56ef900d{STARTED},[o.s.j.s.ServletContextHandler@3501a276{/environment/json,null,AVAILABLE,@Spark}]}]
22:57:16.295 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/job/json->[{GzipHandler@469e8d9e{STARTED},[o.s.j.s.ServletContextHandler@56d07c59{/jobs/job/json,null,AVAILABLE,@Spark}]}]
22:57:16.295 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs->[{GzipHandler@3ae6a9f6{STARTED},[o.s.j.s.ServletContextHandler@5d2d691a{/jobs,null,AVAILABLE,@Spark}]}]
22:57:16.295 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/json->[{GzipHandler@44284db4{STARTED},[o.s.j.s.ServletContextHandler@edcc350{/stages/json,null,AVAILABLE,@Spark}]}]
22:57:16.295 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/stage->[{GzipHandler@31c8e2ae{STARTED},[o.s.j.s.ServletContextHandler@3cdd11cc{/stages/stage,null,AVAILABLE,@Spark}]}]
22:57:16.295 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage/json->[{GzipHandler@9ae7eb5{STARTED},[o.s.j.s.ServletContextHandler@5288383e{/storage/json,null,AVAILABLE,@Spark}]}]
22:57:16.295 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - SQL->[{GzipHandler@78f97207{STARTED},[o.s.j.s.ServletContextHandler@6ec01c04{/SQL,null,AVAILABLE,@Spark}]}]
22:57:16.295 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/stage/kill->[{GzipHandler@7afc93c1{STARTED},[o.s.j.s.ServletContextHandler@7052620d{/stages/stage/kill,null,AVAILABLE,@Spark}]}]
22:57:16.295 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/job->[{GzipHandler@171f458{STARTED},[o.s.j.s.ServletContextHandler@1fdad46b{/jobs/job,null,AVAILABLE,@Spark}]}]
22:57:16.295 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - environment->[{GzipHandler@2762a443{STARTED},[o.s.j.s.ServletContextHandler@73adebf8{/environment,null,AVAILABLE,@Spark}]}]
22:57:16.295 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages->[{GzipHandler@4f73ea1c{STARTED},[o.s.j.s.ServletContextHandler@5e077920{/stages,null,AVAILABLE,@Spark}]}]
22:57:16.295 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - executors->[{GzipHandler@2539b24e{STARTED},[o.s.j.s.ServletContextHandler@328c8921{/executors,null,AVAILABLE,@Spark}]}]
22:57:16.295 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - SQL/json->[{GzipHandler@393833d0{STARTED},[o.s.j.s.ServletContextHandler@5331c7fa{/SQL/json,null,AVAILABLE,@Spark}]}]
22:57:16.295 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/job/kill->[{GzipHandler@4d5f6e4f{STARTED},[o.s.j.s.ServletContextHandler@6cd59fdc{/jobs/job/kill,null,AVAILABLE,@Spark}]}]
22:57:16.295 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - metrics/json->[{GzipHandler@684f5877{STARTED},[o.s.j.s.ServletContextHandler@6156560{/metrics/json,null,AVAILABLE,@Spark}]}]
22:57:16.295 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - SQL/execution->[{GzipHandler@3b1cbf53{STARTED},[o.s.j.s.ServletContextHandler@41aac2ed{/SQL/execution,null,AVAILABLE,@Spark}]}]
22:57:16.295 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - executors/threadDump->[{GzipHandler@30592f62{STARTED},[o.s.j.s.ServletContextHandler@7779b1d5{/executors/threadDump,null,AVAILABLE,@Spark}]}]
22:57:16.295 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - ContextHandlerCollection@7d170ea6{STARTED} added {GzipHandler@46f2ec88{STOPPED},UNMANAGED}
22:57:16.295 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting o.s.j.s.ServletContextHandler@440e32a1{/SQL/execution/json,null,UNAVAILABLE,@Spark}
22:57:16.295 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting o.s.j.s.ServletContextHandler@440e32a1{/SQL/execution/json,null,STARTING,@Spark}
22:57:16.295 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting ServletHandler@2b14006e{STOPPED}
22:57:16.295 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - Path=/[EMBEDDED:null] mapped to servlet=org.apache.spark.ui.JettyUtils$$anon$1-7c98f7a0[EMBEDDED:null]
22:57:16.296 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.http.pathmap.PathMappings - Added MappedResource[pathSpec=ServletPathSpec["/",pathDepth=-1,group=DEFAULT],resource=org.apache.spark.ui.JettyUtils$$anon$1-7c98f7a0@c34d7199==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true] to PathMappings[size=1]
22:57:16.296 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - filterNameMap={org.apache.spark.ui.HttpSecurityFilter-6032a54=org.apache.spark.ui.HttpSecurityFilter-6032a54@6032a54==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true}
22:57:16.296 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - pathFilters=[[/*]/[]/[INCLUDE, REQUEST, ASYNC, FORWARD, ERROR]=>org.apache.spark.ui.HttpSecurityFilter-6032a54]
22:57:16.296 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletFilterMap={}
22:57:16.296 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletPathMap=PathMappings[size=1]
22:57:16.296 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletNameMap={org.apache.spark.ui.JettyUtils$$anon$1-7c98f7a0=org.apache.spark.ui.JettyUtils$$anon$1-7c98f7a0@c34d7199==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true}
22:57:16.296 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting ServletHandler@2b14006e{STARTING}
22:57:16.296 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @13069ms ServletHandler@2b14006e{STARTED}
22:57:16.296 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting org.apache.spark.ui.HttpSecurityFilter-6032a54@6032a54==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true
22:57:16.296 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @13069ms org.apache.spark.ui.HttpSecurityFilter-6032a54@6032a54==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true
22:57:16.296 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.FilterHolder - Filter.init org.apache.spark.ui.HttpSecurityFilter@408302ca
22:57:16.296 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting org.apache.spark.ui.JettyUtils$$anon$1-7c98f7a0@c34d7199==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true
22:57:16.296 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @13069ms org.apache.spark.ui.JettyUtils$$anon$1-7c98f7a0@c34d7199==org.apache.spark.ui.JettyUtils$$anon$1,jsp=null,order=-1,inst=true,async=true
22:57:16.296 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHolder - Servlet.init org.apache.spark.ui.JettyUtils$$anon$1@33301529 for org.apache.spark.ui.JettyUtils$$anon$1-7c98f7a0
22:57:16.296 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@440e32a1{/SQL/execution/json,null,AVAILABLE,@Spark}
22:57:16.296 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @13069ms o.s.j.s.ServletContextHandler@440e32a1{/SQL/execution/json,null,AVAILABLE,@Spark}
22:57:16.296 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting GzipHandler@46f2ec88{STOPPED}
22:57:16.296 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting GzipHandler@46f2ec88{STARTING}
22:57:16.296 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @13070ms GzipHandler@46f2ec88{STARTED}
22:57:16.297 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.DecoratedObjectFactory - Adding Decorator: org.sparkproject.jetty.util.DeprecationWarning@76bba716
22:57:16.297 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - o.s.j.s.ServletContextHandler@118eb9bd{/,null,UNAVAILABLE} added {ServletHandler@40a6ccec{STOPPED},MANAGED}
22:57:16.298 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.http.pathmap.PathMappings - Added MappedResource[pathSpec=ServletPathSpec["*.svgz",pathDepth=0,group=SUFFIX_GLOB],resource=true] to PathMappings[size=1]
22:57:16.298 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.gzip.GzipHandler - GzipHandler@3befd7c9{STOPPED} mime types IncludeExclude@4d58adf4{i=[],ip=org.sparkproject.jetty.util.IncludeExcludeSet$SetContainsPredicate@3b6caad2,e=[image/ief, image/vnd.wap.wbmp, image/jpeg, application/bzip2, image/x-portable-graymap, application/brotli, image/bmp, image/gif, image/x-icon, audio/midi, video/x-msvideo, image/x-xbitmap, application/x-rar-compressed, image/x-portable-bitmap, image/x-rgb, image/x-cmu-raster, application/gzip, audio/x-wav, audio/x-pn-realaudio, audio/basic, application/compress, audio/x-aiff, video/x.ms.asx, video/x.ms.asf, image/png, video/vnd.rn-realvideo, image/x-xwindowdump, image/jpeg2000, video/x-sgi-movie, audio/mpeg, image/xcf, video/mpeg, image/x-portable-pixmap, image/tiff, image/x-portable-anymap, image/x-xpixmap, application/zip, video/quicktime, application/x-xz, video/mp4],ep=org.sparkproject.jetty.util.IncludeExcludeSet$SetContainsPredicate@22cbad5d}
22:57:16.299 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - GzipHandler@3befd7c9{STOPPED} added {o.s.j.s.ServletContextHandler@118eb9bd{/static/sql,null,UNAVAILABLE,@Spark},MANAGED}
22:57:16.299 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - ->[{GzipHandler@47b4fed9{STARTED},[o.s.j.s.ServletContextHandler@19db68e6{/,null,AVAILABLE,@Spark}]}]
22:57:16.299 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage/rdd->[{GzipHandler@1b24f851{STARTED},[o.s.j.s.ServletContextHandler@4320206d{/storage/rdd,null,AVAILABLE,@Spark}]}]
22:57:16.299 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage->[{GzipHandler@1dec257e{STARTED},[o.s.j.s.ServletContextHandler@49d00b2a{/storage,null,AVAILABLE,@Spark}]}]
22:57:16.299 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage/rdd/json->[{GzipHandler@31e69a85{STARTED},[o.s.j.s.ServletContextHandler@1ce6aa02{/storage/rdd/json,null,AVAILABLE,@Spark}]}]
22:57:16.299 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - SQL/execution/json->[{GzipHandler@46f2ec88{STARTED},[o.s.j.s.ServletContextHandler@440e32a1{/SQL/execution/json,null,AVAILABLE,@Spark}]}]
22:57:16.299 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - api->[{GzipHandler@6c495083{STARTED},[o.s.j.s.ServletContextHandler@24d35d46{/api,null,AVAILABLE,@Spark}]}]
22:57:16.299 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/pool/json->[{GzipHandler@76f330b6{STARTED},[o.s.j.s.ServletContextHandler@6257b05c{/stages/pool/json,null,AVAILABLE,@Spark}]}]
22:57:16.299 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/pool->[{GzipHandler@532f7fe8{STARTED},[o.s.j.s.ServletContextHandler@50be7065{/stages/pool,null,AVAILABLE,@Spark}]}]
22:57:16.299 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/json->[{GzipHandler@2c462871{STARTED},[o.s.j.s.ServletContextHandler@6b382dd6{/jobs/json,null,AVAILABLE,@Spark}]}]
22:57:16.299 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - static->[{GzipHandler@670c9f4c{STARTED},[o.s.j.s.ServletContextHandler@1c5e9364{/static,null,AVAILABLE,@Spark}]}]
22:57:16.299 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - executors/json->[{GzipHandler@2dc1fa11{STARTED},[o.s.j.s.ServletContextHandler@5e87276a{/executors/json,null,AVAILABLE,@Spark}]}]
22:57:16.299 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/stage/json->[{GzipHandler@30a40be{STARTED},[o.s.j.s.ServletContextHandler@6def4b33{/stages/stage/json,null,AVAILABLE,@Spark}]}]
22:57:16.299 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - executors/threadDump/json->[{GzipHandler@395860fa{STARTED},[o.s.j.s.ServletContextHandler@5ca18a1d{/executors/threadDump/json,null,AVAILABLE,@Spark}]}]
22:57:16.299 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - environment/json->[{GzipHandler@56ef900d{STARTED},[o.s.j.s.ServletContextHandler@3501a276{/environment/json,null,AVAILABLE,@Spark}]}]
22:57:16.299 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/job/json->[{GzipHandler@469e8d9e{STARTED},[o.s.j.s.ServletContextHandler@56d07c59{/jobs/job/json,null,AVAILABLE,@Spark}]}]
22:57:16.299 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs->[{GzipHandler@3ae6a9f6{STARTED},[o.s.j.s.ServletContextHandler@5d2d691a{/jobs,null,AVAILABLE,@Spark}]}]
22:57:16.299 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/json->[{GzipHandler@44284db4{STARTED},[o.s.j.s.ServletContextHandler@edcc350{/stages/json,null,AVAILABLE,@Spark}]}]
22:57:16.299 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/stage->[{GzipHandler@31c8e2ae{STARTED},[o.s.j.s.ServletContextHandler@3cdd11cc{/stages/stage,null,AVAILABLE,@Spark}]}]
22:57:16.299 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - storage/json->[{GzipHandler@9ae7eb5{STARTED},[o.s.j.s.ServletContextHandler@5288383e{/storage/json,null,AVAILABLE,@Spark}]}]
22:57:16.299 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - SQL->[{GzipHandler@78f97207{STARTED},[o.s.j.s.ServletContextHandler@6ec01c04{/SQL,null,AVAILABLE,@Spark}]}]
22:57:16.299 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - static/sql->[{GzipHandler@3befd7c9{STOPPED},[o.s.j.s.ServletContextHandler@118eb9bd{/static/sql,null,UNAVAILABLE,@Spark}]}]
22:57:16.299 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages/stage/kill->[{GzipHandler@7afc93c1{STARTED},[o.s.j.s.ServletContextHandler@7052620d{/stages/stage/kill,null,AVAILABLE,@Spark}]}]
22:57:16.299 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/job->[{GzipHandler@171f458{STARTED},[o.s.j.s.ServletContextHandler@1fdad46b{/jobs/job,null,AVAILABLE,@Spark}]}]
22:57:16.300 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - environment->[{GzipHandler@2762a443{STARTED},[o.s.j.s.ServletContextHandler@73adebf8{/environment,null,AVAILABLE,@Spark}]}]
22:57:16.300 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - stages->[{GzipHandler@4f73ea1c{STARTED},[o.s.j.s.ServletContextHandler@5e077920{/stages,null,AVAILABLE,@Spark}]}]
22:57:16.300 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - executors->[{GzipHandler@2539b24e{STARTED},[o.s.j.s.ServletContextHandler@328c8921{/executors,null,AVAILABLE,@Spark}]}]
22:57:16.300 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - SQL/json->[{GzipHandler@393833d0{STARTED},[o.s.j.s.ServletContextHandler@5331c7fa{/SQL/json,null,AVAILABLE,@Spark}]}]
22:57:16.300 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - jobs/job/kill->[{GzipHandler@4d5f6e4f{STARTED},[o.s.j.s.ServletContextHandler@6cd59fdc{/jobs/job/kill,null,AVAILABLE,@Spark}]}]
22:57:16.300 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - metrics/json->[{GzipHandler@684f5877{STARTED},[o.s.j.s.ServletContextHandler@6156560{/metrics/json,null,AVAILABLE,@Spark}]}]
22:57:16.300 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - SQL/execution->[{GzipHandler@3b1cbf53{STARTED},[o.s.j.s.ServletContextHandler@41aac2ed{/SQL/execution,null,AVAILABLE,@Spark}]}]
22:57:16.300 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.ContextHandlerCollection - executors/threadDump->[{GzipHandler@30592f62{STARTED},[o.s.j.s.ServletContextHandler@7779b1d5{/executors/threadDump,null,AVAILABLE,@Spark}]}]
22:57:16.300 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.ContainerLifeCycle - ContextHandlerCollection@7d170ea6{STARTED} added {GzipHandler@3befd7c9{STOPPED},UNMANAGED}
22:57:16.300 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting o.s.j.s.ServletContextHandler@118eb9bd{/static/sql,null,UNAVAILABLE,@Spark}
22:57:16.300 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting o.s.j.s.ServletContextHandler@118eb9bd{/static/sql,null,STARTING,@Spark}
22:57:16.300 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting ServletHandler@40a6ccec{STOPPED}
22:57:16.300 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - Path=/[EMBEDDED:null] mapped to servlet=org.sparkproject.jetty.servlet.DefaultServlet-74605451[EMBEDDED:null]
22:57:16.301 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.http.pathmap.PathMappings - Added MappedResource[pathSpec=ServletPathSpec["/",pathDepth=-1,group=DEFAULT],resource=org.sparkproject.jetty.servlet.DefaultServlet-74605451@4337dbda==org.sparkproject.jetty.servlet.DefaultServlet,jsp=null,order=-1,inst=true,async=true] to PathMappings[size=1]
22:57:16.301 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - filterNameMap={org.apache.spark.ui.HttpSecurityFilter-5fc8dedc=org.apache.spark.ui.HttpSecurityFilter-5fc8dedc@5fc8dedc==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true}
22:57:16.301 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - pathFilters=[[/*]/[]/[INCLUDE, REQUEST, ASYNC, FORWARD, ERROR]=>org.apache.spark.ui.HttpSecurityFilter-5fc8dedc]
22:57:16.301 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletFilterMap={}
22:57:16.301 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletPathMap=PathMappings[size=1]
22:57:16.301 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHandler - servletNameMap={org.sparkproject.jetty.servlet.DefaultServlet-74605451=org.sparkproject.jetty.servlet.DefaultServlet-74605451@4337dbda==org.sparkproject.jetty.servlet.DefaultServlet,jsp=null,order=-1,inst=true,async=true}
22:57:16.301 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting ServletHandler@40a6ccec{STARTING}
22:57:16.301 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @13074ms ServletHandler@40a6ccec{STARTED}
22:57:16.301 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting org.apache.spark.ui.HttpSecurityFilter-5fc8dedc@5fc8dedc==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true
22:57:16.302 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @13075ms org.apache.spark.ui.HttpSecurityFilter-5fc8dedc@5fc8dedc==org.apache.spark.ui.HttpSecurityFilter,inst=true,async=true
22:57:16.302 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.FilterHolder - Filter.init org.apache.spark.ui.HttpSecurityFilter@675f7752
22:57:16.302 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting org.sparkproject.jetty.servlet.DefaultServlet-74605451@4337dbda==org.sparkproject.jetty.servlet.DefaultServlet,jsp=null,order=-1,inst=true,async=true
22:57:16.302 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @13075ms org.sparkproject.jetty.servlet.DefaultServlet-74605451@4337dbda==org.sparkproject.jetty.servlet.DefaultServlet,jsp=null,order=-1,inst=true,async=true
22:57:16.302 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.ServletHolder - Servlet.init org.sparkproject.jetty.servlet.DefaultServlet@6f604140 for org.sparkproject.jetty.servlet.DefaultServlet-74605451
22:57:16.347 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.servlet.DefaultServlet - resource base = jar:file:/Users/sue/.m2/repository/org/apache/spark/spark-sql_2.12/3.0.1/spark-sql_2.12-3.0.1.jar!/org/apache/spark/sql/execution/ui/static
22:57:16.347 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@118eb9bd{/static/sql,null,AVAILABLE,@Spark}
22:57:16.347 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @13120ms o.s.j.s.ServletContextHandler@118eb9bd{/static/sql,null,AVAILABLE,@Spark}
22:57:16.347 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - starting GzipHandler@3befd7c9{STOPPED}
22:57:16.347 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - starting GzipHandler@3befd7c9{STARTING}
22:57:16.347 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STARTED @13120ms GzipHandler@3befd7c9{STARTED}
22:57:17.576 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.sql.execution.datasources.DataSource - Some paths were ignored:
  
22:57:17.687 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 100 ms to list leaf files for 1 paths.
22:57:17.794 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 4 ms to list leaf files for 1 paths.
22:57:20.059 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences - Resolving 'value to value#0
22:57:20.496 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences - Resolving 'value to value#6
22:57:21.298 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: 
22:57:21.304 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: 
22:57:21.308 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: (length(trim(value#0, None)) > 0)
22:57:21.312 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<value: string>
22:57:22.167 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.sql.execution.WholeStageCodegenExec - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator inputadapter_input_0;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     inputadapter_input_0 = inputs[0];
/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   protected void processNext() throws java.io.IOException {
/* 025 */     while ( inputadapter_input_0.hasNext()) {
/* 026 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();
/* 027 */
/* 028 */       do {
/* 029 */         boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);
/* 030 */         UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?
/* 031 */         null : (inputadapter_row_0.getUTF8String(0));
/* 032 */
/* 033 */         boolean filter_isNull_0 = true;
/* 034 */         boolean filter_value_0 = false;
/* 035 */         boolean filter_isNull_2 = false;
/* 036 */         UTF8String filter_value_2 = null;
/* 037 */         if (inputadapter_isNull_0) {
/* 038 */           filter_isNull_2 = true;
/* 039 */         } else {
/* 040 */           filter_value_2 = inputadapter_value_0.trim();
/* 041 */         }
/* 042 */         boolean filter_isNull_1 = filter_isNull_2;
/* 043 */         int filter_value_1 = -1;
/* 044 */
/* 045 */         if (!filter_isNull_2) {
/* 046 */           filter_value_1 = (filter_value_2).numChars();
/* 047 */         }
/* 048 */         if (!filter_isNull_1) {
/* 049 */           filter_isNull_0 = false; // resultCode could change nullability.
/* 050 */           filter_value_0 = filter_value_1 > 0;
/* 051 */
/* 052 */         }
/* 053 */         if (filter_isNull_0 || !filter_value_0) continue;
/* 054 */
/* 055 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 056 */
/* 057 */         filter_mutableStateArray_0[0].reset();
/* 058 */
/* 059 */         filter_mutableStateArray_0[0].zeroOutNullBytes();
/* 060 */
/* 061 */         if (inputadapter_isNull_0) {
/* 062 */           filter_mutableStateArray_0[0].setNullAt(0);
/* 063 */         } else {
/* 064 */           filter_mutableStateArray_0[0].write(0, inputadapter_value_0);
/* 065 */         }
/* 066 */         append((filter_mutableStateArray_0[0].getRow()));
/* 067 */
/* 068 */       } while(false);
/* 069 */       if (shouldStop()) return;
/* 070 */     }
/* 071 */   }
/* 072 */
/* 073 */ }

22:57:22.230 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator inputadapter_input_0;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] filter_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     inputadapter_input_0 = inputs[0];
/* 020 */     filter_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   protected void processNext() throws java.io.IOException {
/* 025 */     while ( inputadapter_input_0.hasNext()) {
/* 026 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();
/* 027 */
/* 028 */       do {
/* 029 */         boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);
/* 030 */         UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?
/* 031 */         null : (inputadapter_row_0.getUTF8String(0));
/* 032 */
/* 033 */         boolean filter_isNull_0 = true;
/* 034 */         boolean filter_value_0 = false;
/* 035 */         boolean filter_isNull_2 = false;
/* 036 */         UTF8String filter_value_2 = null;
/* 037 */         if (inputadapter_isNull_0) {
/* 038 */           filter_isNull_2 = true;
/* 039 */         } else {
/* 040 */           filter_value_2 = inputadapter_value_0.trim();
/* 041 */         }
/* 042 */         boolean filter_isNull_1 = filter_isNull_2;
/* 043 */         int filter_value_1 = -1;
/* 044 */
/* 045 */         if (!filter_isNull_2) {
/* 046 */           filter_value_1 = (filter_value_2).numChars();
/* 047 */         }
/* 048 */         if (!filter_isNull_1) {
/* 049 */           filter_isNull_0 = false; // resultCode could change nullability.
/* 050 */           filter_value_0 = filter_value_1 > 0;
/* 051 */
/* 052 */         }
/* 053 */         if (filter_isNull_0 || !filter_value_0) continue;
/* 054 */
/* 055 */         ((org.apache.spark.sql.execution.metric.SQLMetric) references[0] /* numOutputRows */).add(1);
/* 056 */
/* 057 */         filter_mutableStateArray_0[0].reset();
/* 058 */
/* 059 */         filter_mutableStateArray_0[0].zeroOutNullBytes();
/* 060 */
/* 061 */         if (inputadapter_isNull_0) {
/* 062 */           filter_mutableStateArray_0[0].setNullAt(0);
/* 063 */         } else {
/* 064 */           filter_mutableStateArray_0[0].write(0, inputadapter_value_0);
/* 065 */         }
/* 066 */         append((filter_mutableStateArray_0[0].getRow()));
/* 067 */
/* 068 */       } while(false);
/* 069 */       if (shouldStop()) return;
/* 070 */     }
/* 071 */   }
/* 072 */
/* 073 */ }

22:57:22.662 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 478.634917 ms
22:57:22.799 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 170.5 KiB, free 1048.6 MiB)
22:57:22.803 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_0 locally took 86 ms
22:57:22.805 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_0 without replication took 88 ms
22:57:22.939 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 23.8 KiB, free 1048.6 MiB)
22:57:22.945 [dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on saras-air:55957 (size: 23.8 KiB, free: 1048.8 MiB)
22:57:22.948 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
22:57:22.951 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_0_piece0
22:57:22.951 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_0_piece0 locally took 14 ms
22:57:22.952 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_0_piece0 without replication took 14 ms
22:57:22.954 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.SparkContext - Created broadcast 0 from csv at WuzzufJobsAnalysisApplication.java:56
22:57:22.969 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5114295 bytes, open cost is considered as scanning 4194304 bytes.
22:57:23.078 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$doExecute$4$adapted
22:57:23.122 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++
22:57:23.194 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$executeTake$2
22:57:23.203 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$executeTake$2) is now cleaned +++
22:57:23.212 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$runJob$5
22:57:23.237 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++
22:57:23.243 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.SparkContext - Starting job: csv at WuzzufJobsAnalysisApplication.java:56
22:57:23.282 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 0 (csv at WuzzufJobsAnalysisApplication.java:56) with 1 output partitions
22:57:23.283 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (csv at WuzzufJobsAnalysisApplication.java:56)
22:57:23.283 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
22:57:23.285 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
22:57:23.296 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - submitStage(ResultStage 0 (name=csv at WuzzufJobsAnalysisApplication.java:56;jobs=0))
22:57:23.302 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - missing: List()
22:57:23.303 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[3] at csv at WuzzufJobsAnalysisApplication.java:56), which has no missing parents
22:57:23.303 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - submitMissingTasks(ResultStage 0)
22:57:23.464 [dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 10.7 KiB, free 1048.6 MiB)
22:57:23.465 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_1 locally took 8 ms
22:57:23.465 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_1 without replication took 8 ms
22:57:23.473 [dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 5.3 KiB, free 1048.6 MiB)
22:57:23.474 [dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on saras-air:55957 (size: 5.3 KiB, free: 1048.8 MiB)
22:57:23.475 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
22:57:23.475 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_1_piece0
22:57:23.476 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_1_piece0 locally took 3 ms
22:57:23.476 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_1_piece0 without replication took 3 ms
22:57:23.478 [dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 1 from broadcast at DAGScheduler.scala:1223
22:57:23.522 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at csv at WuzzufJobsAnalysisApplication.java:56) (first 15 tasks are for partitions Vector(0))
22:57:23.523 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 0.0 with 1 tasks
22:57:23.555 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Epoch for TaskSet 0.0: 0
22:57:23.562 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Adding pending tasks took 5 ms
22:57:23.566 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Valid locality levels for TaskSet 0.0: NO_PREF, ANY
22:57:23.581 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0.0, runningTasks: 0
22:57:23.582 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSetManager - Valid locality levels for TaskSet 0.0: NO_PREF, ANY
22:57:23.619 [dispatcher-event-loop-1] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0, saras-air, executor driver, partition 0, PROCESS_LOCAL, 7808 bytes)
22:57:23.759 [Executor task launch worker for task 0] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 0.0 (TID 0)
22:57:23.815 [Executor task launch worker for task 0] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (0, 0) -> 1
22:57:24.045 [Executor task launch worker for task 0] DEBUG org.apache.spark.storage.BlockManager - Getting local block broadcast_1
22:57:24.052 [Executor task launch worker for task 0] DEBUG org.apache.spark.storage.BlockManager - Level for block broadcast_1 is StorageLevel(disk, memory, deserialized, 1 replicas)
22:57:24.269 [Executor task launch worker for task 0] INFO org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///Users/sue/Documents/ITI/java-for-ml/final-project/WuzzufJobsAnalysis/src/main/resources/Wuzzuf_Jobs.csv, range: 0-919991, partition values: [empty row]
22:57:24.310 [Executor task launch worker for task 0] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection - code for input[0, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

22:57:24.318 [Executor task launch worker for task 0] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

22:57:24.391 [Executor task launch worker for task 0] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 80.817125 ms
22:57:24.394 [Executor task launch worker for task 0] DEBUG org.apache.spark.storage.BlockManager - Getting local block broadcast_0
22:57:24.394 [Executor task launch worker for task 0] DEBUG org.apache.spark.storage.BlockManager - Level for block broadcast_0 is StorageLevel(disk, memory, deserialized, 1 replicas)
22:57:24.486 [Executor task launch worker for task 0] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 0.0 (TID 0). 1623 bytes result sent to driver
22:57:24.488 [Executor task launch worker for task 0] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - removing (0, 0) from stageTCMP
22:57:24.496 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_0.0, runningTasks: 0
22:57:24.497 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSetManager - No tasks for locality level NO_PREF, so moving to locality level ANY
22:57:24.516 [task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 916 ms on saras-air (executor driver) (1/1)
22:57:24.520 [task-result-getter-0] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 0.0, whose tasks have all completed, from pool 
22:57:24.536 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (csv at WuzzufJobsAnalysisApplication.java:56) finished in 1.187 s
22:57:24.541 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - After removal of stage 0, remaining stages = 0
22:57:24.542 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
22:57:24.543 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 0: Stage finished
22:57:24.547 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.scheduler.DAGScheduler - Job 0 finished: csv at WuzzufJobsAnalysisApplication.java:56, took 1.302850 s
22:57:24.589 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection - code for input[0, string, true].toString:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     boolean isNull_1 = i.isNullAt(0);
/* 024 */     UTF8String value_1 = isNull_1 ?
/* 025 */     null : (i.getUTF8String(0));
/* 026 */     boolean isNull_0 = true;
/* 027 */     java.lang.String value_0 = null;
/* 028 */     if (!isNull_1) {
/* 029 */
/* 030 */       isNull_0 = false;
/* 031 */       if (!isNull_0) {
/* 032 */
/* 033 */         Object funcResult_0 = null;
/* 034 */         funcResult_0 = value_1.toString();
/* 035 */         value_0 = (java.lang.String) funcResult_0;
/* 036 */
/* 037 */       }
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableRow.setNullAt(0);
/* 041 */     } else {
/* 042 */
/* 043 */       mutableRow.update(0, value_0);
/* 044 */     }
/* 045 */
/* 046 */     return mutableRow;
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */ }

22:57:24.595 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     boolean isNull_1 = i.isNullAt(0);
/* 024 */     UTF8String value_1 = isNull_1 ?
/* 025 */     null : (i.getUTF8String(0));
/* 026 */     boolean isNull_0 = true;
/* 027 */     java.lang.String value_0 = null;
/* 028 */     if (!isNull_1) {
/* 029 */
/* 030 */       isNull_0 = false;
/* 031 */       if (!isNull_0) {
/* 032 */
/* 033 */         Object funcResult_0 = null;
/* 034 */         funcResult_0 = value_1.toString();
/* 035 */         value_0 = (java.lang.String) funcResult_0;
/* 036 */
/* 037 */       }
/* 038 */     }
/* 039 */     if (isNull_0) {
/* 040 */       mutableRow.setNullAt(0);
/* 041 */     } else {
/* 042 */
/* 043 */       mutableRow.update(0, value_0);
/* 044 */     }
/* 045 */
/* 046 */     return mutableRow;
/* 047 */   }
/* 048 */
/* 049 */
/* 050 */ }

22:57:24.647 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 57.24875 ms
22:57:24.774 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: 
22:57:24.776 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: 
22:57:24.776 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: 
22:57:24.778 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<value: string>
22:57:24.801 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 170.5 KiB, free 1048.4 MiB)
22:57:24.802 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_2 locally took 14 ms
22:57:24.802 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_2 without replication took 14 ms
22:57:24.835 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 23.8 KiB, free 1048.4 MiB)
22:57:24.836 [dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on saras-air:55957 (size: 23.8 KiB, free: 1048.7 MiB)
22:57:24.836 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
22:57:24.836 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_2_piece0
22:57:24.836 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_2_piece0 locally took 2 ms
22:57:24.836 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_2_piece0 without replication took 2 ms
22:57:24.839 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.SparkContext - Created broadcast 2 from csv at WuzzufJobsAnalysisApplication.java:56
22:57:24.841 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5114295 bytes, open cost is considered as scanning 4194304 bytes.
22:57:24.874 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$rdd$1
22:57:24.943 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$rdd$1) is now cleaned +++
22:57:24.974 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$inferFromDataset$2
22:57:24.977 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$inferFromDataset$2) is now cleaned +++
22:57:25.049 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.sql.execution.SparkSqlParser - Parsing command: WUZZUF_DATA
22:57:25.392 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.sql.execution.SparkSqlParser - Parsing command: SELECT cast (Title as string) Title, cast (Company as string) Company, cast (Location as string) Location, cast (Type as string) Type, cast (Level as string) Level, cast (YearsExp as string) YearsExp, cast (Country as string) Country, cast (Skills as string) Skills FROM WUZZUF_DATA
22:57:25.468 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(0)
22:57:25.479 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 0
22:57:25.482 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 0
22:57:25.483 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(45)
22:57:25.483 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 45
22:57:25.483 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 45
22:57:25.483 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(42)
22:57:25.483 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 42
22:57:25.483 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 42
22:57:25.483 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(8)
22:57:25.483 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 8
22:57:25.483 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 8
22:57:25.483 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(9)
22:57:25.483 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 9
22:57:25.483 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 9
22:57:25.483 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(14)
22:57:25.483 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 14
22:57:25.483 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 14
22:57:25.483 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(22)
22:57:25.483 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 22
22:57:25.483 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 22
22:57:25.483 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(13)
22:57:25.483 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 13
22:57:25.483 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 13
22:57:25.483 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(4)
22:57:25.483 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 4
22:57:25.483 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 4
22:57:25.483 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(33)
22:57:25.483 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 33
22:57:25.483 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 33
22:57:25.483 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(37)
22:57:25.483 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 37
22:57:25.483 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 37
22:57:25.483 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(20)
22:57:25.483 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 20
22:57:25.483 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 20
22:57:25.483 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(11)
22:57:25.483 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 11
22:57:25.483 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 11
22:57:25.484 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(1)
22:57:25.484 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 1
22:57:25.484 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 1
22:57:25.484 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(24)
22:57:25.484 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 24
22:57:25.484 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 24
22:57:25.484 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(26)
22:57:25.484 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 26
22:57:25.484 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 26
22:57:25.484 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(17)
22:57:25.484 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 17
22:57:25.484 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 17
22:57:25.484 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(18)
22:57:25.484 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 18
22:57:25.484 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 18
22:57:25.484 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(19)
22:57:25.484 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 19
22:57:25.484 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 19
22:57:25.484 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(6)
22:57:25.484 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 6
22:57:25.484 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 6
22:57:25.484 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(27)
22:57:25.484 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 27
22:57:25.484 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 27
22:57:25.484 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(41)
22:57:25.484 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 41
22:57:25.484 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 41
22:57:25.484 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanBroadcast(1)
22:57:25.485 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning broadcast 1
22:57:25.486 [Spark Context Cleaner] DEBUG org.apache.spark.broadcast.TorrentBroadcast - Unpersisting TorrentBroadcast 1
22:57:25.505 [block-manager-slave-async-thread-pool-0] DEBUG org.apache.spark.storage.BlockManagerSlaveEndpoint - removing broadcast 1
22:57:25.506 [block-manager-slave-async-thread-pool-0] DEBUG org.apache.spark.storage.BlockManager - Removing broadcast 1
22:57:25.510 [block-manager-slave-async-thread-pool-0] DEBUG org.apache.spark.storage.BlockManager - Removing block broadcast_1
22:57:25.512 [block-manager-slave-async-thread-pool-0] DEBUG org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 of size 10992 dropped from memory (free 1099343094)
22:57:25.513 [block-manager-slave-async-thread-pool-0] DEBUG org.apache.spark.storage.BlockManager - Removing block broadcast_1_piece0
22:57:25.515 [block-manager-slave-async-thread-pool-0] DEBUG org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 of size 5446 dropped from memory (free 1099348540)
22:57:25.524 [dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_1_piece0 on saras-air:55957 in memory (size: 5.3 KiB, free: 1048.8 MiB)
22:57:25.525 [block-manager-slave-async-thread-pool-0] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_1_piece0
22:57:25.525 [block-manager-slave-async-thread-pool-0] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_1_piece0
22:57:25.529 [block-manager-slave-async-thread-pool-2] DEBUG org.apache.spark.storage.BlockManagerSlaveEndpoint - Done removing broadcast 1, response is 0
22:57:25.530 [block-manager-slave-async-thread-pool-2] DEBUG org.apache.spark.storage.BlockManagerSlaveEndpoint - Sent response: 0 to saras-air:55956
22:57:25.531 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned broadcast 1
22:57:25.531 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(44)
22:57:25.531 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 44
22:57:25.531 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 44
22:57:25.531 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(39)
22:57:25.531 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 39
22:57:25.531 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 39
22:57:25.531 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanBroadcast(0)
22:57:25.531 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning broadcast 0
22:57:25.531 [Spark Context Cleaner] DEBUG org.apache.spark.broadcast.TorrentBroadcast - Unpersisting TorrentBroadcast 0
22:57:25.532 [block-manager-slave-async-thread-pool-3] DEBUG org.apache.spark.storage.BlockManagerSlaveEndpoint - removing broadcast 0
22:57:25.532 [block-manager-slave-async-thread-pool-3] DEBUG org.apache.spark.storage.BlockManager - Removing broadcast 0
22:57:25.532 [block-manager-slave-async-thread-pool-3] DEBUG org.apache.spark.storage.BlockManager - Removing block broadcast_0_piece0
22:57:25.534 [block-manager-slave-async-thread-pool-3] DEBUG org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 of size 24400 dropped from memory (free 1099372940)
22:57:25.535 [dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_0_piece0 on saras-air:55957 in memory (size: 23.8 KiB, free: 1048.8 MiB)
22:57:25.535 [block-manager-slave-async-thread-pool-3] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_0_piece0
22:57:25.535 [block-manager-slave-async-thread-pool-3] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_0_piece0
22:57:25.536 [block-manager-slave-async-thread-pool-3] DEBUG org.apache.spark.storage.BlockManager - Removing block broadcast_0
22:57:25.536 [block-manager-slave-async-thread-pool-3] DEBUG org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 of size 174584 dropped from memory (free 1099547524)
22:57:25.538 [block-manager-slave-async-thread-pool-5] DEBUG org.apache.spark.storage.BlockManagerSlaveEndpoint - Done removing broadcast 0, response is 0
22:57:25.539 [block-manager-slave-async-thread-pool-5] DEBUG org.apache.spark.storage.BlockManagerSlaveEndpoint - Sent response: 0 to saras-air:55956
22:57:25.541 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned broadcast 0
22:57:25.541 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(15)
22:57:25.541 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 15
22:57:25.542 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 15
22:57:25.542 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(43)
22:57:25.542 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 43
22:57:25.542 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 43
22:57:25.542 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(7)
22:57:25.542 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 7
22:57:25.542 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 7
22:57:25.542 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(5)
22:57:25.542 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 5
22:57:25.542 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 5
22:57:25.542 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(40)
22:57:25.542 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 40
22:57:25.542 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 40
22:57:25.542 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(28)
22:57:25.542 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 28
22:57:25.542 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 28
22:57:25.542 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(3)
22:57:25.542 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 3
22:57:25.542 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 3
22:57:25.542 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(36)
22:57:25.542 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 36
22:57:25.542 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 36
22:57:25.542 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(38)
22:57:25.542 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 38
22:57:25.542 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 38
22:57:25.542 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(30)
22:57:25.542 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 30
22:57:25.542 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 30
22:57:25.542 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(2)
22:57:25.542 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 2
22:57:25.542 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 2
22:57:25.543 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(35)
22:57:25.543 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 35
22:57:25.543 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 35
22:57:25.543 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(12)
22:57:25.543 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 12
22:57:25.543 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 12
22:57:25.543 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(34)
22:57:25.543 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 34
22:57:25.543 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 34
22:57:25.543 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(16)
22:57:25.543 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 16
22:57:25.543 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 16
22:57:25.543 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(23)
22:57:25.543 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 23
22:57:25.543 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 23
22:57:25.543 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(29)
22:57:25.543 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 29
22:57:25.543 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 29
22:57:25.543 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(32)
22:57:25.543 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 32
22:57:25.543 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 32
22:57:25.543 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(46)
22:57:25.543 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 46
22:57:25.543 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 46
22:57:25.543 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(21)
22:57:25.543 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 21
22:57:25.543 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 21
22:57:25.543 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(10)
22:57:25.543 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 10
22:57:25.543 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 10
22:57:25.543 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanBroadcast(2)
22:57:25.543 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning broadcast 2
22:57:25.543 [Spark Context Cleaner] DEBUG org.apache.spark.broadcast.TorrentBroadcast - Unpersisting TorrentBroadcast 2
22:57:25.544 [block-manager-slave-async-thread-pool-6] DEBUG org.apache.spark.storage.BlockManagerSlaveEndpoint - removing broadcast 2
22:57:25.544 [block-manager-slave-async-thread-pool-6] DEBUG org.apache.spark.storage.BlockManager - Removing broadcast 2
22:57:25.544 [block-manager-slave-async-thread-pool-6] DEBUG org.apache.spark.storage.BlockManager - Removing block broadcast_2
22:57:25.545 [block-manager-slave-async-thread-pool-6] DEBUG org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 of size 174584 dropped from memory (free 1099722108)
22:57:25.545 [block-manager-slave-async-thread-pool-6] DEBUG org.apache.spark.storage.BlockManager - Removing block broadcast_2_piece0
22:57:25.547 [block-manager-slave-async-thread-pool-6] DEBUG org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 of size 24400 dropped from memory (free 1099746508)
22:57:25.548 [dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on saras-air:55957 in memory (size: 23.8 KiB, free: 1048.8 MiB)
22:57:25.548 [block-manager-slave-async-thread-pool-6] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_2_piece0
22:57:25.548 [block-manager-slave-async-thread-pool-6] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_2_piece0
22:57:25.549 [block-manager-slave-async-thread-pool-8] DEBUG org.apache.spark.storage.BlockManagerSlaveEndpoint - Done removing broadcast 2, response is 0
22:57:25.549 [block-manager-slave-async-thread-pool-8] DEBUG org.apache.spark.storage.BlockManagerSlaveEndpoint - Sent response: 0 to saras-air:55956
22:57:25.550 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned broadcast 2
22:57:25.552 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(31)
22:57:25.552 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 31
22:57:25.552 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 31
22:57:25.552 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(25)
22:57:25.552 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 25
22:57:25.552 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 25
22:57:25.642 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences - Resolving 'Title to Title#16
22:57:25.643 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences - Resolving 'Company to Company#17
22:57:25.643 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences - Resolving 'Location to Location#18
22:57:25.643 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences - Resolving 'Type to Type#19
22:57:25.644 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences - Resolving 'Level to Level#20
22:57:25.644 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences - Resolving 'YearsExp to YearsExp#21
22:57:25.645 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences - Resolving 'Country to Country#22
22:57:25.645 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences - Resolving 'Skills to Skills#23
22:57:25.731 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.ml.feature.StringIndexer - Input schema: {"type":"struct","fields":[{"name":"Title","type":"string","nullable":true,"metadata":{}},{"name":"Company","type":"string","nullable":true,"metadata":{}},{"name":"Location","type":"string","nullable":true,"metadata":{}},{"name":"Type","type":"string","nullable":true,"metadata":{}},{"name":"Level","type":"string","nullable":true,"metadata":{}},{"name":"YearsExp","type":"string","nullable":true,"metadata":{}},{"name":"Country","type":"string","nullable":true,"metadata":{}},{"name":"Skills","type":"string","nullable":true,"metadata":{}}]}
22:57:25.768 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.ml.feature.StringIndexer - Expected output schema: {"type":"struct","fields":[{"name":"Title","type":"string","nullable":true,"metadata":{}},{"name":"Company","type":"string","nullable":true,"metadata":{}},{"name":"Location","type":"string","nullable":true,"metadata":{}},{"name":"Type","type":"string","nullable":true,"metadata":{}},{"name":"Level","type":"string","nullable":true,"metadata":{}},{"name":"YearsExp","type":"string","nullable":true,"metadata":{}},{"name":"Country","type":"string","nullable":true,"metadata":{}},{"name":"Skills","type":"string","nullable":true,"metadata":{}},{"name":"type-factorized","type":"double","nullable":false,"metadata":{"ml_attr":{"type":"nominal"}}}]}
22:57:26.164 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: 
22:57:26.165 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: 
22:57:26.165 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: 
22:57:26.166 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<Type: string>
22:57:26.302 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.sql.execution.WholeStageCodegenExec - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator inputadapter_input_0;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] project_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     inputadapter_input_0 = inputs[0];
/* 020 */     project_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   protected void processNext() throws java.io.IOException {
/* 025 */     while ( inputadapter_input_0.hasNext()) {
/* 026 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();
/* 027 */
/* 028 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);
/* 029 */       UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?
/* 030 */       null : (inputadapter_row_0.getUTF8String(0));
/* 031 */       project_mutableStateArray_0[0].reset();
/* 032 */
/* 033 */       project_mutableStateArray_0[0].zeroOutNullBytes();
/* 034 */
/* 035 */       if (inputadapter_isNull_0) {
/* 036 */         project_mutableStateArray_0[0].setNullAt(0);
/* 037 */       } else {
/* 038 */         project_mutableStateArray_0[0].write(0, inputadapter_value_0);
/* 039 */       }
/* 040 */       append((project_mutableStateArray_0[0].getRow()));
/* 041 */       if (shouldStop()) return;
/* 042 */     }
/* 043 */   }
/* 044 */
/* 045 */ }

22:57:26.322 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator inputadapter_input_0;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] project_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     inputadapter_input_0 = inputs[0];
/* 020 */     project_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   protected void processNext() throws java.io.IOException {
/* 025 */     while ( inputadapter_input_0.hasNext()) {
/* 026 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();
/* 027 */
/* 028 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);
/* 029 */       UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?
/* 030 */       null : (inputadapter_row_0.getUTF8String(0));
/* 031 */       project_mutableStateArray_0[0].reset();
/* 032 */
/* 033 */       project_mutableStateArray_0[0].zeroOutNullBytes();
/* 034 */
/* 035 */       if (inputadapter_isNull_0) {
/* 036 */         project_mutableStateArray_0[0].setNullAt(0);
/* 037 */       } else {
/* 038 */         project_mutableStateArray_0[0].write(0, inputadapter_value_0);
/* 039 */       }
/* 040 */       append((project_mutableStateArray_0[0].getRow()));
/* 041 */       if (shouldStop()) return;
/* 042 */     }
/* 043 */   }
/* 044 */
/* 045 */ }

22:57:26.498 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 195.920416 ms
22:57:26.511 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 170.5 KiB, free 1048.6 MiB)
22:57:26.512 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_3 locally took 10 ms
22:57:26.512 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_3 without replication took 11 ms
22:57:26.541 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 23.8 KiB, free 1048.6 MiB)
22:57:26.542 [dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on saras-air:55957 (size: 23.8 KiB, free: 1048.8 MiB)
22:57:26.543 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_3_piece0
22:57:26.543 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_3_piece0
22:57:26.543 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_3_piece0 locally took 2 ms
22:57:26.543 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_3_piece0 without replication took 2 ms
22:57:26.544 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.SparkContext - Created broadcast 3 from collect at StringIndexer.scala:204
22:57:26.550 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5114295 bytes, open cost is considered as scanning 4194304 bytes.
22:57:26.556 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$doExecute$4$adapted
22:57:26.569 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++
22:57:26.586 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$collect$2
22:57:26.601 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$collect$2) is now cleaned +++
22:57:26.729 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$runJob$5
22:57:26.737 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++
22:57:26.742 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.SparkContext - Starting job: collect at StringIndexer.scala:204
22:57:26.747 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 14 (collect at StringIndexer.scala:204) as input to shuffle 0
22:57:26.749 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 1 (collect at StringIndexer.scala:204) with 1 output partitions
22:57:26.749 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 2 (collect at StringIndexer.scala:204)
22:57:26.750 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 1)
22:57:26.752 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 1)
22:57:26.753 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - submitStage(ResultStage 2 (name=collect at StringIndexer.scala:204;jobs=1))
22:57:26.753 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - missing: List(ShuffleMapStage 1)
22:57:26.754 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - submitStage(ShuffleMapStage 1 (name=collect at StringIndexer.scala:204;jobs=1))
22:57:26.755 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - missing: List()
22:57:26.755 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 1 (MapPartitionsRDD[14] at collect at StringIndexer.scala:204), which has no missing parents
22:57:26.755 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - submitMissingTasks(ShuffleMapStage 1)
22:57:26.773 [dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 20.9 KiB, free 1048.6 MiB)
22:57:26.774 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_4 locally took 1 ms
22:57:26.774 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_4 without replication took 1 ms
22:57:26.792 [dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 1048.6 MiB)
22:57:26.794 [dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on saras-air:55957 (size: 10.2 KiB, free: 1048.8 MiB)
22:57:26.794 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_4_piece0
22:57:26.794 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_4_piece0
22:57:26.794 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_4_piece0 locally took 2 ms
22:57:26.794 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_4_piece0 without replication took 2 ms
22:57:26.795 [dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1223
22:57:26.798 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 1 (MapPartitionsRDD[14] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))
22:57:26.798 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 1.0 with 1 tasks
22:57:26.799 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Epoch for TaskSet 1.0: 0
22:57:26.800 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Adding pending tasks took 0 ms
22:57:26.800 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Valid locality levels for TaskSet 1.0: NO_PREF, ANY
22:57:26.803 [dispatcher-event-loop-0] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_1.0, runningTasks: 0
22:57:26.806 [dispatcher-event-loop-0] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1, saras-air, executor driver, partition 0, PROCESS_LOCAL, 7797 bytes)
22:57:26.808 [Executor task launch worker for task 1] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 1.0 (TID 1)
22:57:26.815 [Executor task launch worker for task 1] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (1, 0) -> 1
22:57:26.817 [Executor task launch worker for task 1] DEBUG org.apache.spark.storage.BlockManager - Getting local block broadcast_4
22:57:26.818 [Executor task launch worker for task 1] DEBUG org.apache.spark.storage.BlockManager - Level for block broadcast_4 is StorageLevel(disk, memory, deserialized, 1 replicas)
22:57:26.939 [Executor task launch worker for task 1] INFO org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///Users/sue/Documents/ITI/java-for-ml/final-project/WuzzufJobsAnalysis/src/main/resources/Wuzzuf_Jobs.csv, range: 0-919991, partition values: [empty row]
22:57:26.951 [Executor task launch worker for task 1] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection - code for input[0, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

22:57:26.955 [Executor task launch worker for task 1] DEBUG org.apache.spark.storage.BlockManager - Getting local block broadcast_3
22:57:26.957 [Executor task launch worker for task 1] DEBUG org.apache.spark.storage.BlockManager - Level for block broadcast_3 is StorageLevel(disk, memory, deserialized, 1 replicas)
22:57:27.006 [Executor task launch worker for task 1] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection - code for noop$():
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificMutableProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificMutableProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseMutableProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificMutableProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = new org.apache.spark.sql.catalyst.expressions.GenericInternalRow(1);
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public org.apache.spark.sql.catalyst.expressions.codegen.BaseMutableProjection target(InternalRow row) {
/* 022 */     mutableRow = row;
/* 023 */     return this;
/* 024 */   }
/* 025 */
/* 026 */   /* Provide immutable access to the last projected row. */
/* 027 */   public InternalRow currentValue() {
/* 028 */     return (InternalRow) mutableRow;
/* 029 */   }
/* 030 */
/* 031 */   public java.lang.Object apply(java.lang.Object _i) {
/* 032 */     InternalRow i = (InternalRow) _i;
/* 033 */
/* 034 */
/* 035 */     // copy all the results into MutableRow
/* 036 */
/* 037 */     return mutableRow;
/* 038 */   }
/* 039 */
/* 040 */
/* 041 */ }

22:57:27.009 [Executor task launch worker for task 1] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificMutableProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificMutableProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseMutableProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificMutableProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = new org.apache.spark.sql.catalyst.expressions.GenericInternalRow(1);
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public org.apache.spark.sql.catalyst.expressions.codegen.BaseMutableProjection target(InternalRow row) {
/* 022 */     mutableRow = row;
/* 023 */     return this;
/* 024 */   }
/* 025 */
/* 026 */   /* Provide immutable access to the last projected row. */
/* 027 */   public InternalRow currentValue() {
/* 028 */     return (InternalRow) mutableRow;
/* 029 */   }
/* 030 */
/* 031 */   public java.lang.Object apply(java.lang.Object _i) {
/* 032 */     InternalRow i = (InternalRow) _i;
/* 033 */
/* 034 */
/* 035 */     // copy all the results into MutableRow
/* 036 */
/* 037 */     return mutableRow;
/* 038 */   }
/* 039 */
/* 040 */
/* 041 */ }

22:57:27.038 [Executor task launch worker for task 1] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 30.962334 ms
22:57:27.058 [Executor task launch worker for task 1] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection - code for noop$():
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificMutableProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificMutableProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseMutableProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificMutableProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = new org.apache.spark.sql.catalyst.expressions.GenericInternalRow(1);
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public org.apache.spark.sql.catalyst.expressions.codegen.BaseMutableProjection target(InternalRow row) {
/* 022 */     mutableRow = row;
/* 023 */     return this;
/* 024 */   }
/* 025 */
/* 026 */   /* Provide immutable access to the last projected row. */
/* 027 */   public InternalRow currentValue() {
/* 028 */     return (InternalRow) mutableRow;
/* 029 */   }
/* 030 */
/* 031 */   public java.lang.Object apply(java.lang.Object _i) {
/* 032 */     InternalRow i = (InternalRow) _i;
/* 033 */
/* 034 */
/* 035 */     // copy all the results into MutableRow
/* 036 */
/* 037 */     return mutableRow;
/* 038 */   }
/* 039 */
/* 040 */
/* 041 */ }

22:57:27.062 [Executor task launch worker for task 1] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

22:57:27.063 [Executor task launch worker for task 1] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

22:57:27.074 [Executor task launch worker for task 1] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 11.1095 ms
22:57:27.080 [Executor task launch worker for task 1] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection - code for input[0, binary, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     byte[] value_0 = isNull_0 ?
/* 033 */     null : (i.getBinary(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

22:57:27.081 [Executor task launch worker for task 1] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     byte[] value_0 = isNull_0 ?
/* 033 */     null : (i.getBinary(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

22:57:27.100 [Executor task launch worker for task 1] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 20.48475 ms
22:57:27.109 [Executor task launch worker for task 1] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection - code for noop$():
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificMutableProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificMutableProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseMutableProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificMutableProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = new org.apache.spark.sql.catalyst.expressions.GenericInternalRow(1);
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public org.apache.spark.sql.catalyst.expressions.codegen.BaseMutableProjection target(InternalRow row) {
/* 022 */     mutableRow = row;
/* 023 */     return this;
/* 024 */   }
/* 025 */
/* 026 */   /* Provide immutable access to the last projected row. */
/* 027 */   public InternalRow currentValue() {
/* 028 */     return (InternalRow) mutableRow;
/* 029 */   }
/* 030 */
/* 031 */   public java.lang.Object apply(java.lang.Object _i) {
/* 032 */     InternalRow i = (InternalRow) _i;
/* 033 */
/* 034 */
/* 035 */     // copy all the results into MutableRow
/* 036 */
/* 037 */     return mutableRow;
/* 038 */   }
/* 039 */
/* 040 */
/* 041 */ }

22:57:27.119 [Executor task launch worker for task 1] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection - code for createexternalrow(input[0, string, true].toString, StructField(Type,StringType,true)):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     Object[] values_0 = new Object[1];
/* 024 */
/* 025 */     boolean isNull_2 = i.isNullAt(0);
/* 026 */     UTF8String value_2 = isNull_2 ?
/* 027 */     null : (i.getUTF8String(0));
/* 028 */     boolean isNull_1 = true;
/* 029 */     java.lang.String value_1 = null;
/* 030 */     if (!isNull_2) {
/* 031 */
/* 032 */       isNull_1 = false;
/* 033 */       if (!isNull_1) {
/* 034 */
/* 035 */         Object funcResult_0 = null;
/* 036 */         funcResult_0 = value_2.toString();
/* 037 */         value_1 = (java.lang.String) funcResult_0;
/* 038 */
/* 039 */       }
/* 040 */     }
/* 041 */     if (isNull_1) {
/* 042 */       values_0[0] = null;
/* 043 */     } else {
/* 044 */       values_0[0] = value_1;
/* 045 */     }
/* 046 */
/* 047 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));
/* 048 */     if (false) {
/* 049 */       mutableRow.setNullAt(0);
/* 050 */     } else {
/* 051 */
/* 052 */       mutableRow.update(0, value_0);
/* 053 */     }
/* 054 */
/* 055 */     return mutableRow;
/* 056 */   }
/* 057 */
/* 058 */
/* 059 */ }

22:57:27.119 [Executor task launch worker for task 1] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     Object[] values_0 = new Object[1];
/* 024 */
/* 025 */     boolean isNull_2 = i.isNullAt(0);
/* 026 */     UTF8String value_2 = isNull_2 ?
/* 027 */     null : (i.getUTF8String(0));
/* 028 */     boolean isNull_1 = true;
/* 029 */     java.lang.String value_1 = null;
/* 030 */     if (!isNull_2) {
/* 031 */
/* 032 */       isNull_1 = false;
/* 033 */       if (!isNull_1) {
/* 034 */
/* 035 */         Object funcResult_0 = null;
/* 036 */         funcResult_0 = value_2.toString();
/* 037 */         value_1 = (java.lang.String) funcResult_0;
/* 038 */
/* 039 */       }
/* 040 */     }
/* 041 */     if (isNull_1) {
/* 042 */       values_0[0] = null;
/* 043 */     } else {
/* 044 */       values_0[0] = value_1;
/* 045 */     }
/* 046 */
/* 047 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));
/* 048 */     if (false) {
/* 049 */       mutableRow.setNullAt(0);
/* 050 */     } else {
/* 051 */
/* 052 */       mutableRow.update(0, value_0);
/* 053 */     }
/* 054 */
/* 055 */     return mutableRow;
/* 056 */   }
/* 057 */
/* 058 */
/* 059 */ }

22:57:27.178 [Executor task launch worker for task 1] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 59.221041 ms
22:57:27.263 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(51)
22:57:27.263 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 51
22:57:27.263 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 51
22:57:27.263 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(50)
22:57:27.263 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 50
22:57:27.263 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 50
22:57:27.263 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(53)
22:57:27.263 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 53
22:57:27.263 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 53
22:57:27.263 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(52)
22:57:27.263 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 52
22:57:27.263 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 52
22:57:27.263 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(47)
22:57:27.263 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 47
22:57:27.263 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 47
22:57:27.263 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(57)
22:57:27.263 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 57
22:57:27.264 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 57
22:57:27.264 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(49)
22:57:27.264 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 49
22:57:27.264 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 49
22:57:27.264 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(54)
22:57:27.264 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 54
22:57:27.264 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 54
22:57:27.264 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(56)
22:57:27.264 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 56
22:57:27.264 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 56
22:57:27.264 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(55)
22:57:27.264 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 55
22:57:27.264 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 55
22:57:27.264 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(48)
22:57:27.264 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 48
22:57:27.264 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 48
22:57:27.715 [Executor task launch worker for task 1] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection - code for encodeusingserializer(input[0, java.lang.Object, true], true):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.serializer.KryoSerializerInstance kryoSerializer;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 010 */
/* 011 */   public SpecificUnsafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */
/* 014 */     kryoSerializer = (org.apache.spark.serializer.KryoSerializerInstance) org.apache.spark.sql.catalyst.expressions.objects.SerializerSupport$.MODULE$.newSerializer(true);
/* 015 */
/* 016 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   public void initialize(int partitionIndex) {
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   // Scala.Function1 need this
/* 025 */   public java.lang.Object apply(java.lang.Object row) {
/* 026 */     return apply((InternalRow) row);
/* 027 */   }
/* 028 */
/* 029 */   public UnsafeRow apply(InternalRow i) {
/* 030 */     mutableStateArray_0[0].reset();
/* 031 */
/* 032 */
/* 033 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 034 */
/* 035 */     boolean isNull_1 = i.isNullAt(0);
/* 036 */     java.lang.Object value_1 = isNull_1 ?
/* 037 */     null : ((java.lang.Object)i.get(0, null));
/* 038 */     final byte[] value_0 =
/* 039 */     isNull_1 ? null : kryoSerializer.serialize(value_1, null).array();
/* 040 */     if (isNull_1) {
/* 041 */       mutableStateArray_0[0].setNullAt(0);
/* 042 */     } else {
/* 043 */       mutableStateArray_0[0].write(0, value_0);
/* 044 */     }
/* 045 */     return (mutableStateArray_0[0].getRow());
/* 046 */   }
/* 047 */
/* 048 */
/* 049 */ }

22:57:27.718 [Executor task launch worker for task 1] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.serializer.KryoSerializerInstance kryoSerializer;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 010 */
/* 011 */   public SpecificUnsafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */
/* 014 */     kryoSerializer = (org.apache.spark.serializer.KryoSerializerInstance) org.apache.spark.sql.catalyst.expressions.objects.SerializerSupport$.MODULE$.newSerializer(true);
/* 015 */
/* 016 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   public void initialize(int partitionIndex) {
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   // Scala.Function1 need this
/* 025 */   public java.lang.Object apply(java.lang.Object row) {
/* 026 */     return apply((InternalRow) row);
/* 027 */   }
/* 028 */
/* 029 */   public UnsafeRow apply(InternalRow i) {
/* 030 */     mutableStateArray_0[0].reset();
/* 031 */
/* 032 */
/* 033 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 034 */
/* 035 */     boolean isNull_1 = i.isNullAt(0);
/* 036 */     java.lang.Object value_1 = isNull_1 ?
/* 037 */     null : ((java.lang.Object)i.get(0, null));
/* 038 */     final byte[] value_0 =
/* 039 */     isNull_1 ? null : kryoSerializer.serialize(value_1, null).array();
/* 040 */     if (isNull_1) {
/* 041 */       mutableStateArray_0[0].setNullAt(0);
/* 042 */     } else {
/* 043 */       mutableStateArray_0[0].write(0, value_0);
/* 044 */     }
/* 045 */     return (mutableStateArray_0[0].getRow());
/* 046 */   }
/* 047 */
/* 048 */
/* 049 */ }

22:57:27.862 [Executor task launch worker for task 1] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 146.424541 ms
22:57:28.352 [Executor task launch worker for task 1] DEBUG org.apache.spark.shuffle.sort.io.LocalDiskShuffleMapOutputWriter - Writing shuffle index file for mapId 1 with length 1
22:57:28.361 [Executor task launch worker for task 1] DEBUG org.apache.spark.shuffle.IndexShuffleBlockResolver - Shuffle index for mapId 1: [608]
22:57:28.380 [Executor task launch worker for task 1] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 1.0 (TID 1). 2326 bytes result sent to driver
22:57:28.380 [Executor task launch worker for task 1] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - removing (1, 0) from stageTCMP
22:57:28.389 [dispatcher-event-loop-0] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_1.0, runningTasks: 0
22:57:28.389 [dispatcher-event-loop-0] DEBUG org.apache.spark.scheduler.TaskSetManager - No tasks for locality level NO_PREF, so moving to locality level ANY
22:57:28.400 [task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 1597 ms on saras-air (executor driver) (1/1)
22:57:28.400 [task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 1.0, whose tasks have all completed, from pool 
22:57:28.403 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - ShuffleMapTask finished on driver
22:57:28.405 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 1 (collect at StringIndexer.scala:204) finished in 1.645 s
22:57:28.406 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
22:57:28.407 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()
22:57:28.408 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 2)
22:57:28.408 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()
22:57:28.408 [dag-scheduler-event-loop] DEBUG org.apache.spark.MapOutputTrackerMaster - Increasing epoch to 1
22:57:28.413 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - submitStage(ResultStage 2 (name=collect at StringIndexer.scala:204;jobs=1))
22:57:28.413 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - missing: List()
22:57:28.414 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 2 (MapPartitionsRDD[17] at collect at StringIndexer.scala:204), which has no missing parents
22:57:28.414 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - submitMissingTasks(ResultStage 2)
22:57:28.448 [dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 20.9 KiB, free 1048.6 MiB)
22:57:28.449 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_5 locally took 4 ms
22:57:28.449 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_5 without replication took 4 ms
22:57:28.454 [dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 10.4 KiB, free 1048.5 MiB)
22:57:28.456 [dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on saras-air:55957 (size: 10.4 KiB, free: 1048.8 MiB)
22:57:28.456 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_5_piece0
22:57:28.457 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_5_piece0
22:57:28.457 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_5_piece0 locally took 3 ms
22:57:28.460 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_5_piece0 without replication took 6 ms
22:57:28.468 [dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1223
22:57:28.469 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[17] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))
22:57:28.469 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 2.0 with 1 tasks
22:57:28.470 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Epoch for TaskSet 2.0: 1
22:57:28.484 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Adding pending tasks took 13 ms
22:57:28.485 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Valid locality levels for TaskSet 2.0: NODE_LOCAL, ANY
22:57:28.486 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_2.0, runningTasks: 0
22:57:28.489 [dispatcher-event-loop-1] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 2, saras-air, executor driver, partition 0, NODE_LOCAL, 7325 bytes)
22:57:28.490 [Executor task launch worker for task 2] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 2.0 (TID 2)
22:57:28.502 [Executor task launch worker for task 2] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (2, 0) -> 1
22:57:28.504 [Executor task launch worker for task 2] DEBUG org.apache.spark.storage.BlockManager - Getting local block broadcast_5
22:57:28.505 [Executor task launch worker for task 2] DEBUG org.apache.spark.storage.BlockManager - Level for block broadcast_5 is StorageLevel(disk, memory, deserialized, 1 replicas)
22:57:28.539 [Executor task launch worker for task 2] DEBUG org.apache.spark.MapOutputTrackerMaster - Fetching outputs for shuffle 0, partitions 0-1
22:57:28.556 [Executor task launch worker for task 2] DEBUG org.apache.spark.storage.ShuffleBlockFetcherIterator - maxBytesInFlight: 50331648, targetRemoteRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
22:57:28.567 [Executor task launch worker for task 2] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (652.0 B) non-empty blocks including 1 (652.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
22:57:28.568 [Executor task launch worker for task 2] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 13 ms
22:57:28.569 [Executor task launch worker for task 2] DEBUG org.apache.spark.storage.ShuffleBlockFetcherIterator - Start fetching local blocks: (shuffle_0_1_0,0)
22:57:28.574 [Executor task launch worker for task 2] DEBUG org.apache.spark.storage.ShuffleBlockFetcherIterator - Got local blocks in 19 ms
22:57:28.594 [Executor task launch worker for task 2] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection - code for noop$():
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificMutableProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificMutableProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseMutableProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificMutableProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = new org.apache.spark.sql.catalyst.expressions.GenericInternalRow(1);
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public org.apache.spark.sql.catalyst.expressions.codegen.BaseMutableProjection target(InternalRow row) {
/* 022 */     mutableRow = row;
/* 023 */     return this;
/* 024 */   }
/* 025 */
/* 026 */   /* Provide immutable access to the last projected row. */
/* 027 */   public InternalRow currentValue() {
/* 028 */     return (InternalRow) mutableRow;
/* 029 */   }
/* 030 */
/* 031 */   public java.lang.Object apply(java.lang.Object _i) {
/* 032 */     InternalRow i = (InternalRow) _i;
/* 033 */
/* 034 */
/* 035 */     // copy all the results into MutableRow
/* 036 */
/* 037 */     return mutableRow;
/* 038 */   }
/* 039 */
/* 040 */
/* 041 */ }

22:57:28.599 [Executor task launch worker for task 2] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection - code for noop$():
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificMutableProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificMutableProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseMutableProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificMutableProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = new org.apache.spark.sql.catalyst.expressions.GenericInternalRow(1);
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public org.apache.spark.sql.catalyst.expressions.codegen.BaseMutableProjection target(InternalRow row) {
/* 022 */     mutableRow = row;
/* 023 */     return this;
/* 024 */   }
/* 025 */
/* 026 */   /* Provide immutable access to the last projected row. */
/* 027 */   public InternalRow currentValue() {
/* 028 */     return (InternalRow) mutableRow;
/* 029 */   }
/* 030 */
/* 031 */   public java.lang.Object apply(java.lang.Object _i) {
/* 032 */     InternalRow i = (InternalRow) _i;
/* 033 */
/* 034 */
/* 035 */     // copy all the results into MutableRow
/* 036 */
/* 037 */     return mutableRow;
/* 038 */   }
/* 039 */
/* 040 */
/* 041 */ }

22:57:28.601 [Executor task launch worker for task 2] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

22:57:28.607 [Executor task launch worker for task 2] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection - code for noop$():
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificMutableProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificMutableProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseMutableProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificMutableProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = new org.apache.spark.sql.catalyst.expressions.GenericInternalRow(1);
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public org.apache.spark.sql.catalyst.expressions.codegen.BaseMutableProjection target(InternalRow row) {
/* 022 */     mutableRow = row;
/* 023 */     return this;
/* 024 */   }
/* 025 */
/* 026 */   /* Provide immutable access to the last projected row. */
/* 027 */   public InternalRow currentValue() {
/* 028 */     return (InternalRow) mutableRow;
/* 029 */   }
/* 030 */
/* 031 */   public java.lang.Object apply(java.lang.Object _i) {
/* 032 */     InternalRow i = (InternalRow) _i;
/* 033 */
/* 034 */
/* 035 */     // copy all the results into MutableRow
/* 036 */
/* 037 */     return mutableRow;
/* 038 */   }
/* 039 */
/* 040 */
/* 041 */ }

22:57:28.623 [Executor task launch worker for task 2] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection - code for input[0, binary, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     byte[] value_0 = isNull_0 ?
/* 033 */     null : (i.getBinary(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

22:57:28.630 [Executor task launch worker for task 2] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection - code for noop$():
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificMutableProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificMutableProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseMutableProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificMutableProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = new org.apache.spark.sql.catalyst.expressions.GenericInternalRow(1);
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public org.apache.spark.sql.catalyst.expressions.codegen.BaseMutableProjection target(InternalRow row) {
/* 022 */     mutableRow = row;
/* 023 */     return this;
/* 024 */   }
/* 025 */
/* 026 */   /* Provide immutable access to the last projected row. */
/* 027 */   public InternalRow currentValue() {
/* 028 */     return (InternalRow) mutableRow;
/* 029 */   }
/* 030 */
/* 031 */   public java.lang.Object apply(java.lang.Object _i) {
/* 032 */     InternalRow i = (InternalRow) _i;
/* 033 */
/* 034 */
/* 035 */     // copy all the results into MutableRow
/* 036 */
/* 037 */     return mutableRow;
/* 038 */   }
/* 039 */
/* 040 */
/* 041 */ }

22:57:28.634 [Executor task launch worker for task 2] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection - code for decodeusingserializer(input[0, binary, true], Array[org.apache.spark.util.collection.OpenHashMap], true):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */   private org.apache.spark.serializer.KryoSerializerInstance kryoSerializer;
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */     kryoSerializer = (org.apache.spark.serializer.KryoSerializerInstance) org.apache.spark.sql.catalyst.expressions.objects.SerializerSupport$.MODULE$.newSerializer(true);
/* 016 */
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   public void initialize(int partitionIndex) {
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   public java.lang.Object apply(java.lang.Object _i) {
/* 025 */     InternalRow i = (InternalRow) _i;
/* 026 */     boolean isNull_1 = i.isNullAt(0);
/* 027 */     byte[] value_1 = isNull_1 ?
/* 028 */     null : (i.getBinary(0));
/* 029 */     final org.apache.spark.util.collection.OpenHashMap[] value_0 =
/* 030 */     isNull_1 ? null : (org.apache.spark.util.collection.OpenHashMap[]) kryoSerializer.deserialize(java.nio.ByteBuffer.wrap(value_1), null);
/* 031 */     if (isNull_1) {
/* 032 */       mutableRow.setNullAt(0);
/* 033 */     } else {
/* 034 */
/* 035 */       mutableRow.update(0, value_0);
/* 036 */     }
/* 037 */
/* 038 */     return mutableRow;
/* 039 */   }
/* 040 */
/* 041 */
/* 042 */ }

22:57:28.635 [Executor task launch worker for task 2] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */   private org.apache.spark.serializer.KryoSerializerInstance kryoSerializer;
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */     kryoSerializer = (org.apache.spark.serializer.KryoSerializerInstance) org.apache.spark.sql.catalyst.expressions.objects.SerializerSupport$.MODULE$.newSerializer(true);
/* 016 */
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   public void initialize(int partitionIndex) {
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   public java.lang.Object apply(java.lang.Object _i) {
/* 025 */     InternalRow i = (InternalRow) _i;
/* 026 */     boolean isNull_1 = i.isNullAt(0);
/* 027 */     byte[] value_1 = isNull_1 ?
/* 028 */     null : (i.getBinary(0));
/* 029 */     final org.apache.spark.util.collection.OpenHashMap[] value_0 =
/* 030 */     isNull_1 ? null : (org.apache.spark.util.collection.OpenHashMap[]) kryoSerializer.deserialize(java.nio.ByteBuffer.wrap(value_1), null);
/* 031 */     if (isNull_1) {
/* 032 */       mutableRow.setNullAt(0);
/* 033 */     } else {
/* 034 */
/* 035 */       mutableRow.update(0, value_0);
/* 036 */     }
/* 037 */
/* 038 */     return mutableRow;
/* 039 */   }
/* 040 */
/* 041 */
/* 042 */ }

22:57:28.698 [Executor task launch worker for task 2] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 63.65075 ms
22:57:28.798 [Executor task launch worker for task 2] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection - code for encodeusingserializer(input[0, java.lang.Object, true], true):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.serializer.KryoSerializerInstance kryoSerializer;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 010 */
/* 011 */   public SpecificUnsafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */
/* 014 */     kryoSerializer = (org.apache.spark.serializer.KryoSerializerInstance) org.apache.spark.sql.catalyst.expressions.objects.SerializerSupport$.MODULE$.newSerializer(true);
/* 015 */
/* 016 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   public void initialize(int partitionIndex) {
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   // Scala.Function1 need this
/* 025 */   public java.lang.Object apply(java.lang.Object row) {
/* 026 */     return apply((InternalRow) row);
/* 027 */   }
/* 028 */
/* 029 */   public UnsafeRow apply(InternalRow i) {
/* 030 */     mutableStateArray_0[0].reset();
/* 031 */
/* 032 */
/* 033 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 034 */
/* 035 */     boolean isNull_1 = i.isNullAt(0);
/* 036 */     java.lang.Object value_1 = isNull_1 ?
/* 037 */     null : ((java.lang.Object)i.get(0, null));
/* 038 */     final byte[] value_0 =
/* 039 */     isNull_1 ? null : kryoSerializer.serialize(value_1, null).array();
/* 040 */     if (isNull_1) {
/* 041 */       mutableStateArray_0[0].setNullAt(0);
/* 042 */     } else {
/* 043 */       mutableStateArray_0[0].write(0, value_0);
/* 044 */     }
/* 045 */     return (mutableStateArray_0[0].getRow());
/* 046 */   }
/* 047 */
/* 048 */
/* 049 */ }

22:57:28.817 [Executor task launch worker for task 2] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 2.0 (TID 2). 3733 bytes result sent to driver
22:57:28.818 [Executor task launch worker for task 2] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - removing (2, 0) from stageTCMP
22:57:28.830 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_2.0, runningTasks: 0
22:57:28.831 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSetManager - No tasks for locality level NODE_LOCAL, so moving to locality level ANY
22:57:28.835 [task-result-getter-2] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 2) in 349 ms on saras-air (executor driver) (1/1)
22:57:28.836 [task-result-getter-2] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 2.0, whose tasks have all completed, from pool 
22:57:28.841 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 2 (collect at StringIndexer.scala:204) finished in 0.400 s
22:57:28.841 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - After removal of stage 2, remaining stages = 1
22:57:28.841 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - After removal of stage 1, remaining stages = 0
22:57:28.841 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
22:57:28.842 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 2: Stage finished
22:57:28.843 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.scheduler.DAGScheduler - Job 1 finished: collect at StringIndexer.scala:204, took 2.099594 s
22:57:28.850 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection - code for decodeusingserializer(input[0, binary, true], Array[org.apache.spark.util.collection.OpenHashMap], true):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */   private org.apache.spark.serializer.KryoSerializerInstance kryoSerializer;
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */     kryoSerializer = (org.apache.spark.serializer.KryoSerializerInstance) org.apache.spark.sql.catalyst.expressions.objects.SerializerSupport$.MODULE$.newSerializer(true);
/* 016 */
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   public void initialize(int partitionIndex) {
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   public java.lang.Object apply(java.lang.Object _i) {
/* 025 */     InternalRow i = (InternalRow) _i;
/* 026 */     boolean isNull_1 = i.isNullAt(0);
/* 027 */     byte[] value_1 = isNull_1 ?
/* 028 */     null : (i.getBinary(0));
/* 029 */     final org.apache.spark.util.collection.OpenHashMap[] value_0 =
/* 030 */     isNull_1 ? null : (org.apache.spark.util.collection.OpenHashMap[]) kryoSerializer.deserialize(java.nio.ByteBuffer.wrap(value_1), null);
/* 031 */     if (isNull_1) {
/* 032 */       mutableRow.setNullAt(0);
/* 033 */     } else {
/* 034 */
/* 035 */       mutableRow.update(0, value_0);
/* 036 */     }
/* 037 */
/* 038 */     return mutableRow;
/* 039 */   }
/* 040 */
/* 041 */
/* 042 */ }

22:57:28.914 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.ml.feature.StringIndexerModel - Input schema: {"type":"struct","fields":[{"name":"Title","type":"string","nullable":true,"metadata":{}},{"name":"Company","type":"string","nullable":true,"metadata":{}},{"name":"Location","type":"string","nullable":true,"metadata":{}},{"name":"Type","type":"string","nullable":true,"metadata":{}},{"name":"Level","type":"string","nullable":true,"metadata":{}},{"name":"YearsExp","type":"string","nullable":true,"metadata":{}},{"name":"Country","type":"string","nullable":true,"metadata":{}},{"name":"Skills","type":"string","nullable":true,"metadata":{}}]}
22:57:28.918 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.ml.feature.StringIndexerModel - Expected output schema: {"type":"struct","fields":[{"name":"Title","type":"string","nullable":true,"metadata":{}},{"name":"Company","type":"string","nullable":true,"metadata":{}},{"name":"Location","type":"string","nullable":true,"metadata":{}},{"name":"Type","type":"string","nullable":true,"metadata":{}},{"name":"Level","type":"string","nullable":true,"metadata":{}},{"name":"YearsExp","type":"string","nullable":true,"metadata":{}},{"name":"Country","type":"string","nullable":true,"metadata":{}},{"name":"Skills","type":"string","nullable":true,"metadata":{}},{"name":"type-factorized","type":"double","nullable":false,"metadata":{"ml_attr":{"type":"nominal"}}}]}
+++++========++++++++Done
root
 |-- Title: string (nullable = true)
 |-- Company: string (nullable = true)
 |-- Location: string (nullable = true)
 |-- Type: string (nullable = true)
 |-- Level: string (nullable = true)
 |-- YearsExp: string (nullable = true)
 |-- Country: string (nullable = true)
 |-- Skills: string (nullable = true)
 |-- type-factorized: double (nullable = false)

22:57:29.072 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.ml.feature.StringIndexer - Input schema: {"type":"struct","fields":[{"name":"Title","type":"string","nullable":true,"metadata":{}},{"name":"Company","type":"string","nullable":true,"metadata":{}},{"name":"Location","type":"string","nullable":true,"metadata":{}},{"name":"Type","type":"string","nullable":true,"metadata":{}},{"name":"Level","type":"string","nullable":true,"metadata":{}},{"name":"YearsExp","type":"string","nullable":true,"metadata":{}},{"name":"Country","type":"string","nullable":true,"metadata":{}},{"name":"Skills","type":"string","nullable":true,"metadata":{}},{"name":"type-factorized","type":"double","nullable":false,"metadata":{"ml_attr":{"vals":["Full Time","Internship","Part Time","Freelance / Project","Work From Home","Shift Based","__unknown"],"type":"nominal","name":"type-factorized"}}}]}
22:57:29.073 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.ml.feature.StringIndexer - Expected output schema: {"type":"struct","fields":[{"name":"Title","type":"string","nullable":true,"metadata":{}},{"name":"Company","type":"string","nullable":true,"metadata":{}},{"name":"Location","type":"string","nullable":true,"metadata":{}},{"name":"Type","type":"string","nullable":true,"metadata":{}},{"name":"Level","type":"string","nullable":true,"metadata":{}},{"name":"YearsExp","type":"string","nullable":true,"metadata":{}},{"name":"Country","type":"string","nullable":true,"metadata":{}},{"name":"Skills","type":"string","nullable":true,"metadata":{}},{"name":"type-factorized","type":"double","nullable":false,"metadata":{"ml_attr":{"vals":["Full Time","Internship","Part Time","Freelance / Project","Work From Home","Shift Based","__unknown"],"type":"nominal","name":"type-factorized"}}},{"name":"level-factorized","type":"double","nullable":false,"metadata":{"ml_attr":{"type":"nominal"}}}]}
22:57:29.208 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: 
22:57:29.208 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: 
22:57:29.208 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: 
22:57:29.208 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<Level: string>
22:57:29.242 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.sql.execution.WholeStageCodegenExec - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator inputadapter_input_0;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] project_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     inputadapter_input_0 = inputs[0];
/* 020 */     project_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   protected void processNext() throws java.io.IOException {
/* 025 */     while ( inputadapter_input_0.hasNext()) {
/* 026 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();
/* 027 */
/* 028 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);
/* 029 */       UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?
/* 030 */       null : (inputadapter_row_0.getUTF8String(0));
/* 031 */       project_mutableStateArray_0[0].reset();
/* 032 */
/* 033 */       project_mutableStateArray_0[0].zeroOutNullBytes();
/* 034 */
/* 035 */       if (inputadapter_isNull_0) {
/* 036 */         project_mutableStateArray_0[0].setNullAt(0);
/* 037 */       } else {
/* 038 */         project_mutableStateArray_0[0].write(0, inputadapter_value_0);
/* 039 */       }
/* 040 */       append((project_mutableStateArray_0[0].getRow()));
/* 041 */       if (shouldStop()) return;
/* 042 */     }
/* 043 */   }
/* 044 */
/* 045 */ }

22:57:29.249 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 170.5 KiB, free 1048.4 MiB)
22:57:29.250 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_6 locally took 6 ms
22:57:29.250 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_6 without replication took 6 ms
22:57:29.275 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 23.8 KiB, free 1048.4 MiB)
22:57:29.276 [dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on saras-air:55957 (size: 23.8 KiB, free: 1048.7 MiB)
22:57:29.276 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_6_piece0
22:57:29.276 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_6_piece0
22:57:29.276 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_6_piece0 locally took 1 ms
22:57:29.276 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_6_piece0 without replication took 1 ms
22:57:29.279 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.SparkContext - Created broadcast 6 from collect at StringIndexer.scala:204
22:57:29.281 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5114295 bytes, open cost is considered as scanning 4194304 bytes.
22:57:29.286 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$doExecute$4$adapted
22:57:29.293 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++
22:57:29.302 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$collect$2
22:57:29.309 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$collect$2) is now cleaned +++
22:57:29.314 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$runJob$5
22:57:29.321 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++
22:57:29.322 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.SparkContext - Starting job: collect at StringIndexer.scala:204
22:57:29.323 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Registering RDD 22 (collect at StringIndexer.scala:204) as input to shuffle 1
22:57:29.324 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 2 (collect at StringIndexer.scala:204) with 1 output partitions
22:57:29.324 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 4 (collect at StringIndexer.scala:204)
22:57:29.324 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 3)
22:57:29.324 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 3)
22:57:29.324 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - submitStage(ResultStage 4 (name=collect at StringIndexer.scala:204;jobs=2))
22:57:29.324 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - missing: List(ShuffleMapStage 3)
22:57:29.324 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - submitStage(ShuffleMapStage 3 (name=collect at StringIndexer.scala:204;jobs=2))
22:57:29.325 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - missing: List()
22:57:29.325 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 3 (MapPartitionsRDD[22] at collect at StringIndexer.scala:204), which has no missing parents
22:57:29.325 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - submitMissingTasks(ShuffleMapStage 3)
22:57:29.339 [dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_7 stored as values in memory (estimated size 20.9 KiB, free 1048.3 MiB)
22:57:29.339 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_7 locally took 7 ms
22:57:29.339 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_7 without replication took 7 ms
22:57:29.362 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(63)
22:57:29.363 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 63
22:57:29.363 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 63
22:57:29.363 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(61)
22:57:29.363 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 61
22:57:29.363 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 61
22:57:29.363 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(123)
22:57:29.363 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 123
22:57:29.363 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 123
22:57:29.363 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(59)
22:57:29.363 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 59
22:57:29.363 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 59
22:57:29.363 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanBroadcast(5)
22:57:29.363 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning broadcast 5
22:57:29.363 [Spark Context Cleaner] DEBUG org.apache.spark.broadcast.TorrentBroadcast - Unpersisting TorrentBroadcast 5
22:57:29.365 [block-manager-slave-async-thread-pool-9] DEBUG org.apache.spark.storage.BlockManagerSlaveEndpoint - removing broadcast 5
22:57:29.366 [block-manager-slave-async-thread-pool-9] DEBUG org.apache.spark.storage.BlockManager - Removing broadcast 5
22:57:29.366 [dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_7_piece0 stored as bytes in memory (estimated size 10.2 KiB, free 1048.3 MiB)
22:57:29.366 [block-manager-slave-async-thread-pool-9] DEBUG org.apache.spark.storage.BlockManager - Removing block broadcast_5
22:57:29.369 [dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on saras-air:55957 (size: 10.2 KiB, free: 1048.7 MiB)
22:57:29.369 [block-manager-slave-async-thread-pool-9] DEBUG org.apache.spark.storage.memory.MemoryStore - Block broadcast_5 of size 21392 dropped from memory (free 1099274262)
22:57:29.369 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_7_piece0
22:57:29.369 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_7_piece0
22:57:29.369 [block-manager-slave-async-thread-pool-9] DEBUG org.apache.spark.storage.BlockManager - Removing block broadcast_5_piece0
22:57:29.369 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_7_piece0 locally took 5 ms
22:57:29.370 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_7_piece0 without replication took 5 ms
22:57:29.376 [block-manager-slave-async-thread-pool-9] DEBUG org.apache.spark.storage.memory.MemoryStore - Block broadcast_5_piece0 of size 10621 dropped from memory (free 1099284883)
22:57:29.378 [dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 7 from broadcast at DAGScheduler.scala:1223
22:57:29.378 [dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_5_piece0 on saras-air:55957 in memory (size: 10.4 KiB, free: 1048.7 MiB)
22:57:29.379 [block-manager-slave-async-thread-pool-9] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_5_piece0
22:57:29.379 [block-manager-slave-async-thread-pool-9] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_5_piece0
22:57:29.382 [block-manager-slave-async-thread-pool-11] DEBUG org.apache.spark.storage.BlockManagerSlaveEndpoint - Done removing broadcast 5, response is 0
22:57:29.383 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[22] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))
22:57:29.383 [block-manager-slave-async-thread-pool-11] DEBUG org.apache.spark.storage.BlockManagerSlaveEndpoint - Sent response: 0 to saras-air:55956
22:57:29.383 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 3.0 with 1 tasks
22:57:29.384 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned broadcast 5
22:57:29.384 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(97)
22:57:29.384 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 97
22:57:29.384 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 97
22:57:29.384 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(73)
22:57:29.384 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 73
22:57:29.384 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 73
22:57:29.384 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(130)
22:57:29.384 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 130
22:57:29.384 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 130
22:57:29.384 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(87)
22:57:29.384 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 87
22:57:29.384 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 87
22:57:29.384 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(117)
22:57:29.384 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 117
22:57:29.385 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 117
22:57:29.385 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(118)
22:57:29.385 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 118
22:57:29.385 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 118
22:57:29.385 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(74)
22:57:29.385 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 74
22:57:29.385 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 74
22:57:29.385 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(90)
22:57:29.385 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 90
22:57:29.385 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 90
22:57:29.385 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(68)
22:57:29.385 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 68
22:57:29.385 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 68
22:57:29.385 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(122)
22:57:29.385 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 122
22:57:29.385 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 122
22:57:29.385 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(80)
22:57:29.385 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 80
22:57:29.385 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 80
22:57:29.385 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(134)
22:57:29.385 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 134
22:57:29.385 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 134
22:57:29.385 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(112)
22:57:29.385 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 112
22:57:29.385 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 112
22:57:29.385 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(71)
22:57:29.385 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 71
22:57:29.385 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 71
22:57:29.386 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(104)
22:57:29.386 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 104
22:57:29.386 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Epoch for TaskSet 3.0: 1
22:57:29.386 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 104
22:57:29.386 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Adding pending tasks took 0 ms
22:57:29.386 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(83)
22:57:29.386 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 83
22:57:29.387 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Valid locality levels for TaskSet 3.0: NO_PREF, ANY
22:57:29.387 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 83
22:57:29.388 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(92)
22:57:29.393 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 92
22:57:29.394 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 92
22:57:29.395 [dispatcher-event-loop-0] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_3.0, runningTasks: 0
22:57:29.395 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanBroadcast(3)
22:57:29.395 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning broadcast 3
22:57:29.395 [Spark Context Cleaner] DEBUG org.apache.spark.broadcast.TorrentBroadcast - Unpersisting TorrentBroadcast 3
22:57:29.396 [dispatcher-event-loop-0] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 3.0 (TID 3, saras-air, executor driver, partition 0, PROCESS_LOCAL, 7797 bytes)
22:57:29.398 [block-manager-slave-async-thread-pool-12] DEBUG org.apache.spark.storage.BlockManagerSlaveEndpoint - removing broadcast 3
22:57:29.398 [block-manager-slave-async-thread-pool-12] DEBUG org.apache.spark.storage.BlockManager - Removing broadcast 3
22:57:29.398 [block-manager-slave-async-thread-pool-12] DEBUG org.apache.spark.storage.BlockManager - Removing block broadcast_3_piece0
22:57:29.402 [block-manager-slave-async-thread-pool-12] DEBUG org.apache.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 of size 24400 dropped from memory (free 1099309283)
22:57:29.406 [dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_3_piece0 on saras-air:55957 in memory (size: 23.8 KiB, free: 1048.8 MiB)
22:57:29.406 [Executor task launch worker for task 3] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 3.0 (TID 3)
22:57:29.407 [block-manager-slave-async-thread-pool-12] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_3_piece0
22:57:29.407 [block-manager-slave-async-thread-pool-12] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_3_piece0
22:57:29.408 [block-manager-slave-async-thread-pool-12] DEBUG org.apache.spark.storage.BlockManager - Removing block broadcast_3
22:57:29.408 [block-manager-slave-async-thread-pool-12] DEBUG org.apache.spark.storage.memory.MemoryStore - Block broadcast_3 of size 174584 dropped from memory (free 1099483867)
22:57:29.411 [block-manager-slave-async-thread-pool-14] DEBUG org.apache.spark.storage.BlockManagerSlaveEndpoint - Done removing broadcast 3, response is 0
22:57:29.411 [block-manager-slave-async-thread-pool-14] DEBUG org.apache.spark.storage.BlockManagerSlaveEndpoint - Sent response: 0 to saras-air:55956
22:57:29.413 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned broadcast 3
22:57:29.413 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(128)
22:57:29.413 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 128
22:57:29.413 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 128
22:57:29.413 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(65)
22:57:29.413 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 65
22:57:29.413 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 65
22:57:29.413 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(116)
22:57:29.414 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 116
22:57:29.414 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 116
22:57:29.414 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(139)
22:57:29.414 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 139
22:57:29.414 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 139
22:57:29.414 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(103)
22:57:29.414 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 103
22:57:29.414 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 103
22:57:29.414 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(89)
22:57:29.414 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 89
22:57:29.414 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 89
22:57:29.414 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(107)
22:57:29.414 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 107
22:57:29.414 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 107
22:57:29.414 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(126)
22:57:29.414 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 126
22:57:29.414 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 126
22:57:29.415 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(99)
22:57:29.415 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 99
22:57:29.415 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 99
22:57:29.415 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(76)
22:57:29.415 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 76
22:57:29.415 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 76
22:57:29.415 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(82)
22:57:29.415 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 82
22:57:29.415 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 82
22:57:29.415 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(79)
22:57:29.415 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 79
22:57:29.415 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 79
22:57:29.415 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(75)
22:57:29.415 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 75
22:57:29.415 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 75
22:57:29.415 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(129)
22:57:29.415 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 129
22:57:29.415 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 129
22:57:29.415 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(88)
22:57:29.415 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 88
22:57:29.415 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 88
22:57:29.415 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(102)
22:57:29.415 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 102
22:57:29.415 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 102
22:57:29.415 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanShuffle(0)
22:57:29.425 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning shuffle 0
22:57:29.537 [Executor task launch worker for task 3] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (3, 0) -> 1
22:57:29.557 [block-manager-slave-async-thread-pool-15] DEBUG org.apache.spark.storage.BlockManagerSlaveEndpoint - removing shuffle 0
22:57:29.561 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned shuffle 0
22:57:29.565 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(96)
22:57:29.566 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 96
22:57:29.567 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 96
22:57:29.567 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(127)
22:57:29.567 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 127
22:57:29.567 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 127
22:57:29.567 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(66)
22:57:29.567 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 66
22:57:29.567 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 66
22:57:29.567 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(108)
22:57:29.567 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 108
22:57:29.567 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 108
22:57:29.567 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(124)
22:57:29.567 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 124
22:57:29.567 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 124
22:57:29.567 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(62)
22:57:29.567 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 62
22:57:29.567 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 62
22:57:29.567 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(64)
22:57:29.567 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 64
22:57:29.567 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 64
22:57:29.567 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(131)
22:57:29.567 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 131
22:57:29.574 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 131
22:57:29.574 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(67)
22:57:29.574 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 67
22:57:29.575 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 67
22:57:29.575 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(133)
22:57:29.575 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 133
22:57:29.575 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 133
22:57:29.575 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(113)
22:57:29.575 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 113
22:57:29.575 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 113
22:57:29.575 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(135)
22:57:29.575 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 135
22:57:29.575 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 135
22:57:29.575 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(84)
22:57:29.575 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 84
22:57:29.575 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 84
22:57:29.575 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(58)
22:57:29.575 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 58
22:57:29.575 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 58
22:57:29.575 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(121)
22:57:29.575 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 121
22:57:29.575 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 121
22:57:29.575 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(136)
22:57:29.575 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 136
22:57:29.575 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 136
22:57:29.575 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(110)
22:57:29.575 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 110
22:57:29.575 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 110
22:57:29.575 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(106)
22:57:29.575 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 106
22:57:29.575 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 106
22:57:29.575 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(111)
22:57:29.575 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 111
22:57:29.575 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 111
22:57:29.575 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(105)
22:57:29.575 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 105
22:57:29.575 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 105
22:57:29.575 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(77)
22:57:29.575 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 77
22:57:29.575 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 77
22:57:29.575 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(91)
22:57:29.575 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 91
22:57:29.575 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 91
22:57:29.575 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(137)
22:57:29.575 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 137
22:57:29.575 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 137
22:57:29.575 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(81)
22:57:29.575 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 81
22:57:29.575 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 81
22:57:29.575 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(95)
22:57:29.575 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 95
22:57:29.575 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 95
22:57:29.575 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(125)
22:57:29.575 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 125
22:57:29.575 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 125
22:57:29.575 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(98)
22:57:29.575 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 98
22:57:29.575 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 98
22:57:29.575 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(93)
22:57:29.575 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 93
22:57:29.575 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 93
22:57:29.575 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(86)
22:57:29.575 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 86
22:57:29.575 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 86
22:57:29.575 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(132)
22:57:29.575 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 132
22:57:29.575 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 132
22:57:29.575 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(120)
22:57:29.575 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 120
22:57:29.575 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 120
22:57:29.575 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanBroadcast(4)
22:57:29.575 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning broadcast 4
22:57:29.575 [Spark Context Cleaner] DEBUG org.apache.spark.broadcast.TorrentBroadcast - Unpersisting TorrentBroadcast 4
22:57:29.584 [block-manager-slave-async-thread-pool-16] DEBUG org.apache.spark.storage.BlockManagerSlaveEndpoint - removing broadcast 4
22:57:29.584 [block-manager-slave-async-thread-pool-16] DEBUG org.apache.spark.storage.BlockManager - Removing broadcast 4
22:57:29.588 [block-manager-slave-async-thread-pool-16] DEBUG org.apache.spark.storage.BlockManager - Removing block broadcast_4_piece0
22:57:29.595 [Executor task launch worker for task 3] DEBUG org.apache.spark.storage.BlockManager - Getting local block broadcast_7
22:57:29.597 [block-manager-slave-async-thread-pool-18] DEBUG org.apache.spark.storage.BlockManagerSlaveEndpoint - Done removing shuffle 0, response is true
22:57:29.597 [block-manager-slave-async-thread-pool-18] DEBUG org.apache.spark.storage.BlockManagerSlaveEndpoint - Sent response: true to saras-air:55956
22:57:29.600 [Executor task launch worker for task 3] DEBUG org.apache.spark.storage.BlockManager - Level for block broadcast_7 is StorageLevel(disk, memory, deserialized, 1 replicas)
22:57:29.600 [block-manager-slave-async-thread-pool-16] DEBUG org.apache.spark.storage.memory.MemoryStore - Block broadcast_4_piece0 of size 10446 dropped from memory (free 1099494313)
22:57:29.608 [dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_4_piece0 on saras-air:55957 in memory (size: 10.2 KiB, free: 1048.8 MiB)
22:57:29.609 [block-manager-slave-async-thread-pool-16] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_4_piece0
22:57:29.609 [block-manager-slave-async-thread-pool-16] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_4_piece0
22:57:29.611 [block-manager-slave-async-thread-pool-16] DEBUG org.apache.spark.storage.BlockManager - Removing block broadcast_4
22:57:29.611 [block-manager-slave-async-thread-pool-16] DEBUG org.apache.spark.storage.memory.MemoryStore - Block broadcast_4 of size 21384 dropped from memory (free 1099515697)
22:57:29.613 [block-manager-slave-async-thread-pool-20] DEBUG org.apache.spark.storage.BlockManagerSlaveEndpoint - Done removing broadcast 4, response is 0
22:57:29.613 [block-manager-slave-async-thread-pool-20] DEBUG org.apache.spark.storage.BlockManagerSlaveEndpoint - Sent response: 0 to saras-air:55956
22:57:29.614 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned broadcast 4
22:57:29.615 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(85)
22:57:29.615 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 85
22:57:29.615 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 85
22:57:29.615 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(72)
22:57:29.615 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 72
22:57:29.615 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 72
22:57:29.615 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(138)
22:57:29.615 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 138
22:57:29.615 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 138
22:57:29.615 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(70)
22:57:29.615 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 70
22:57:29.615 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 70
22:57:29.615 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(94)
22:57:29.615 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 94
22:57:29.615 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 94
22:57:29.615 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(69)
22:57:29.615 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 69
22:57:29.615 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 69
22:57:29.615 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(78)
22:57:29.615 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 78
22:57:29.615 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 78
22:57:29.615 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(115)
22:57:29.615 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 115
22:57:29.615 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 115
22:57:29.615 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(101)
22:57:29.615 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 101
22:57:29.615 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 101
22:57:29.615 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(60)
22:57:29.615 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 60
22:57:29.615 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 60
22:57:29.615 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(119)
22:57:29.615 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 119
22:57:29.615 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 119
22:57:29.615 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(100)
22:57:29.615 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 100
22:57:29.615 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 100
22:57:29.615 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(109)
22:57:29.615 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 109
22:57:29.615 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 109
22:57:29.615 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(114)
22:57:29.615 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 114
22:57:29.615 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 114
22:57:29.635 [Executor task launch worker for task 3] INFO org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///Users/sue/Documents/ITI/java-for-ml/final-project/WuzzufJobsAnalysis/src/main/resources/Wuzzuf_Jobs.csv, range: 0-919991, partition values: [empty row]
22:57:29.650 [Executor task launch worker for task 3] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection - code for input[0, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     UTF8String value_0 = isNull_0 ?
/* 033 */     null : (i.getUTF8String(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

22:57:29.651 [Executor task launch worker for task 3] DEBUG org.apache.spark.storage.BlockManager - Getting local block broadcast_6
22:57:29.651 [Executor task launch worker for task 3] DEBUG org.apache.spark.storage.BlockManager - Level for block broadcast_6 is StorageLevel(disk, memory, deserialized, 1 replicas)
22:57:29.698 [Executor task launch worker for task 3] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection - code for noop$():
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificMutableProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificMutableProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseMutableProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificMutableProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = new org.apache.spark.sql.catalyst.expressions.GenericInternalRow(1);
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public org.apache.spark.sql.catalyst.expressions.codegen.BaseMutableProjection target(InternalRow row) {
/* 022 */     mutableRow = row;
/* 023 */     return this;
/* 024 */   }
/* 025 */
/* 026 */   /* Provide immutable access to the last projected row. */
/* 027 */   public InternalRow currentValue() {
/* 028 */     return (InternalRow) mutableRow;
/* 029 */   }
/* 030 */
/* 031 */   public java.lang.Object apply(java.lang.Object _i) {
/* 032 */     InternalRow i = (InternalRow) _i;
/* 033 */
/* 034 */
/* 035 */     // copy all the results into MutableRow
/* 036 */
/* 037 */     return mutableRow;
/* 038 */   }
/* 039 */
/* 040 */
/* 041 */ }

22:57:29.701 [Executor task launch worker for task 3] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection - code for noop$():
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificMutableProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificMutableProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseMutableProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificMutableProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = new org.apache.spark.sql.catalyst.expressions.GenericInternalRow(1);
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public org.apache.spark.sql.catalyst.expressions.codegen.BaseMutableProjection target(InternalRow row) {
/* 022 */     mutableRow = row;
/* 023 */     return this;
/* 024 */   }
/* 025 */
/* 026 */   /* Provide immutable access to the last projected row. */
/* 027 */   public InternalRow currentValue() {
/* 028 */     return (InternalRow) mutableRow;
/* 029 */   }
/* 030 */
/* 031 */   public java.lang.Object apply(java.lang.Object _i) {
/* 032 */     InternalRow i = (InternalRow) _i;
/* 033 */
/* 034 */
/* 035 */     // copy all the results into MutableRow
/* 036 */
/* 037 */     return mutableRow;
/* 038 */   }
/* 039 */
/* 040 */
/* 041 */ }

22:57:29.703 [Executor task launch worker for task 3] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

22:57:29.706 [Executor task launch worker for task 3] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection - code for input[0, binary, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     byte[] value_0 = isNull_0 ?
/* 033 */     null : (i.getBinary(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

22:57:29.708 [Executor task launch worker for task 3] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection - code for noop$():
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificMutableProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificMutableProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseMutableProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificMutableProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = new org.apache.spark.sql.catalyst.expressions.GenericInternalRow(1);
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public org.apache.spark.sql.catalyst.expressions.codegen.BaseMutableProjection target(InternalRow row) {
/* 022 */     mutableRow = row;
/* 023 */     return this;
/* 024 */   }
/* 025 */
/* 026 */   /* Provide immutable access to the last projected row. */
/* 027 */   public InternalRow currentValue() {
/* 028 */     return (InternalRow) mutableRow;
/* 029 */   }
/* 030 */
/* 031 */   public java.lang.Object apply(java.lang.Object _i) {
/* 032 */     InternalRow i = (InternalRow) _i;
/* 033 */
/* 034 */
/* 035 */     // copy all the results into MutableRow
/* 036 */
/* 037 */     return mutableRow;
/* 038 */   }
/* 039 */
/* 040 */
/* 041 */ }

22:57:29.721 [Executor task launch worker for task 3] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection - code for createexternalrow(input[0, string, true].toString, StructField(Level,StringType,true)):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     Object[] values_0 = new Object[1];
/* 024 */
/* 025 */     boolean isNull_2 = i.isNullAt(0);
/* 026 */     UTF8String value_2 = isNull_2 ?
/* 027 */     null : (i.getUTF8String(0));
/* 028 */     boolean isNull_1 = true;
/* 029 */     java.lang.String value_1 = null;
/* 030 */     if (!isNull_2) {
/* 031 */
/* 032 */       isNull_1 = false;
/* 033 */       if (!isNull_1) {
/* 034 */
/* 035 */         Object funcResult_0 = null;
/* 036 */         funcResult_0 = value_2.toString();
/* 037 */         value_1 = (java.lang.String) funcResult_0;
/* 038 */
/* 039 */       }
/* 040 */     }
/* 041 */     if (isNull_1) {
/* 042 */       values_0[0] = null;
/* 043 */     } else {
/* 044 */       values_0[0] = value_1;
/* 045 */     }
/* 046 */
/* 047 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));
/* 048 */     if (false) {
/* 049 */       mutableRow.setNullAt(0);
/* 050 */     } else {
/* 051 */
/* 052 */       mutableRow.update(0, value_0);
/* 053 */     }
/* 054 */
/* 055 */     return mutableRow;
/* 056 */   }
/* 057 */
/* 058 */
/* 059 */ }

22:57:29.861 [Executor task launch worker for task 3] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection - code for encodeusingserializer(input[0, java.lang.Object, true], true):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.serializer.KryoSerializerInstance kryoSerializer;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 010 */
/* 011 */   public SpecificUnsafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */
/* 014 */     kryoSerializer = (org.apache.spark.serializer.KryoSerializerInstance) org.apache.spark.sql.catalyst.expressions.objects.SerializerSupport$.MODULE$.newSerializer(true);
/* 015 */
/* 016 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   public void initialize(int partitionIndex) {
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   // Scala.Function1 need this
/* 025 */   public java.lang.Object apply(java.lang.Object row) {
/* 026 */     return apply((InternalRow) row);
/* 027 */   }
/* 028 */
/* 029 */   public UnsafeRow apply(InternalRow i) {
/* 030 */     mutableStateArray_0[0].reset();
/* 031 */
/* 032 */
/* 033 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 034 */
/* 035 */     boolean isNull_1 = i.isNullAt(0);
/* 036 */     java.lang.Object value_1 = isNull_1 ?
/* 037 */     null : ((java.lang.Object)i.get(0, null));
/* 038 */     final byte[] value_0 =
/* 039 */     isNull_1 ? null : kryoSerializer.serialize(value_1, null).array();
/* 040 */     if (isNull_1) {
/* 041 */       mutableStateArray_0[0].setNullAt(0);
/* 042 */     } else {
/* 043 */       mutableStateArray_0[0].write(0, value_0);
/* 044 */     }
/* 045 */     return (mutableStateArray_0[0].getRow());
/* 046 */   }
/* 047 */
/* 048 */
/* 049 */ }

22:57:29.993 [Executor task launch worker for task 3] DEBUG org.apache.spark.shuffle.sort.io.LocalDiskShuffleMapOutputWriter - Writing shuffle index file for mapId 3 with length 1
22:57:30.002 [Executor task launch worker for task 3] DEBUG org.apache.spark.shuffle.IndexShuffleBlockResolver - Shuffle index for mapId 3: [661]
22:57:30.061 [Executor task launch worker for task 3] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 3.0 (TID 3). 2283 bytes result sent to driver
22:57:30.062 [Executor task launch worker for task 3] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - removing (3, 0) from stageTCMP
22:57:30.068 [dispatcher-event-loop-0] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_3.0, runningTasks: 0
22:57:30.069 [dispatcher-event-loop-0] DEBUG org.apache.spark.scheduler.TaskSetManager - No tasks for locality level NO_PREF, so moving to locality level ANY
22:57:30.074 [task-result-getter-3] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 3.0 (TID 3) in 679 ms on saras-air (executor driver) (1/1)
22:57:30.075 [task-result-getter-3] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 3.0, whose tasks have all completed, from pool 
22:57:30.080 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - ShuffleMapTask finished on driver
22:57:30.082 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 3 (collect at StringIndexer.scala:204) finished in 0.755 s
22:57:30.083 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages
22:57:30.083 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - running: Set()
22:57:30.083 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 4)
22:57:30.083 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - failed: Set()
22:57:30.083 [dag-scheduler-event-loop] DEBUG org.apache.spark.MapOutputTrackerMaster - Increasing epoch to 2
22:57:30.085 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - submitStage(ResultStage 4 (name=collect at StringIndexer.scala:204;jobs=2))
22:57:30.086 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - missing: List()
22:57:30.087 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 4 (MapPartitionsRDD[25] at collect at StringIndexer.scala:204), which has no missing parents
22:57:30.087 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - submitMissingTasks(ResultStage 4)
22:57:30.130 [dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_8 stored as values in memory (estimated size 20.9 KiB, free 1048.6 MiB)
22:57:30.133 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_8 locally took 19 ms
22:57:30.137 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_8 without replication took 24 ms
22:57:30.196 [dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_8_piece0 stored as bytes in memory (estimated size 10.4 KiB, free 1048.5 MiB)
22:57:30.198 [dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_8_piece0 in memory on saras-air:55957 (size: 10.4 KiB, free: 1048.8 MiB)
22:57:30.199 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_8_piece0
22:57:30.199 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_8_piece0
22:57:30.199 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_8_piece0 locally took 4 ms
22:57:30.199 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_8_piece0 without replication took 4 ms
22:57:30.202 [dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 8 from broadcast at DAGScheduler.scala:1223
22:57:30.206 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[25] at collect at StringIndexer.scala:204) (first 15 tasks are for partitions Vector(0))
22:57:30.206 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 4.0 with 1 tasks
22:57:30.209 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Epoch for TaskSet 4.0: 2
22:57:30.210 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Adding pending tasks took 0 ms
22:57:30.210 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Valid locality levels for TaskSet 4.0: NODE_LOCAL, ANY
22:57:30.214 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_4.0, runningTasks: 0
22:57:30.217 [dispatcher-event-loop-1] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 4.0 (TID 4, saras-air, executor driver, partition 0, NODE_LOCAL, 7325 bytes)
22:57:30.221 [Executor task launch worker for task 4] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 4.0 (TID 4)
22:57:30.230 [Executor task launch worker for task 4] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (4, 0) -> 1
22:57:30.236 [Executor task launch worker for task 4] DEBUG org.apache.spark.storage.BlockManager - Getting local block broadcast_8
22:57:30.236 [Executor task launch worker for task 4] DEBUG org.apache.spark.storage.BlockManager - Level for block broadcast_8 is StorageLevel(disk, memory, deserialized, 1 replicas)
22:57:30.401 [Executor task launch worker for task 4] DEBUG org.apache.spark.MapOutputTrackerMaster - Fetching outputs for shuffle 1, partitions 0-1
22:57:30.431 [Executor task launch worker for task 4] DEBUG org.apache.spark.storage.ShuffleBlockFetcherIterator - maxBytesInFlight: 50331648, targetRemoteRequestSize: 10066329, maxBlocksInFlightPerAddress: 2147483647
22:57:30.445 [Executor task launch worker for task 4] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Getting 1 (717.0 B) non-empty blocks including 1 (717.0 B) local and 0 (0.0 B) host-local and 0 (0.0 B) remote blocks
22:57:30.446 [Executor task launch worker for task 4] INFO org.apache.spark.storage.ShuffleBlockFetcherIterator - Started 0 remote fetches in 18 ms
22:57:30.447 [Executor task launch worker for task 4] DEBUG org.apache.spark.storage.ShuffleBlockFetcherIterator - Start fetching local blocks: (shuffle_1_3_0,0)
22:57:30.455 [Executor task launch worker for task 4] DEBUG org.apache.spark.storage.ShuffleBlockFetcherIterator - Got local blocks in 28 ms
22:57:30.495 [Executor task launch worker for task 4] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection - code for noop$():
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificMutableProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificMutableProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseMutableProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificMutableProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = new org.apache.spark.sql.catalyst.expressions.GenericInternalRow(1);
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public org.apache.spark.sql.catalyst.expressions.codegen.BaseMutableProjection target(InternalRow row) {
/* 022 */     mutableRow = row;
/* 023 */     return this;
/* 024 */   }
/* 025 */
/* 026 */   /* Provide immutable access to the last projected row. */
/* 027 */   public InternalRow currentValue() {
/* 028 */     return (InternalRow) mutableRow;
/* 029 */   }
/* 030 */
/* 031 */   public java.lang.Object apply(java.lang.Object _i) {
/* 032 */     InternalRow i = (InternalRow) _i;
/* 033 */
/* 034 */
/* 035 */     // copy all the results into MutableRow
/* 036 */
/* 037 */     return mutableRow;
/* 038 */   }
/* 039 */
/* 040 */
/* 041 */ }

22:57:30.504 [Executor task launch worker for task 4] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection - code for noop$():
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificMutableProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificMutableProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseMutableProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificMutableProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = new org.apache.spark.sql.catalyst.expressions.GenericInternalRow(1);
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public org.apache.spark.sql.catalyst.expressions.codegen.BaseMutableProjection target(InternalRow row) {
/* 022 */     mutableRow = row;
/* 023 */     return this;
/* 024 */   }
/* 025 */
/* 026 */   /* Provide immutable access to the last projected row. */
/* 027 */   public InternalRow currentValue() {
/* 028 */     return (InternalRow) mutableRow;
/* 029 */   }
/* 030 */
/* 031 */   public java.lang.Object apply(java.lang.Object _i) {
/* 032 */     InternalRow i = (InternalRow) _i;
/* 033 */
/* 034 */
/* 035 */     // copy all the results into MutableRow
/* 036 */
/* 037 */     return mutableRow;
/* 038 */   }
/* 039 */
/* 040 */
/* 041 */ }

22:57:30.516 [Executor task launch worker for task 4] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection - code for :
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(0, 0);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */     return (mutableStateArray_0[0].getRow());
/* 028 */   }
/* 029 */
/* 030 */
/* 031 */ }

22:57:30.519 [Executor task launch worker for task 4] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection - code for noop$():
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificMutableProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificMutableProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseMutableProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificMutableProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = new org.apache.spark.sql.catalyst.expressions.GenericInternalRow(1);
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public org.apache.spark.sql.catalyst.expressions.codegen.BaseMutableProjection target(InternalRow row) {
/* 022 */     mutableRow = row;
/* 023 */     return this;
/* 024 */   }
/* 025 */
/* 026 */   /* Provide immutable access to the last projected row. */
/* 027 */   public InternalRow currentValue() {
/* 028 */     return (InternalRow) mutableRow;
/* 029 */   }
/* 030 */
/* 031 */   public java.lang.Object apply(java.lang.Object _i) {
/* 032 */     InternalRow i = (InternalRow) _i;
/* 033 */
/* 034 */
/* 035 */     // copy all the results into MutableRow
/* 036 */
/* 037 */     return mutableRow;
/* 038 */   }
/* 039 */
/* 040 */
/* 041 */ }

22:57:30.529 [Executor task launch worker for task 4] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection - code for input[0, binary, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */
/* 031 */     boolean isNull_0 = i.isNullAt(0);
/* 032 */     byte[] value_0 = isNull_0 ?
/* 033 */     null : (i.getBinary(0));
/* 034 */     if (isNull_0) {
/* 035 */       mutableStateArray_0[0].setNullAt(0);
/* 036 */     } else {
/* 037 */       mutableStateArray_0[0].write(0, value_0);
/* 038 */     }
/* 039 */     return (mutableStateArray_0[0].getRow());
/* 040 */   }
/* 041 */
/* 042 */
/* 043 */ }

22:57:30.532 [Executor task launch worker for task 4] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection - code for noop$():
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificMutableProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificMutableProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseMutableProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificMutableProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = new org.apache.spark.sql.catalyst.expressions.GenericInternalRow(1);
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public org.apache.spark.sql.catalyst.expressions.codegen.BaseMutableProjection target(InternalRow row) {
/* 022 */     mutableRow = row;
/* 023 */     return this;
/* 024 */   }
/* 025 */
/* 026 */   /* Provide immutable access to the last projected row. */
/* 027 */   public InternalRow currentValue() {
/* 028 */     return (InternalRow) mutableRow;
/* 029 */   }
/* 030 */
/* 031 */   public java.lang.Object apply(java.lang.Object _i) {
/* 032 */     InternalRow i = (InternalRow) _i;
/* 033 */
/* 034 */
/* 035 */     // copy all the results into MutableRow
/* 036 */
/* 037 */     return mutableRow;
/* 038 */   }
/* 039 */
/* 040 */
/* 041 */ }

22:57:30.542 [Executor task launch worker for task 4] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection - code for decodeusingserializer(input[0, binary, true], Array[org.apache.spark.util.collection.OpenHashMap], true):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */   private org.apache.spark.serializer.KryoSerializerInstance kryoSerializer;
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */     kryoSerializer = (org.apache.spark.serializer.KryoSerializerInstance) org.apache.spark.sql.catalyst.expressions.objects.SerializerSupport$.MODULE$.newSerializer(true);
/* 016 */
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   public void initialize(int partitionIndex) {
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   public java.lang.Object apply(java.lang.Object _i) {
/* 025 */     InternalRow i = (InternalRow) _i;
/* 026 */     boolean isNull_1 = i.isNullAt(0);
/* 027 */     byte[] value_1 = isNull_1 ?
/* 028 */     null : (i.getBinary(0));
/* 029 */     final org.apache.spark.util.collection.OpenHashMap[] value_0 =
/* 030 */     isNull_1 ? null : (org.apache.spark.util.collection.OpenHashMap[]) kryoSerializer.deserialize(java.nio.ByteBuffer.wrap(value_1), null);
/* 031 */     if (isNull_1) {
/* 032 */       mutableRow.setNullAt(0);
/* 033 */     } else {
/* 034 */
/* 035 */       mutableRow.update(0, value_0);
/* 036 */     }
/* 037 */
/* 038 */     return mutableRow;
/* 039 */   }
/* 040 */
/* 041 */
/* 042 */ }

22:57:30.658 [Executor task launch worker for task 4] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection - code for encodeusingserializer(input[0, java.lang.Object, true], true):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.serializer.KryoSerializerInstance kryoSerializer;
/* 009 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 010 */
/* 011 */   public SpecificUnsafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */
/* 014 */     kryoSerializer = (org.apache.spark.serializer.KryoSerializerInstance) org.apache.spark.sql.catalyst.expressions.objects.SerializerSupport$.MODULE$.newSerializer(true);
/* 015 */
/* 016 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(1, 32);
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   public void initialize(int partitionIndex) {
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   // Scala.Function1 need this
/* 025 */   public java.lang.Object apply(java.lang.Object row) {
/* 026 */     return apply((InternalRow) row);
/* 027 */   }
/* 028 */
/* 029 */   public UnsafeRow apply(InternalRow i) {
/* 030 */     mutableStateArray_0[0].reset();
/* 031 */
/* 032 */
/* 033 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 034 */
/* 035 */     boolean isNull_1 = i.isNullAt(0);
/* 036 */     java.lang.Object value_1 = isNull_1 ?
/* 037 */     null : ((java.lang.Object)i.get(0, null));
/* 038 */     final byte[] value_0 =
/* 039 */     isNull_1 ? null : kryoSerializer.serialize(value_1, null).array();
/* 040 */     if (isNull_1) {
/* 041 */       mutableStateArray_0[0].setNullAt(0);
/* 042 */     } else {
/* 043 */       mutableStateArray_0[0].write(0, value_0);
/* 044 */     }
/* 045 */     return (mutableStateArray_0[0].getRow());
/* 046 */   }
/* 047 */
/* 048 */
/* 049 */ }

22:57:30.703 [Executor task launch worker for task 4] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 4.0 (TID 4). 3789 bytes result sent to driver
22:57:30.705 [Executor task launch worker for task 4] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - removing (4, 0) from stageTCMP
22:57:30.720 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_4.0, runningTasks: 0
22:57:30.721 [dispatcher-event-loop-1] DEBUG org.apache.spark.scheduler.TaskSetManager - No tasks for locality level NODE_LOCAL, so moving to locality level ANY
22:57:30.727 [task-result-getter-0] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 4.0 (TID 4) in 512 ms on saras-air (executor driver) (1/1)
22:57:30.728 [task-result-getter-0] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 4.0, whose tasks have all completed, from pool 
22:57:30.738 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 4 (collect at StringIndexer.scala:204) finished in 0.635 s
22:57:30.740 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - After removal of stage 4, remaining stages = 1
22:57:30.740 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - After removal of stage 3, remaining stages = 0
22:57:30.740 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 2 is finished. Cancelling potential speculative or zombie tasks for this job
22:57:30.740 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 4: Stage finished
22:57:30.743 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.scheduler.DAGScheduler - Job 2 finished: collect at StringIndexer.scala:204, took 1.420969 s
22:57:30.945 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection - code for decodeusingserializer(input[0, binary, true], Array[org.apache.spark.util.collection.OpenHashMap], true):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */   private org.apache.spark.serializer.KryoSerializerInstance kryoSerializer;
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */     kryoSerializer = (org.apache.spark.serializer.KryoSerializerInstance) org.apache.spark.sql.catalyst.expressions.objects.SerializerSupport$.MODULE$.newSerializer(true);
/* 016 */
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   public void initialize(int partitionIndex) {
/* 021 */
/* 022 */   }
/* 023 */
/* 024 */   public java.lang.Object apply(java.lang.Object _i) {
/* 025 */     InternalRow i = (InternalRow) _i;
/* 026 */     boolean isNull_1 = i.isNullAt(0);
/* 027 */     byte[] value_1 = isNull_1 ?
/* 028 */     null : (i.getBinary(0));
/* 029 */     final org.apache.spark.util.collection.OpenHashMap[] value_0 =
/* 030 */     isNull_1 ? null : (org.apache.spark.util.collection.OpenHashMap[]) kryoSerializer.deserialize(java.nio.ByteBuffer.wrap(value_1), null);
/* 031 */     if (isNull_1) {
/* 032 */       mutableRow.setNullAt(0);
/* 033 */     } else {
/* 034 */
/* 035 */       mutableRow.update(0, value_0);
/* 036 */     }
/* 037 */
/* 038 */     return mutableRow;
/* 039 */   }
/* 040 */
/* 041 */
/* 042 */ }

22:57:31.037 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.ml.feature.StringIndexerModel - Input schema: {"type":"struct","fields":[{"name":"Title","type":"string","nullable":true,"metadata":{}},{"name":"Company","type":"string","nullable":true,"metadata":{}},{"name":"Location","type":"string","nullable":true,"metadata":{}},{"name":"Type","type":"string","nullable":true,"metadata":{}},{"name":"Level","type":"string","nullable":true,"metadata":{}},{"name":"YearsExp","type":"string","nullable":true,"metadata":{}},{"name":"Country","type":"string","nullable":true,"metadata":{}},{"name":"Skills","type":"string","nullable":true,"metadata":{}},{"name":"type-factorized","type":"double","nullable":false,"metadata":{"ml_attr":{"vals":["Full Time","Internship","Part Time","Freelance / Project","Work From Home","Shift Based","__unknown"],"type":"nominal","name":"type-factorized"}}}]}
22:57:31.047 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.ml.feature.StringIndexerModel - Expected output schema: {"type":"struct","fields":[{"name":"Title","type":"string","nullable":true,"metadata":{}},{"name":"Company","type":"string","nullable":true,"metadata":{}},{"name":"Location","type":"string","nullable":true,"metadata":{}},{"name":"Type","type":"string","nullable":true,"metadata":{}},{"name":"Level","type":"string","nullable":true,"metadata":{}},{"name":"YearsExp","type":"string","nullable":true,"metadata":{}},{"name":"Country","type":"string","nullable":true,"metadata":{}},{"name":"Skills","type":"string","nullable":true,"metadata":{}},{"name":"type-factorized","type":"double","nullable":false,"metadata":{"ml_attr":{"vals":["Full Time","Internship","Part Time","Freelance / Project","Work From Home","Shift Based","__unknown"],"type":"nominal","name":"type-factorized"}}},{"name":"level-factorized","type":"double","nullable":false,"metadata":{"ml_attr":{"type":"nominal"}}}]}
+++++========++++++++Done
root
 |-- Title: string (nullable = true)
 |-- Company: string (nullable = true)
 |-- Location: string (nullable = true)
 |-- Type: string (nullable = true)
 |-- Level: string (nullable = true)
 |-- YearsExp: string (nullable = true)
 |-- Country: string (nullable = true)
 |-- Skills: string (nullable = true)
 |-- type-factorized: double (nullable = false)
 |-- level-factorized: double (nullable = false)

+++++========++++++++Done
22:57:31.311 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences - Resolving 'YearsExp to YearsExp#37
root
 |-- Title: string (nullable = true)
 |-- Company: string (nullable = true)
 |-- Location: string (nullable = true)
 |-- YearsExp: string (nullable = true)
 |-- Country: string (nullable = true)
 |-- Skills: string (nullable = true)
 |-- type-factorized: double (nullable = false)
 |-- level-factorized: double (nullable = false)
 |-- MinMaxYearsExp: string (nullable = true)

+++++========++++++++Done
22:57:31.629 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pruning directories with: 
22:57:31.630 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: 
22:57:31.631 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: 
22:57:31.637 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.sql.execution.datasources.FileSourceStrategy - Output Data Schema: struct<Title: string, Company: string, Location: string, Type: string, Level: string ... 6 more fields>
22:57:31.760 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.sql.execution.WholeStageCodegenExec - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator inputadapter_input_0;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] project_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[3];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     inputadapter_input_0 = inputs[0];
/* 020 */     project_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(8, 224);
/* 021 */     project_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(8, 192);
/* 022 */     project_mutableStateArray_0[2] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(9, 288);
/* 023 */
/* 024 */   }
/* 025 */
/* 026 */   protected void processNext() throws java.io.IOException {
/* 027 */     while ( inputadapter_input_0.hasNext()) {
/* 028 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();
/* 029 */
/* 030 */       boolean inputadapter_isNull_3 = inputadapter_row_0.isNullAt(3);
/* 031 */       UTF8String inputadapter_value_3 = inputadapter_isNull_3 ?
/* 032 */       null : (inputadapter_row_0.getUTF8String(3));
/* 033 */       Object project_arg_0 = inputadapter_isNull_3 ? null : ((scala.Function1[]) references[0] /* converters */)[0].apply(inputadapter_value_3);
/* 034 */
/* 035 */       Double project_result_0 = null;
/* 036 */       try {
/* 037 */         project_result_0 = (Double)((scala.Function1) references[2] /* udf */).apply(project_arg_0);
/* 038 */       } catch (Exception e) {
/* 039 */         throw new org.apache.spark.SparkException(((java.lang.String) references[1] /* errMsg */), e);
/* 040 */       }
/* 041 */
/* 042 */       boolean project_isNull_7 = project_result_0 == null;
/* 043 */       double project_value_7 = -1.0;
/* 044 */       if (!project_isNull_7) {
/* 045 */         project_value_7 = project_result_0;
/* 046 */       }
/* 047 */
/* 048 */       boolean inputadapter_isNull_4 = inputadapter_row_0.isNullAt(4);
/* 049 */       UTF8String inputadapter_value_4 = inputadapter_isNull_4 ?
/* 050 */       null : (inputadapter_row_0.getUTF8String(4));
/* 051 */       Object project_arg_1 = inputadapter_isNull_4 ? null : ((scala.Function1[]) references[3] /* converters */)[0].apply(inputadapter_value_4);
/* 052 */
/* 053 */       Double project_result_1 = null;
/* 054 */       try {
/* 055 */         project_result_1 = (Double)((scala.Function1) references[5] /* udf */).apply(project_arg_1);
/* 056 */       } catch (Exception e) {
/* 057 */         throw new org.apache.spark.SparkException(((java.lang.String) references[4] /* errMsg */), e);
/* 058 */       }
/* 059 */
/* 060 */       boolean project_isNull_24 = project_result_1 == null;
/* 061 */       double project_value_24 = -1.0;
/* 062 */       if (!project_isNull_24) {
/* 063 */         project_value_24 = project_result_1;
/* 064 */       }
/* 065 */
/* 066 */       boolean inputadapter_isNull_5 = inputadapter_row_0.isNullAt(5);
/* 067 */       UTF8String inputadapter_value_5 = inputadapter_isNull_5 ?
/* 068 */       null : (inputadapter_row_0.getUTF8String(5));
/* 069 */
/* 070 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);
/* 071 */       UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?
/* 072 */       null : (inputadapter_row_0.getUTF8String(0));
/* 073 */       boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);
/* 074 */       UTF8String inputadapter_value_1 = inputadapter_isNull_1 ?
/* 075 */       null : (inputadapter_row_0.getUTF8String(1));
/* 076 */       boolean inputadapter_isNull_2 = inputadapter_row_0.isNullAt(2);
/* 077 */       UTF8String inputadapter_value_2 = inputadapter_isNull_2 ?
/* 078 */       null : (inputadapter_row_0.getUTF8String(2));
/* 079 */       boolean inputadapter_isNull_6 = inputadapter_row_0.isNullAt(6);
/* 080 */       UTF8String inputadapter_value_6 = inputadapter_isNull_6 ?
/* 081 */       null : (inputadapter_row_0.getUTF8String(6));
/* 082 */       boolean inputadapter_isNull_7 = inputadapter_row_0.isNullAt(7);
/* 083 */       UTF8String inputadapter_value_7 = inputadapter_isNull_7 ?
/* 084 */       null : (inputadapter_row_0.getUTF8String(7));
/* 085 */       boolean project_isNull_40 = project_isNull_7;
/* 086 */       UTF8String project_value_40 = null;
/* 087 */       if (!project_isNull_7) {
/* 088 */         project_value_40 = UTF8String.fromString(String.valueOf(project_value_7));
/* 089 */       }
/* 090 */       boolean project_isNull_42 = project_isNull_24;
/* 091 */       UTF8String project_value_42 = null;
/* 092 */       if (!project_isNull_24) {
/* 093 */         project_value_42 = UTF8String.fromString(String.valueOf(project_value_24));
/* 094 */       }
/* 095 */       Object project_arg_2 = inputadapter_isNull_5 ? null : ((scala.Function1[]) references[6] /* converters */)[0].apply(inputadapter_value_5);
/* 096 */
/* 097 */       UTF8String project_result_2 = null;
/* 098 */       try {
/* 099 */         project_result_2 = (UTF8String)((scala.Function1[]) references[6] /* converters */)[1].apply(((scala.Function1) references[8] /* udf */).apply(project_arg_2));
/* 100 */       } catch (Exception e) {
/* 101 */         throw new org.apache.spark.SparkException(((java.lang.String) references[7] /* errMsg */), e);
/* 102 */       }
/* 103 */
/* 104 */       boolean project_isNull_44 = project_result_2 == null;
/* 105 */       UTF8String project_value_44 = null;
/* 106 */       if (!project_isNull_44) {
/* 107 */         project_value_44 = project_result_2;
/* 108 */       }
/* 109 */       project_mutableStateArray_0[2].reset();
/* 110 */
/* 111 */       project_mutableStateArray_0[2].zeroOutNullBytes();
/* 112 */
/* 113 */       if (inputadapter_isNull_0) {
/* 114 */         project_mutableStateArray_0[2].setNullAt(0);
/* 115 */       } else {
/* 116 */         project_mutableStateArray_0[2].write(0, inputadapter_value_0);
/* 117 */       }
/* 118 */
/* 119 */       if (inputadapter_isNull_1) {
/* 120 */         project_mutableStateArray_0[2].setNullAt(1);
/* 121 */       } else {
/* 122 */         project_mutableStateArray_0[2].write(1, inputadapter_value_1);
/* 123 */       }
/* 124 */
/* 125 */       if (inputadapter_isNull_2) {
/* 126 */         project_mutableStateArray_0[2].setNullAt(2);
/* 127 */       } else {
/* 128 */         project_mutableStateArray_0[2].write(2, inputadapter_value_2);
/* 129 */       }
/* 130 */
/* 131 */       if (inputadapter_isNull_5) {
/* 132 */         project_mutableStateArray_0[2].setNullAt(3);
/* 133 */       } else {
/* 134 */         project_mutableStateArray_0[2].write(3, inputadapter_value_5);
/* 135 */       }
/* 136 */
/* 137 */       if (inputadapter_isNull_6) {
/* 138 */         project_mutableStateArray_0[2].setNullAt(4);
/* 139 */       } else {
/* 140 */         project_mutableStateArray_0[2].write(4, inputadapter_value_6);
/* 141 */       }
/* 142 */
/* 143 */       if (inputadapter_isNull_7) {
/* 144 */         project_mutableStateArray_0[2].setNullAt(5);
/* 145 */       } else {
/* 146 */         project_mutableStateArray_0[2].write(5, inputadapter_value_7);
/* 147 */       }
/* 148 */
/* 149 */       project_mutableStateArray_0[2].write(6, project_value_40);
/* 150 */
/* 151 */       project_mutableStateArray_0[2].write(7, project_value_42);
/* 152 */
/* 153 */       if (project_isNull_44) {
/* 154 */         project_mutableStateArray_0[2].setNullAt(8);
/* 155 */       } else {
/* 156 */         project_mutableStateArray_0[2].write(8, project_value_44);
/* 157 */       }
/* 158 */       append((project_mutableStateArray_0[2].getRow()));
/* 159 */       if (shouldStop()) return;
/* 160 */     }
/* 161 */   }
/* 162 */
/* 163 */ }

22:57:31.767 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - 
/* 001 */ public Object generate(Object[] references) {
/* 002 */   return new GeneratedIteratorForCodegenStage1(references);
/* 003 */ }
/* 004 */
/* 005 */ // codegenStageId=1
/* 006 */ final class GeneratedIteratorForCodegenStage1 extends org.apache.spark.sql.execution.BufferedRowIterator {
/* 007 */   private Object[] references;
/* 008 */   private scala.collection.Iterator[] inputs;
/* 009 */   private scala.collection.Iterator inputadapter_input_0;
/* 010 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] project_mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[3];
/* 011 */
/* 012 */   public GeneratedIteratorForCodegenStage1(Object[] references) {
/* 013 */     this.references = references;
/* 014 */   }
/* 015 */
/* 016 */   public void init(int index, scala.collection.Iterator[] inputs) {
/* 017 */     partitionIndex = index;
/* 018 */     this.inputs = inputs;
/* 019 */     inputadapter_input_0 = inputs[0];
/* 020 */     project_mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(8, 224);
/* 021 */     project_mutableStateArray_0[1] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(8, 192);
/* 022 */     project_mutableStateArray_0[2] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(9, 288);
/* 023 */
/* 024 */   }
/* 025 */
/* 026 */   protected void processNext() throws java.io.IOException {
/* 027 */     while ( inputadapter_input_0.hasNext()) {
/* 028 */       InternalRow inputadapter_row_0 = (InternalRow) inputadapter_input_0.next();
/* 029 */
/* 030 */       boolean inputadapter_isNull_3 = inputadapter_row_0.isNullAt(3);
/* 031 */       UTF8String inputadapter_value_3 = inputadapter_isNull_3 ?
/* 032 */       null : (inputadapter_row_0.getUTF8String(3));
/* 033 */       Object project_arg_0 = inputadapter_isNull_3 ? null : ((scala.Function1[]) references[0] /* converters */)[0].apply(inputadapter_value_3);
/* 034 */
/* 035 */       Double project_result_0 = null;
/* 036 */       try {
/* 037 */         project_result_0 = (Double)((scala.Function1) references[2] /* udf */).apply(project_arg_0);
/* 038 */       } catch (Exception e) {
/* 039 */         throw new org.apache.spark.SparkException(((java.lang.String) references[1] /* errMsg */), e);
/* 040 */       }
/* 041 */
/* 042 */       boolean project_isNull_7 = project_result_0 == null;
/* 043 */       double project_value_7 = -1.0;
/* 044 */       if (!project_isNull_7) {
/* 045 */         project_value_7 = project_result_0;
/* 046 */       }
/* 047 */
/* 048 */       boolean inputadapter_isNull_4 = inputadapter_row_0.isNullAt(4);
/* 049 */       UTF8String inputadapter_value_4 = inputadapter_isNull_4 ?
/* 050 */       null : (inputadapter_row_0.getUTF8String(4));
/* 051 */       Object project_arg_1 = inputadapter_isNull_4 ? null : ((scala.Function1[]) references[3] /* converters */)[0].apply(inputadapter_value_4);
/* 052 */
/* 053 */       Double project_result_1 = null;
/* 054 */       try {
/* 055 */         project_result_1 = (Double)((scala.Function1) references[5] /* udf */).apply(project_arg_1);
/* 056 */       } catch (Exception e) {
/* 057 */         throw new org.apache.spark.SparkException(((java.lang.String) references[4] /* errMsg */), e);
/* 058 */       }
/* 059 */
/* 060 */       boolean project_isNull_24 = project_result_1 == null;
/* 061 */       double project_value_24 = -1.0;
/* 062 */       if (!project_isNull_24) {
/* 063 */         project_value_24 = project_result_1;
/* 064 */       }
/* 065 */
/* 066 */       boolean inputadapter_isNull_5 = inputadapter_row_0.isNullAt(5);
/* 067 */       UTF8String inputadapter_value_5 = inputadapter_isNull_5 ?
/* 068 */       null : (inputadapter_row_0.getUTF8String(5));
/* 069 */
/* 070 */       boolean inputadapter_isNull_0 = inputadapter_row_0.isNullAt(0);
/* 071 */       UTF8String inputadapter_value_0 = inputadapter_isNull_0 ?
/* 072 */       null : (inputadapter_row_0.getUTF8String(0));
/* 073 */       boolean inputadapter_isNull_1 = inputadapter_row_0.isNullAt(1);
/* 074 */       UTF8String inputadapter_value_1 = inputadapter_isNull_1 ?
/* 075 */       null : (inputadapter_row_0.getUTF8String(1));
/* 076 */       boolean inputadapter_isNull_2 = inputadapter_row_0.isNullAt(2);
/* 077 */       UTF8String inputadapter_value_2 = inputadapter_isNull_2 ?
/* 078 */       null : (inputadapter_row_0.getUTF8String(2));
/* 079 */       boolean inputadapter_isNull_6 = inputadapter_row_0.isNullAt(6);
/* 080 */       UTF8String inputadapter_value_6 = inputadapter_isNull_6 ?
/* 081 */       null : (inputadapter_row_0.getUTF8String(6));
/* 082 */       boolean inputadapter_isNull_7 = inputadapter_row_0.isNullAt(7);
/* 083 */       UTF8String inputadapter_value_7 = inputadapter_isNull_7 ?
/* 084 */       null : (inputadapter_row_0.getUTF8String(7));
/* 085 */       boolean project_isNull_40 = project_isNull_7;
/* 086 */       UTF8String project_value_40 = null;
/* 087 */       if (!project_isNull_7) {
/* 088 */         project_value_40 = UTF8String.fromString(String.valueOf(project_value_7));
/* 089 */       }
/* 090 */       boolean project_isNull_42 = project_isNull_24;
/* 091 */       UTF8String project_value_42 = null;
/* 092 */       if (!project_isNull_24) {
/* 093 */         project_value_42 = UTF8String.fromString(String.valueOf(project_value_24));
/* 094 */       }
/* 095 */       Object project_arg_2 = inputadapter_isNull_5 ? null : ((scala.Function1[]) references[6] /* converters */)[0].apply(inputadapter_value_5);
/* 096 */
/* 097 */       UTF8String project_result_2 = null;
/* 098 */       try {
/* 099 */         project_result_2 = (UTF8String)((scala.Function1[]) references[6] /* converters */)[1].apply(((scala.Function1) references[8] /* udf */).apply(project_arg_2));
/* 100 */       } catch (Exception e) {
/* 101 */         throw new org.apache.spark.SparkException(((java.lang.String) references[7] /* errMsg */), e);
/* 102 */       }
/* 103 */
/* 104 */       boolean project_isNull_44 = project_result_2 == null;
/* 105 */       UTF8String project_value_44 = null;
/* 106 */       if (!project_isNull_44) {
/* 107 */         project_value_44 = project_result_2;
/* 108 */       }
/* 109 */       project_mutableStateArray_0[2].reset();
/* 110 */
/* 111 */       project_mutableStateArray_0[2].zeroOutNullBytes();
/* 112 */
/* 113 */       if (inputadapter_isNull_0) {
/* 114 */         project_mutableStateArray_0[2].setNullAt(0);
/* 115 */       } else {
/* 116 */         project_mutableStateArray_0[2].write(0, inputadapter_value_0);
/* 117 */       }
/* 118 */
/* 119 */       if (inputadapter_isNull_1) {
/* 120 */         project_mutableStateArray_0[2].setNullAt(1);
/* 121 */       } else {
/* 122 */         project_mutableStateArray_0[2].write(1, inputadapter_value_1);
/* 123 */       }
/* 124 */
/* 125 */       if (inputadapter_isNull_2) {
/* 126 */         project_mutableStateArray_0[2].setNullAt(2);
/* 127 */       } else {
/* 128 */         project_mutableStateArray_0[2].write(2, inputadapter_value_2);
/* 129 */       }
/* 130 */
/* 131 */       if (inputadapter_isNull_5) {
/* 132 */         project_mutableStateArray_0[2].setNullAt(3);
/* 133 */       } else {
/* 134 */         project_mutableStateArray_0[2].write(3, inputadapter_value_5);
/* 135 */       }
/* 136 */
/* 137 */       if (inputadapter_isNull_6) {
/* 138 */         project_mutableStateArray_0[2].setNullAt(4);
/* 139 */       } else {
/* 140 */         project_mutableStateArray_0[2].write(4, inputadapter_value_6);
/* 141 */       }
/* 142 */
/* 143 */       if (inputadapter_isNull_7) {
/* 144 */         project_mutableStateArray_0[2].setNullAt(5);
/* 145 */       } else {
/* 146 */         project_mutableStateArray_0[2].write(5, inputadapter_value_7);
/* 147 */       }
/* 148 */
/* 149 */       project_mutableStateArray_0[2].write(6, project_value_40);
/* 150 */
/* 151 */       project_mutableStateArray_0[2].write(7, project_value_42);
/* 152 */
/* 153 */       if (project_isNull_44) {
/* 154 */         project_mutableStateArray_0[2].setNullAt(8);
/* 155 */       } else {
/* 156 */         project_mutableStateArray_0[2].write(8, project_value_44);
/* 157 */       }
/* 158 */       append((project_mutableStateArray_0[2].getRow()));
/* 159 */       if (shouldStop()) return;
/* 160 */     }
/* 161 */   }
/* 162 */
/* 163 */ }

22:57:31.881 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 120.6875 ms
22:57:31.891 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_9 stored as values in memory (estimated size 170.5 KiB, free 1048.4 MiB)
22:57:31.893 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_9 locally took 8 ms
22:57:31.893 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_9 without replication took 8 ms
22:57:31.942 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(143)
22:57:31.943 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 143
22:57:31.943 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 143
22:57:31.943 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(155)
22:57:31.943 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 155
22:57:31.943 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 155
22:57:31.943 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(153)
22:57:31.943 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 153
22:57:31.943 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 153
22:57:31.943 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(150)
22:57:31.943 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 150
22:57:31.943 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 150
22:57:31.944 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(205)
22:57:31.944 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 205
22:57:31.944 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 205
22:57:31.944 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(198)
22:57:31.944 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 198
22:57:31.944 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 198
22:57:31.944 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(141)
22:57:31.944 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 141
22:57:31.944 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 141
22:57:31.944 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanBroadcast(7)
22:57:31.944 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning broadcast 7
22:57:31.944 [Spark Context Cleaner] DEBUG org.apache.spark.broadcast.TorrentBroadcast - Unpersisting TorrentBroadcast 7
22:57:31.950 [block-manager-slave-async-thread-pool-21] DEBUG org.apache.spark.storage.BlockManagerSlaveEndpoint - removing broadcast 7
22:57:31.951 [block-manager-slave-async-thread-pool-21] DEBUG org.apache.spark.storage.BlockManager - Removing broadcast 7
22:57:31.965 [block-manager-slave-async-thread-pool-21] DEBUG org.apache.spark.storage.BlockManager - Removing block broadcast_7
22:57:31.967 [block-manager-slave-async-thread-pool-21] DEBUG org.apache.spark.storage.memory.MemoryStore - Block broadcast_7 of size 21376 dropped from memory (free 1099306095)
22:57:31.968 [block-manager-slave-async-thread-pool-21] DEBUG org.apache.spark.storage.BlockManager - Removing block broadcast_7_piece0
22:57:31.969 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_9_piece0 stored as bytes in memory (estimated size 23.8 KiB, free 1048.4 MiB)
22:57:31.970 [block-manager-slave-async-thread-pool-21] DEBUG org.apache.spark.storage.memory.MemoryStore - Block broadcast_7_piece0 of size 10451 dropped from memory (free 1099316546)
22:57:31.973 [dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_9_piece0 in memory on saras-air:55957 (size: 23.8 KiB, free: 1048.7 MiB)
22:57:31.974 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_9_piece0
22:57:31.974 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_9_piece0
22:57:31.974 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_9_piece0 locally took 9 ms
22:57:31.975 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_9_piece0 without replication took 9 ms
22:57:31.977 [dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_7_piece0 on saras-air:55957 in memory (size: 10.2 KiB, free: 1048.7 MiB)
22:57:31.978 [block-manager-slave-async-thread-pool-21] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_7_piece0
22:57:31.978 [block-manager-slave-async-thread-pool-21] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_7_piece0
22:57:31.978 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.SparkContext - Created broadcast 9 from show at WuzzufJobsAnalysisApplication.java:109
22:57:31.981 [block-manager-slave-async-thread-pool-23] DEBUG org.apache.spark.storage.BlockManagerSlaveEndpoint - Done removing broadcast 7, response is 0
22:57:31.985 [block-manager-slave-async-thread-pool-23] DEBUG org.apache.spark.storage.BlockManagerSlaveEndpoint - Sent response: 0 to saras-air:55956
22:57:31.987 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned broadcast 7
22:57:31.988 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(177)
22:57:31.988 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 177
22:57:31.988 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 177
22:57:31.988 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(142)
22:57:31.988 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 142
22:57:31.988 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 142
22:57:31.988 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(146)
22:57:31.988 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 146
22:57:31.988 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 146
22:57:31.988 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(173)
22:57:31.988 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 173
22:57:31.988 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 173
22:57:31.988 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(140)
22:57:31.988 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 140
22:57:31.988 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 140
22:57:31.988 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(145)
22:57:31.988 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 145
22:57:31.988 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 145
22:57:31.988 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(170)
22:57:31.988 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 170
22:57:31.988 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 170
22:57:31.988 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(199)
22:57:31.988 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 199
22:57:31.988 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 199
22:57:31.988 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(168)
22:57:31.988 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 5114295 bytes, open cost is considered as scanning 4194304 bytes.
22:57:31.988 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 168
22:57:31.988 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 168
22:57:31.988 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(175)
22:57:31.988 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 175
22:57:31.988 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 175
22:57:31.988 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(181)
22:57:31.988 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 181
22:57:31.988 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 181
22:57:31.988 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(207)
22:57:31.988 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 207
22:57:31.988 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 207
22:57:31.988 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(157)
22:57:31.988 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 157
22:57:31.989 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 157
22:57:31.989 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(204)
22:57:31.989 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 204
22:57:31.989 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 204
22:57:31.989 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(178)
22:57:31.989 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 178
22:57:31.989 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 178
22:57:31.989 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(210)
22:57:31.989 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 210
22:57:31.989 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 210
22:57:31.989 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(147)
22:57:31.989 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 147
22:57:31.989 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 147
22:57:31.989 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(206)
22:57:31.989 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 206
22:57:31.989 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 206
22:57:31.989 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(162)
22:57:31.989 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 162
22:57:31.989 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 162
22:57:31.989 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(193)
22:57:31.989 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 193
22:57:31.989 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 193
22:57:31.989 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(203)
22:57:31.989 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 203
22:57:31.989 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 203
22:57:31.989 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(189)
22:57:31.989 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 189
22:57:31.989 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 189
22:57:31.989 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(167)
22:57:31.989 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 167
22:57:31.989 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 167
22:57:31.989 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(197)
22:57:31.989 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 197
22:57:31.989 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 197
22:57:31.989 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(194)
22:57:31.989 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 194
22:57:31.989 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 194
22:57:31.989 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(180)
22:57:31.989 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 180
22:57:31.989 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 180
22:57:31.989 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(182)
22:57:31.989 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 182
22:57:31.989 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 182
22:57:31.989 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(144)
22:57:31.989 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 144
22:57:31.989 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 144
22:57:31.989 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanBroadcast(6)
22:57:31.989 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning broadcast 6
22:57:31.989 [Spark Context Cleaner] DEBUG org.apache.spark.broadcast.TorrentBroadcast - Unpersisting TorrentBroadcast 6
22:57:31.991 [block-manager-slave-async-thread-pool-24] DEBUG org.apache.spark.storage.BlockManagerSlaveEndpoint - removing broadcast 6
22:57:31.991 [block-manager-slave-async-thread-pool-24] DEBUG org.apache.spark.storage.BlockManager - Removing broadcast 6
22:57:31.991 [block-manager-slave-async-thread-pool-24] DEBUG org.apache.spark.storage.BlockManager - Removing block broadcast_6_piece0
22:57:31.994 [block-manager-slave-async-thread-pool-24] DEBUG org.apache.spark.storage.memory.MemoryStore - Block broadcast_6_piece0 of size 24400 dropped from memory (free 1099340946)
22:57:31.995 [dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_6_piece0 on saras-air:55957 in memory (size: 23.8 KiB, free: 1048.8 MiB)
22:57:31.996 [block-manager-slave-async-thread-pool-24] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_6_piece0
22:57:31.996 [block-manager-slave-async-thread-pool-24] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_6_piece0
22:57:31.997 [block-manager-slave-async-thread-pool-24] DEBUG org.apache.spark.storage.BlockManager - Removing block broadcast_6
22:57:31.998 [block-manager-slave-async-thread-pool-24] DEBUG org.apache.spark.storage.memory.MemoryStore - Block broadcast_6 of size 174584 dropped from memory (free 1099515530)
22:57:31.999 [block-manager-slave-async-thread-pool-26] DEBUG org.apache.spark.storage.BlockManagerSlaveEndpoint - Done removing broadcast 6, response is 0
22:57:31.999 [block-manager-slave-async-thread-pool-26] DEBUG org.apache.spark.storage.BlockManagerSlaveEndpoint - Sent response: 0 to saras-air:55956
22:57:32.000 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned broadcast 6
22:57:32.070 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(176)
22:57:32.094 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 176
22:57:32.091 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$doExecute$4$adapted
22:57:32.100 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 176
22:57:32.105 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(179)
22:57:32.108 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 179
22:57:32.110 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 179
22:57:32.111 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(190)
22:57:32.111 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 190
22:57:32.111 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 190
22:57:32.111 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(208)
22:57:32.111 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 208
22:57:32.111 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 208
22:57:32.115 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanShuffle(1)
22:57:32.115 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning shuffle 1
22:57:32.140 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned shuffle 1
22:57:32.140 [block-manager-slave-async-thread-pool-27] DEBUG org.apache.spark.storage.BlockManagerSlaveEndpoint - removing shuffle 1
22:57:32.141 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(160)
22:57:32.141 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 160
22:57:32.141 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 160
22:57:32.141 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(186)
22:57:32.141 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 186
22:57:32.141 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 186
22:57:32.141 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(188)
22:57:32.141 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 188
22:57:32.141 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 188
22:57:32.141 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(154)
22:57:32.141 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 154
22:57:32.141 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 154
22:57:32.141 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(202)
22:57:32.142 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 202
22:57:32.142 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 202
22:57:32.142 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(200)
22:57:32.142 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 200
22:57:32.142 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 200
22:57:32.142 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(161)
22:57:32.142 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 161
22:57:32.142 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 161
22:57:32.142 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(209)
22:57:32.142 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 209
22:57:32.142 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 209
22:57:32.142 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(165)
22:57:32.142 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 165
22:57:32.142 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 165
22:57:32.142 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(149)
22:57:32.142 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 149
22:57:32.142 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 149
22:57:32.142 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(174)
22:57:32.142 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 174
22:57:32.142 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 174
22:57:32.142 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(187)
22:57:32.142 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 187
22:57:32.142 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 187
22:57:32.142 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(164)
22:57:32.142 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 164
22:57:32.142 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 164
22:57:32.142 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(183)
22:57:32.142 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 183
22:57:32.142 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 183
22:57:32.143 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(166)
22:57:32.144 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 166
22:57:32.144 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 166
22:57:32.144 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(191)
22:57:32.144 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 191
22:57:32.144 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 191
22:57:32.144 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(196)
22:57:32.144 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 196
22:57:32.144 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 196
22:57:32.144 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(169)
22:57:32.144 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 169
22:57:32.144 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 169
22:57:32.144 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(159)
22:57:32.144 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 159
22:57:32.144 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 159
22:57:32.144 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(158)
22:57:32.144 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 158
22:57:32.144 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 158
22:57:32.144 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(195)
22:57:32.144 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 195
22:57:32.144 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 195
22:57:32.144 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(163)
22:57:32.144 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 163
22:57:32.144 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 163
22:57:32.144 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(184)
22:57:32.144 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 184
22:57:32.144 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 184
22:57:32.145 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanBroadcast(8)
22:57:32.145 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning broadcast 8
22:57:32.145 [Spark Context Cleaner] DEBUG org.apache.spark.broadcast.TorrentBroadcast - Unpersisting TorrentBroadcast 8
22:57:32.146 [block-manager-slave-async-thread-pool-29] DEBUG org.apache.spark.storage.BlockManagerSlaveEndpoint - Done removing shuffle 1, response is true
22:57:32.146 [block-manager-slave-async-thread-pool-29] DEBUG org.apache.spark.storage.BlockManagerSlaveEndpoint - Sent response: true to saras-air:55956
22:57:32.148 [block-manager-slave-async-thread-pool-30] DEBUG org.apache.spark.storage.BlockManagerSlaveEndpoint - removing broadcast 8
22:57:32.149 [block-manager-slave-async-thread-pool-30] DEBUG org.apache.spark.storage.BlockManager - Removing broadcast 8
22:57:32.153 [block-manager-slave-async-thread-pool-30] DEBUG org.apache.spark.storage.BlockManager - Removing block broadcast_8
22:57:32.158 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$doExecute$4$adapted) is now cleaned +++
22:57:32.158 [block-manager-slave-async-thread-pool-30] DEBUG org.apache.spark.storage.memory.MemoryStore - Block broadcast_8 of size 21384 dropped from memory (free 1099536914)
22:57:32.166 [block-manager-slave-async-thread-pool-30] DEBUG org.apache.spark.storage.BlockManager - Removing block broadcast_8_piece0
22:57:32.169 [block-manager-slave-async-thread-pool-30] DEBUG org.apache.spark.storage.memory.MemoryStore - Block broadcast_8_piece0 of size 10610 dropped from memory (free 1099547524)
22:57:32.186 [dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Removed broadcast_8_piece0 on saras-air:55957 in memory (size: 10.4 KiB, free: 1048.8 MiB)
22:57:32.187 [block-manager-slave-async-thread-pool-30] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_8_piece0
22:57:32.188 [block-manager-slave-async-thread-pool-30] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_8_piece0
22:57:32.198 [block-manager-slave-async-thread-pool-32] DEBUG org.apache.spark.storage.BlockManagerSlaveEndpoint - Done removing broadcast 8, response is 0
22:57:32.200 [block-manager-slave-async-thread-pool-32] DEBUG org.apache.spark.storage.BlockManagerSlaveEndpoint - Sent response: 0 to saras-air:55956
22:57:32.201 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned broadcast 8
22:57:32.201 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(151)
22:57:32.201 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 151
22:57:32.202 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 151
22:57:32.202 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(171)
22:57:32.202 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 171
22:57:32.202 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 171
22:57:32.202 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(148)
22:57:32.202 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 148
22:57:32.202 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 148
22:57:32.202 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(201)
22:57:32.202 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 201
22:57:32.202 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 201
22:57:32.202 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(152)
22:57:32.202 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 152
22:57:32.202 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 152
22:57:32.202 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(192)
22:57:32.202 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 192
22:57:32.202 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 192
22:57:32.202 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(172)
22:57:32.202 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 172
22:57:32.202 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 172
22:57:32.202 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(185)
22:57:32.202 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 185
22:57:32.202 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 185
22:57:32.202 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Got cleaning task CleanAccum(156)
22:57:32.202 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaning accumulator 156
22:57:32.202 [Spark Context Cleaner] DEBUG org.apache.spark.ContextCleaner - Cleaned accumulator 156
22:57:32.288 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$executeTake$2
22:57:32.300 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$executeTake$2) is now cleaned +++
22:57:32.301 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.util.ClosureCleaner - Cleaning indylambda closure: $anonfun$runJob$5
22:57:32.311 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.util.ClosureCleaner -  +++ indylambda closure ($anonfun$runJob$5) is now cleaned +++
22:57:32.311 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.SparkContext - Starting job: show at WuzzufJobsAnalysisApplication.java:109
22:57:32.319 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Got job 3 (show at WuzzufJobsAnalysisApplication.java:109) with 1 output partitions
22:57:32.320 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 5 (show at WuzzufJobsAnalysisApplication.java:109)
22:57:32.320 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()
22:57:32.323 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Missing parents: List()
22:57:32.326 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - submitStage(ResultStage 5 (name=show at WuzzufJobsAnalysisApplication.java:109;jobs=3))
22:57:32.326 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - missing: List()
22:57:32.326 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 5 (MapPartitionsRDD[29] at show at WuzzufJobsAnalysisApplication.java:109), which has no missing parents
22:57:32.326 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - submitMissingTasks(ResultStage 5)
22:57:32.351 [dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_10 stored as values in memory (estimated size 22.1 KiB, free 1048.6 MiB)
22:57:32.352 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_10 locally took 4 ms
22:57:32.352 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_10 without replication took 4 ms
22:57:32.357 [dag-scheduler-event-loop] INFO org.apache.spark.storage.memory.MemoryStore - Block broadcast_10_piece0 stored as bytes in memory (estimated size 9.6 KiB, free 1048.6 MiB)
22:57:32.358 [dispatcher-BlockManagerMaster] INFO org.apache.spark.storage.BlockManagerInfo - Added broadcast_10_piece0 in memory on saras-air:55957 (size: 9.6 KiB, free: 1048.8 MiB)
22:57:32.359 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManagerMaster - Updated info of block broadcast_10_piece0
22:57:32.359 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Told master about block broadcast_10_piece0
22:57:32.359 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Put block broadcast_10_piece0 locally took 2 ms
22:57:32.359 [dag-scheduler-event-loop] DEBUG org.apache.spark.storage.BlockManager - Putting block broadcast_10_piece0 without replication took 2 ms
22:57:32.359 [dag-scheduler-event-loop] INFO org.apache.spark.SparkContext - Created broadcast 10 from broadcast at DAGScheduler.scala:1223
22:57:32.361 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[29] at show at WuzzufJobsAnalysisApplication.java:109) (first 15 tasks are for partitions Vector(0))
22:57:32.361 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Adding task set 5.0 with 1 tasks
22:57:32.362 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Epoch for TaskSet 5.0: 2
22:57:32.362 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Adding pending tasks took 0 ms
22:57:32.362 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.TaskSetManager - Valid locality levels for TaskSet 5.0: NO_PREF, ANY
22:57:32.363 [dispatcher-event-loop-0] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_5.0, runningTasks: 0
22:57:32.370 [dispatcher-event-loop-0] INFO org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 5, saras-air, executor driver, partition 0, PROCESS_LOCAL, 7808 bytes)
22:57:32.375 [Executor task launch worker for task 5] INFO org.apache.spark.executor.Executor - Running task 0.0 in stage 5.0 (TID 5)
22:57:32.383 [Executor task launch worker for task 5] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - stageTCMP: (5, 0) -> 1
22:57:32.387 [Executor task launch worker for task 5] DEBUG org.apache.spark.storage.BlockManager - Getting local block broadcast_10
22:57:32.387 [Executor task launch worker for task 5] DEBUG org.apache.spark.storage.BlockManager - Level for block broadcast_10 is StorageLevel(disk, memory, deserialized, 1 replicas)
22:57:32.500 [Executor task launch worker for task 5] INFO org.apache.spark.sql.execution.datasources.FileScanRDD - Reading File path: file:///Users/sue/Documents/ITI/java-for-ml/final-project/WuzzufJobsAnalysis/src/main/resources/Wuzzuf_Jobs.csv, range: 0-919991, partition values: [empty row]
22:57:32.526 [Executor task launch worker for task 5] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection - code for input[0, string, true],input[1, string, true],input[2, string, true],input[3, string, true],input[4, string, true],input[5, string, true],input[6, string, true],input[7, string, true]:
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(8, 256);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */     writeFields_0_0(i);
/* 031 */     writeFields_0_1(i);
/* 032 */     return (mutableStateArray_0[0].getRow());
/* 033 */   }
/* 034 */
/* 035 */
/* 036 */   private void writeFields_0_1(InternalRow i) {
/* 037 */
/* 038 */     boolean isNull_5 = i.isNullAt(5);
/* 039 */     UTF8String value_5 = isNull_5 ?
/* 040 */     null : (i.getUTF8String(5));
/* 041 */     if (isNull_5) {
/* 042 */       mutableStateArray_0[0].setNullAt(5);
/* 043 */     } else {
/* 044 */       mutableStateArray_0[0].write(5, value_5);
/* 045 */     }
/* 046 */
/* 047 */     boolean isNull_6 = i.isNullAt(6);
/* 048 */     UTF8String value_6 = isNull_6 ?
/* 049 */     null : (i.getUTF8String(6));
/* 050 */     if (isNull_6) {
/* 051 */       mutableStateArray_0[0].setNullAt(6);
/* 052 */     } else {
/* 053 */       mutableStateArray_0[0].write(6, value_6);
/* 054 */     }
/* 055 */
/* 056 */     boolean isNull_7 = i.isNullAt(7);
/* 057 */     UTF8String value_7 = isNull_7 ?
/* 058 */     null : (i.getUTF8String(7));
/* 059 */     if (isNull_7) {
/* 060 */       mutableStateArray_0[0].setNullAt(7);
/* 061 */     } else {
/* 062 */       mutableStateArray_0[0].write(7, value_7);
/* 063 */     }
/* 064 */
/* 065 */   }
/* 066 */
/* 067 */
/* 068 */   private void writeFields_0_0(InternalRow i) {
/* 069 */
/* 070 */     boolean isNull_0 = i.isNullAt(0);
/* 071 */     UTF8String value_0 = isNull_0 ?
/* 072 */     null : (i.getUTF8String(0));
/* 073 */     if (isNull_0) {
/* 074 */       mutableStateArray_0[0].setNullAt(0);
/* 075 */     } else {
/* 076 */       mutableStateArray_0[0].write(0, value_0);
/* 077 */     }
/* 078 */
/* 079 */     boolean isNull_1 = i.isNullAt(1);
/* 080 */     UTF8String value_1 = isNull_1 ?
/* 081 */     null : (i.getUTF8String(1));
/* 082 */     if (isNull_1) {
/* 083 */       mutableStateArray_0[0].setNullAt(1);
/* 084 */     } else {
/* 085 */       mutableStateArray_0[0].write(1, value_1);
/* 086 */     }
/* 087 */
/* 088 */     boolean isNull_2 = i.isNullAt(2);
/* 089 */     UTF8String value_2 = isNull_2 ?
/* 090 */     null : (i.getUTF8String(2));
/* 091 */     if (isNull_2) {
/* 092 */       mutableStateArray_0[0].setNullAt(2);
/* 093 */     } else {
/* 094 */       mutableStateArray_0[0].write(2, value_2);
/* 095 */     }
/* 096 */
/* 097 */     boolean isNull_3 = i.isNullAt(3);
/* 098 */     UTF8String value_3 = isNull_3 ?
/* 099 */     null : (i.getUTF8String(3));
/* 100 */     if (isNull_3) {
/* 101 */       mutableStateArray_0[0].setNullAt(3);
/* 102 */     } else {
/* 103 */       mutableStateArray_0[0].write(3, value_3);
/* 104 */     }
/* 105 */
/* 106 */     boolean isNull_4 = i.isNullAt(4);
/* 107 */     UTF8String value_4 = isNull_4 ?
/* 108 */     null : (i.getUTF8String(4));
/* 109 */     if (isNull_4) {
/* 110 */       mutableStateArray_0[0].setNullAt(4);
/* 111 */     } else {
/* 112 */       mutableStateArray_0[0].write(4, value_4);
/* 113 */     }
/* 114 */
/* 115 */   }
/* 116 */
/* 117 */ }

22:57:32.529 [Executor task launch worker for task 5] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificUnsafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[] mutableStateArray_0 = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter[1];
/* 009 */
/* 010 */   public SpecificUnsafeProjection(Object[] references) {
/* 011 */     this.references = references;
/* 012 */     mutableStateArray_0[0] = new org.apache.spark.sql.catalyst.expressions.codegen.UnsafeRowWriter(8, 256);
/* 013 */
/* 014 */   }
/* 015 */
/* 016 */   public void initialize(int partitionIndex) {
/* 017 */
/* 018 */   }
/* 019 */
/* 020 */   // Scala.Function1 need this
/* 021 */   public java.lang.Object apply(java.lang.Object row) {
/* 022 */     return apply((InternalRow) row);
/* 023 */   }
/* 024 */
/* 025 */   public UnsafeRow apply(InternalRow i) {
/* 026 */     mutableStateArray_0[0].reset();
/* 027 */
/* 028 */
/* 029 */     mutableStateArray_0[0].zeroOutNullBytes();
/* 030 */     writeFields_0_0(i);
/* 031 */     writeFields_0_1(i);
/* 032 */     return (mutableStateArray_0[0].getRow());
/* 033 */   }
/* 034 */
/* 035 */
/* 036 */   private void writeFields_0_1(InternalRow i) {
/* 037 */
/* 038 */     boolean isNull_5 = i.isNullAt(5);
/* 039 */     UTF8String value_5 = isNull_5 ?
/* 040 */     null : (i.getUTF8String(5));
/* 041 */     if (isNull_5) {
/* 042 */       mutableStateArray_0[0].setNullAt(5);
/* 043 */     } else {
/* 044 */       mutableStateArray_0[0].write(5, value_5);
/* 045 */     }
/* 046 */
/* 047 */     boolean isNull_6 = i.isNullAt(6);
/* 048 */     UTF8String value_6 = isNull_6 ?
/* 049 */     null : (i.getUTF8String(6));
/* 050 */     if (isNull_6) {
/* 051 */       mutableStateArray_0[0].setNullAt(6);
/* 052 */     } else {
/* 053 */       mutableStateArray_0[0].write(6, value_6);
/* 054 */     }
/* 055 */
/* 056 */     boolean isNull_7 = i.isNullAt(7);
/* 057 */     UTF8String value_7 = isNull_7 ?
/* 058 */     null : (i.getUTF8String(7));
/* 059 */     if (isNull_7) {
/* 060 */       mutableStateArray_0[0].setNullAt(7);
/* 061 */     } else {
/* 062 */       mutableStateArray_0[0].write(7, value_7);
/* 063 */     }
/* 064 */
/* 065 */   }
/* 066 */
/* 067 */
/* 068 */   private void writeFields_0_0(InternalRow i) {
/* 069 */
/* 070 */     boolean isNull_0 = i.isNullAt(0);
/* 071 */     UTF8String value_0 = isNull_0 ?
/* 072 */     null : (i.getUTF8String(0));
/* 073 */     if (isNull_0) {
/* 074 */       mutableStateArray_0[0].setNullAt(0);
/* 075 */     } else {
/* 076 */       mutableStateArray_0[0].write(0, value_0);
/* 077 */     }
/* 078 */
/* 079 */     boolean isNull_1 = i.isNullAt(1);
/* 080 */     UTF8String value_1 = isNull_1 ?
/* 081 */     null : (i.getUTF8String(1));
/* 082 */     if (isNull_1) {
/* 083 */       mutableStateArray_0[0].setNullAt(1);
/* 084 */     } else {
/* 085 */       mutableStateArray_0[0].write(1, value_1);
/* 086 */     }
/* 087 */
/* 088 */     boolean isNull_2 = i.isNullAt(2);
/* 089 */     UTF8String value_2 = isNull_2 ?
/* 090 */     null : (i.getUTF8String(2));
/* 091 */     if (isNull_2) {
/* 092 */       mutableStateArray_0[0].setNullAt(2);
/* 093 */     } else {
/* 094 */       mutableStateArray_0[0].write(2, value_2);
/* 095 */     }
/* 096 */
/* 097 */     boolean isNull_3 = i.isNullAt(3);
/* 098 */     UTF8String value_3 = isNull_3 ?
/* 099 */     null : (i.getUTF8String(3));
/* 100 */     if (isNull_3) {
/* 101 */       mutableStateArray_0[0].setNullAt(3);
/* 102 */     } else {
/* 103 */       mutableStateArray_0[0].write(3, value_3);
/* 104 */     }
/* 105 */
/* 106 */     boolean isNull_4 = i.isNullAt(4);
/* 107 */     UTF8String value_4 = isNull_4 ?
/* 108 */     null : (i.getUTF8String(4));
/* 109 */     if (isNull_4) {
/* 110 */       mutableStateArray_0[0].setNullAt(4);
/* 111 */     } else {
/* 112 */       mutableStateArray_0[0].write(4, value_4);
/* 113 */     }
/* 114 */
/* 115 */   }
/* 116 */
/* 117 */ }

22:57:32.642 [Executor task launch worker for task 5] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 115.214208 ms
22:57:32.644 [Executor task launch worker for task 5] DEBUG org.apache.spark.storage.BlockManager - Getting local block broadcast_9
22:57:32.645 [Executor task launch worker for task 5] DEBUG org.apache.spark.storage.BlockManager - Level for block broadcast_9 is StorageLevel(disk, memory, deserialized, 1 replicas)
22:57:32.714 [Executor task launch worker for task 5] INFO org.apache.spark.executor.Executor - Finished task 0.0 in stage 5.0 (TID 5). 4621 bytes result sent to driver
22:57:32.715 [Executor task launch worker for task 5] DEBUG org.apache.spark.executor.ExecutorMetricsPoller - removing (5, 0) from stageTCMP
22:57:32.717 [dispatcher-event-loop-0] DEBUG org.apache.spark.scheduler.TaskSchedulerImpl - parentName: , name: TaskSet_5.0, runningTasks: 0
22:57:32.717 [dispatcher-event-loop-0] DEBUG org.apache.spark.scheduler.TaskSetManager - No tasks for locality level NO_PREF, so moving to locality level ANY
22:57:32.721 [task-result-getter-1] INFO org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 5.0 (TID 5) in 357 ms on saras-air (executor driver) (1/1)
22:57:32.721 [task-result-getter-1] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Removed TaskSet 5.0, whose tasks have all completed, from pool 
22:57:32.723 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - ResultStage 5 (show at WuzzufJobsAnalysisApplication.java:109) finished in 0.388 s
22:57:32.724 [dag-scheduler-event-loop] DEBUG org.apache.spark.scheduler.DAGScheduler - After removal of stage 5, remaining stages = 0
22:57:32.724 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.DAGScheduler - Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
22:57:32.724 [dag-scheduler-event-loop] INFO org.apache.spark.scheduler.TaskSchedulerImpl - Killing all running tasks in stage 5: Stage finished
22:57:32.724 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.scheduler.DAGScheduler - Job 3 finished: show at WuzzufJobsAnalysisApplication.java:109, took 0.412589 s
22:57:32.747 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.GenerateSafeProjection - code for createexternalrow(input[0, string, true].toString, input[1, string, true].toString, input[2, string, true].toString, input[3, string, true].toString, input[4, string, true].toString, input[5, string, true].toString, input[6, string, false].toString, input[7, string, false].toString, input[8, string, true].toString, StructField(Title,StringType,true), StructField(Company,StringType,true), StructField(Location,StringType,true), StructField(YearsExp,StringType,true), StructField(Country,StringType,true), StructField(Skills,StringType,true), StructField(type-factorized,StringType,false), StructField(level-factorized,StringType,false), StructField(MinMaxYearsExp,StringType,true)):
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     Object[] values_0 = new Object[9];
/* 024 */     createExternalRow_0_0(i, values_0);
/* 025 */     createExternalRow_0_1(i, values_0);
/* 026 */     createExternalRow_0_2(i, values_0);
/* 027 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));
/* 028 */     if (false) {
/* 029 */       mutableRow.setNullAt(0);
/* 030 */     } else {
/* 031 */
/* 032 */       mutableRow.update(0, value_0);
/* 033 */     }
/* 034 */
/* 035 */     return mutableRow;
/* 036 */   }
/* 037 */
/* 038 */
/* 039 */   private void createExternalRow_0_2(InternalRow i, Object[] values_0) {
/* 040 */
/* 041 */     UTF8String value_14 = i.getUTF8String(6);
/* 042 */     boolean isNull_13 = true;
/* 043 */     java.lang.String value_13 = null;
/* 044 */     if (!false) {
/* 045 */
/* 046 */       isNull_13 = false;
/* 047 */       if (!isNull_13) {
/* 048 */
/* 049 */         Object funcResult_6 = null;
/* 050 */         funcResult_6 = value_14.toString();
/* 051 */         value_13 = (java.lang.String) funcResult_6;
/* 052 */
/* 053 */       }
/* 054 */     }
/* 055 */     if (isNull_13) {
/* 056 */       values_0[6] = null;
/* 057 */     } else {
/* 058 */       values_0[6] = value_13;
/* 059 */     }
/* 060 */
/* 061 */     UTF8String value_16 = i.getUTF8String(7);
/* 062 */     boolean isNull_15 = true;
/* 063 */     java.lang.String value_15 = null;
/* 064 */     if (!false) {
/* 065 */
/* 066 */       isNull_15 = false;
/* 067 */       if (!isNull_15) {
/* 068 */
/* 069 */         Object funcResult_7 = null;
/* 070 */         funcResult_7 = value_16.toString();
/* 071 */         value_15 = (java.lang.String) funcResult_7;
/* 072 */
/* 073 */       }
/* 074 */     }
/* 075 */     if (isNull_15) {
/* 076 */       values_0[7] = null;
/* 077 */     } else {
/* 078 */       values_0[7] = value_15;
/* 079 */     }
/* 080 */
/* 081 */     boolean isNull_18 = i.isNullAt(8);
/* 082 */     UTF8String value_18 = isNull_18 ?
/* 083 */     null : (i.getUTF8String(8));
/* 084 */     boolean isNull_17 = true;
/* 085 */     java.lang.String value_17 = null;
/* 086 */     if (!isNull_18) {
/* 087 */
/* 088 */       isNull_17 = false;
/* 089 */       if (!isNull_17) {
/* 090 */
/* 091 */         Object funcResult_8 = null;
/* 092 */         funcResult_8 = value_18.toString();
/* 093 */         value_17 = (java.lang.String) funcResult_8;
/* 094 */
/* 095 */       }
/* 096 */     }
/* 097 */     if (isNull_17) {
/* 098 */       values_0[8] = null;
/* 099 */     } else {
/* 100 */       values_0[8] = value_17;
/* 101 */     }
/* 102 */
/* 103 */   }
/* 104 */
/* 105 */
/* 106 */   private void createExternalRow_0_1(InternalRow i, Object[] values_0) {
/* 107 */
/* 108 */     boolean isNull_8 = i.isNullAt(3);
/* 109 */     UTF8String value_8 = isNull_8 ?
/* 110 */     null : (i.getUTF8String(3));
/* 111 */     boolean isNull_7 = true;
/* 112 */     java.lang.String value_7 = null;
/* 113 */     if (!isNull_8) {
/* 114 */
/* 115 */       isNull_7 = false;
/* 116 */       if (!isNull_7) {
/* 117 */
/* 118 */         Object funcResult_3 = null;
/* 119 */         funcResult_3 = value_8.toString();
/* 120 */         value_7 = (java.lang.String) funcResult_3;
/* 121 */
/* 122 */       }
/* 123 */     }
/* 124 */     if (isNull_7) {
/* 125 */       values_0[3] = null;
/* 126 */     } else {
/* 127 */       values_0[3] = value_7;
/* 128 */     }
/* 129 */
/* 130 */     boolean isNull_10 = i.isNullAt(4);
/* 131 */     UTF8String value_10 = isNull_10 ?
/* 132 */     null : (i.getUTF8String(4));
/* 133 */     boolean isNull_9 = true;
/* 134 */     java.lang.String value_9 = null;
/* 135 */     if (!isNull_10) {
/* 136 */
/* 137 */       isNull_9 = false;
/* 138 */       if (!isNull_9) {
/* 139 */
/* 140 */         Object funcResult_4 = null;
/* 141 */         funcResult_4 = value_10.toString();
/* 142 */         value_9 = (java.lang.String) funcResult_4;
/* 143 */
/* 144 */       }
/* 145 */     }
/* 146 */     if (isNull_9) {
/* 147 */       values_0[4] = null;
/* 148 */     } else {
/* 149 */       values_0[4] = value_9;
/* 150 */     }
/* 151 */
/* 152 */     boolean isNull_12 = i.isNullAt(5);
/* 153 */     UTF8String value_12 = isNull_12 ?
/* 154 */     null : (i.getUTF8String(5));
/* 155 */     boolean isNull_11 = true;
/* 156 */     java.lang.String value_11 = null;
/* 157 */     if (!isNull_12) {
/* 158 */
/* 159 */       isNull_11 = false;
/* 160 */       if (!isNull_11) {
/* 161 */
/* 162 */         Object funcResult_5 = null;
/* 163 */         funcResult_5 = value_12.toString();
/* 164 */         value_11 = (java.lang.String) funcResult_5;
/* 165 */
/* 166 */       }
/* 167 */     }
/* 168 */     if (isNull_11) {
/* 169 */       values_0[5] = null;
/* 170 */     } else {
/* 171 */       values_0[5] = value_11;
/* 172 */     }
/* 173 */
/* 174 */   }
/* 175 */
/* 176 */
/* 177 */   private void createExternalRow_0_0(InternalRow i, Object[] values_0) {
/* 178 */
/* 179 */     boolean isNull_2 = i.isNullAt(0);
/* 180 */     UTF8String value_2 = isNull_2 ?
/* 181 */     null : (i.getUTF8String(0));
/* 182 */     boolean isNull_1 = true;
/* 183 */     java.lang.String value_1 = null;
/* 184 */     if (!isNull_2) {
/* 185 */
/* 186 */       isNull_1 = false;
/* 187 */       if (!isNull_1) {
/* 188 */
/* 189 */         Object funcResult_0 = null;
/* 190 */         funcResult_0 = value_2.toString();
/* 191 */         value_1 = (java.lang.String) funcResult_0;
/* 192 */
/* 193 */       }
/* 194 */     }
/* 195 */     if (isNull_1) {
/* 196 */       values_0[0] = null;
/* 197 */     } else {
/* 198 */       values_0[0] = value_1;
/* 199 */     }
/* 200 */
/* 201 */     boolean isNull_4 = i.isNullAt(1);
/* 202 */     UTF8String value_4 = isNull_4 ?
/* 203 */     null : (i.getUTF8String(1));
/* 204 */     boolean isNull_3 = true;
/* 205 */     java.lang.String value_3 = null;
/* 206 */     if (!isNull_4) {
/* 207 */
/* 208 */       isNull_3 = false;
/* 209 */       if (!isNull_3) {
/* 210 */
/* 211 */         Object funcResult_1 = null;
/* 212 */         funcResult_1 = value_4.toString();
/* 213 */         value_3 = (java.lang.String) funcResult_1;
/* 214 */
/* 215 */       }
/* 216 */     }
/* 217 */     if (isNull_3) {
/* 218 */       values_0[1] = null;
/* 219 */     } else {
/* 220 */       values_0[1] = value_3;
/* 221 */     }
/* 222 */
/* 223 */     boolean isNull_6 = i.isNullAt(2);
/* 224 */     UTF8String value_6 = isNull_6 ?
/* 225 */     null : (i.getUTF8String(2));
/* 226 */     boolean isNull_5 = true;
/* 227 */     java.lang.String value_5 = null;
/* 228 */     if (!isNull_6) {
/* 229 */
/* 230 */       isNull_5 = false;
/* 231 */       if (!isNull_5) {
/* 232 */
/* 233 */         Object funcResult_2 = null;
/* 234 */         funcResult_2 = value_6.toString();
/* 235 */         value_5 = (java.lang.String) funcResult_2;
/* 236 */
/* 237 */       }
/* 238 */     }
/* 239 */     if (isNull_5) {
/* 240 */       values_0[2] = null;
/* 241 */     } else {
/* 242 */       values_0[2] = value_5;
/* 243 */     }
/* 244 */
/* 245 */   }
/* 246 */
/* 247 */ }

22:57:32.751 [com.example.main.WuzzufJobsAnalysisApplication.main()] DEBUG org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - 
/* 001 */ public java.lang.Object generate(Object[] references) {
/* 002 */   return new SpecificSafeProjection(references);
/* 003 */ }
/* 004 */
/* 005 */ class SpecificSafeProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseProjection {
/* 006 */
/* 007 */   private Object[] references;
/* 008 */   private InternalRow mutableRow;
/* 009 */
/* 010 */
/* 011 */   public SpecificSafeProjection(Object[] references) {
/* 012 */     this.references = references;
/* 013 */     mutableRow = (InternalRow) references[references.length - 1];
/* 014 */
/* 015 */   }
/* 016 */
/* 017 */   public void initialize(int partitionIndex) {
/* 018 */
/* 019 */   }
/* 020 */
/* 021 */   public java.lang.Object apply(java.lang.Object _i) {
/* 022 */     InternalRow i = (InternalRow) _i;
/* 023 */     Object[] values_0 = new Object[9];
/* 024 */     createExternalRow_0_0(i, values_0);
/* 025 */     createExternalRow_0_1(i, values_0);
/* 026 */     createExternalRow_0_2(i, values_0);
/* 027 */     final org.apache.spark.sql.Row value_0 = new org.apache.spark.sql.catalyst.expressions.GenericRowWithSchema(values_0, ((org.apache.spark.sql.types.StructType) references[0] /* schema */));
/* 028 */     if (false) {
/* 029 */       mutableRow.setNullAt(0);
/* 030 */     } else {
/* 031 */
/* 032 */       mutableRow.update(0, value_0);
/* 033 */     }
/* 034 */
/* 035 */     return mutableRow;
/* 036 */   }
/* 037 */
/* 038 */
/* 039 */   private void createExternalRow_0_2(InternalRow i, Object[] values_0) {
/* 040 */
/* 041 */     UTF8String value_14 = i.getUTF8String(6);
/* 042 */     boolean isNull_13 = true;
/* 043 */     java.lang.String value_13 = null;
/* 044 */     if (!false) {
/* 045 */
/* 046 */       isNull_13 = false;
/* 047 */       if (!isNull_13) {
/* 048 */
/* 049 */         Object funcResult_6 = null;
/* 050 */         funcResult_6 = value_14.toString();
/* 051 */         value_13 = (java.lang.String) funcResult_6;
/* 052 */
/* 053 */       }
/* 054 */     }
/* 055 */     if (isNull_13) {
/* 056 */       values_0[6] = null;
/* 057 */     } else {
/* 058 */       values_0[6] = value_13;
/* 059 */     }
/* 060 */
/* 061 */     UTF8String value_16 = i.getUTF8String(7);
/* 062 */     boolean isNull_15 = true;
/* 063 */     java.lang.String value_15 = null;
/* 064 */     if (!false) {
/* 065 */
/* 066 */       isNull_15 = false;
/* 067 */       if (!isNull_15) {
/* 068 */
/* 069 */         Object funcResult_7 = null;
/* 070 */         funcResult_7 = value_16.toString();
/* 071 */         value_15 = (java.lang.String) funcResult_7;
/* 072 */
/* 073 */       }
/* 074 */     }
/* 075 */     if (isNull_15) {
/* 076 */       values_0[7] = null;
/* 077 */     } else {
/* 078 */       values_0[7] = value_15;
/* 079 */     }
/* 080 */
/* 081 */     boolean isNull_18 = i.isNullAt(8);
/* 082 */     UTF8String value_18 = isNull_18 ?
/* 083 */     null : (i.getUTF8String(8));
/* 084 */     boolean isNull_17 = true;
/* 085 */     java.lang.String value_17 = null;
/* 086 */     if (!isNull_18) {
/* 087 */
/* 088 */       isNull_17 = false;
/* 089 */       if (!isNull_17) {
/* 090 */
/* 091 */         Object funcResult_8 = null;
/* 092 */         funcResult_8 = value_18.toString();
/* 093 */         value_17 = (java.lang.String) funcResult_8;
/* 094 */
/* 095 */       }
/* 096 */     }
/* 097 */     if (isNull_17) {
/* 098 */       values_0[8] = null;
/* 099 */     } else {
/* 100 */       values_0[8] = value_17;
/* 101 */     }
/* 102 */
/* 103 */   }
/* 104 */
/* 105 */
/* 106 */   private void createExternalRow_0_1(InternalRow i, Object[] values_0) {
/* 107 */
/* 108 */     boolean isNull_8 = i.isNullAt(3);
/* 109 */     UTF8String value_8 = isNull_8 ?
/* 110 */     null : (i.getUTF8String(3));
/* 111 */     boolean isNull_7 = true;
/* 112 */     java.lang.String value_7 = null;
/* 113 */     if (!isNull_8) {
/* 114 */
/* 115 */       isNull_7 = false;
/* 116 */       if (!isNull_7) {
/* 117 */
/* 118 */         Object funcResult_3 = null;
/* 119 */         funcResult_3 = value_8.toString();
/* 120 */         value_7 = (java.lang.String) funcResult_3;
/* 121 */
/* 122 */       }
/* 123 */     }
/* 124 */     if (isNull_7) {
/* 125 */       values_0[3] = null;
/* 126 */     } else {
/* 127 */       values_0[3] = value_7;
/* 128 */     }
/* 129 */
/* 130 */     boolean isNull_10 = i.isNullAt(4);
/* 131 */     UTF8String value_10 = isNull_10 ?
/* 132 */     null : (i.getUTF8String(4));
/* 133 */     boolean isNull_9 = true;
/* 134 */     java.lang.String value_9 = null;
/* 135 */     if (!isNull_10) {
/* 136 */
/* 137 */       isNull_9 = false;
/* 138 */       if (!isNull_9) {
/* 139 */
/* 140 */         Object funcResult_4 = null;
/* 141 */         funcResult_4 = value_10.toString();
/* 142 */         value_9 = (java.lang.String) funcResult_4;
/* 143 */
/* 144 */       }
/* 145 */     }
/* 146 */     if (isNull_9) {
/* 147 */       values_0[4] = null;
/* 148 */     } else {
/* 149 */       values_0[4] = value_9;
/* 150 */     }
/* 151 */
/* 152 */     boolean isNull_12 = i.isNullAt(5);
/* 153 */     UTF8String value_12 = isNull_12 ?
/* 154 */     null : (i.getUTF8String(5));
/* 155 */     boolean isNull_11 = true;
/* 156 */     java.lang.String value_11 = null;
/* 157 */     if (!isNull_12) {
/* 158 */
/* 159 */       isNull_11 = false;
/* 160 */       if (!isNull_11) {
/* 161 */
/* 162 */         Object funcResult_5 = null;
/* 163 */         funcResult_5 = value_12.toString();
/* 164 */         value_11 = (java.lang.String) funcResult_5;
/* 165 */
/* 166 */       }
/* 167 */     }
/* 168 */     if (isNull_11) {
/* 169 */       values_0[5] = null;
/* 170 */     } else {
/* 171 */       values_0[5] = value_11;
/* 172 */     }
/* 173 */
/* 174 */   }
/* 175 */
/* 176 */
/* 177 */   private void createExternalRow_0_0(InternalRow i, Object[] values_0) {
/* 178 */
/* 179 */     boolean isNull_2 = i.isNullAt(0);
/* 180 */     UTF8String value_2 = isNull_2 ?
/* 181 */     null : (i.getUTF8String(0));
/* 182 */     boolean isNull_1 = true;
/* 183 */     java.lang.String value_1 = null;
/* 184 */     if (!isNull_2) {
/* 185 */
/* 186 */       isNull_1 = false;
/* 187 */       if (!isNull_1) {
/* 188 */
/* 189 */         Object funcResult_0 = null;
/* 190 */         funcResult_0 = value_2.toString();
/* 191 */         value_1 = (java.lang.String) funcResult_0;
/* 192 */
/* 193 */       }
/* 194 */     }
/* 195 */     if (isNull_1) {
/* 196 */       values_0[0] = null;
/* 197 */     } else {
/* 198 */       values_0[0] = value_1;
/* 199 */     }
/* 200 */
/* 201 */     boolean isNull_4 = i.isNullAt(1);
/* 202 */     UTF8String value_4 = isNull_4 ?
/* 203 */     null : (i.getUTF8String(1));
/* 204 */     boolean isNull_3 = true;
/* 205 */     java.lang.String value_3 = null;
/* 206 */     if (!isNull_4) {
/* 207 */
/* 208 */       isNull_3 = false;
/* 209 */       if (!isNull_3) {
/* 210 */
/* 211 */         Object funcResult_1 = null;
/* 212 */         funcResult_1 = value_4.toString();
/* 213 */         value_3 = (java.lang.String) funcResult_1;
/* 214 */
/* 215 */       }
/* 216 */     }
/* 217 */     if (isNull_3) {
/* 218 */       values_0[1] = null;
/* 219 */     } else {
/* 220 */       values_0[1] = value_3;
/* 221 */     }
/* 222 */
/* 223 */     boolean isNull_6 = i.isNullAt(2);
/* 224 */     UTF8String value_6 = isNull_6 ?
/* 225 */     null : (i.getUTF8String(2));
/* 226 */     boolean isNull_5 = true;
/* 227 */     java.lang.String value_5 = null;
/* 228 */     if (!isNull_6) {
/* 229 */
/* 230 */       isNull_5 = false;
/* 231 */       if (!isNull_5) {
/* 232 */
/* 233 */         Object funcResult_2 = null;
/* 234 */         funcResult_2 = value_6.toString();
/* 235 */         value_5 = (java.lang.String) funcResult_2;
/* 236 */
/* 237 */       }
/* 238 */     }
/* 239 */     if (isNull_5) {
/* 240 */       values_0[2] = null;
/* 241 */     } else {
/* 242 */       values_0[2] = value_5;
/* 243 */     }
/* 244 */
/* 245 */   }
/* 246 */
/* 247 */ }

22:57:32.847 [com.example.main.WuzzufJobsAnalysisApplication.main()] INFO org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 98.760708 ms
+--------------------+--------------------+-----------+---------------+-------+--------------------+---------------+----------------+--------------+
|               Title|             Company|   Location|       YearsExp|Country|              Skills|type-factorized|level-factorized|MinMaxYearsExp|
+--------------------+--------------------+-----------+---------------+-------+--------------------+---------------+----------------+--------------+
|Customer Service ...|    Johnson Controls|  New Cairo| 1-3 Yrs of Exp|  Cairo|Call Center, Cust...|            0.0|             1.0|           1 3|
|   Marketing Manager|SYE English Commu...|      Dokki|5-10 Yrs of Exp|   Giza|Market Research, ...|            0.0|             2.0|          5 10|
|Medical Represent...|     So Value Pharma|      Cairo| 0-5 Yrs of Exp|  Egypt|Cardio-metabolic,...|            0.0|             1.0|           0 5|
|Order Handling Sp...|    Johnson Controls|  New Cairo| 3-5 Yrs of Exp|  Cairo|Supply Officer, S...|            0.0|             0.0|           3 5|
|         Storekeeper|        Confidential|      Cairo|  3+ Yrs of Exp|  Egypt|Warehousing, Stor...|            0.0|             0.0|           3 3|
|Senior .NET Devel...|               Dexef|      Maadi| 3-5 Yrs of Exp|  Cairo|ASP.NET, Desktop ...|            0.0|             0.0|           3 5|
|Preschool French ...|Leap Development Hub|  New Cairo| 1-3 Yrs of Exp|  Cairo|Education, Educat...|            0.0|             1.0|           1 3|
|Junior Business D...|E3mel Business fo...|      Maadi| 1-3 Yrs of Exp|  Cairo|Sales Target, Sal...|            0.0|             1.0|           1 3|
|Indoor Sales Exec...|         Line Design|  Nasr City| 1-3 Yrs of Exp|  Cairo|sales skills, Sal...|            0.0|             0.0|           1 3|
|Content Creator /...|            EL KHETA| Alexandria|  1+ Yrs of Exp|  Egypt|Copywriter, SEO, ...|            0.0|             1.0|           1 1|
|STEAM Instructor ...|             BeSteam|  Nasr City|null Yrs of Exp|  Cairo|Education, Traini...|            1.0|             8.0|          null|
|English Instructo...|             Speakup|     Helwan| 1-5 Yrs of Exp|  Cairo|3 Vacancies, Male...|            2.0|             7.0|           1 5|
|      Lawyer - Cairo|      Bayt El Khebra|      Maadi| 15+ Yrs of Exp|  Cairo|legal, lawyer, Legal|            0.0|             0.0|         15 15|
|Site Supervisor ...|      ZDS Architects|      Cairo|  2+ Yrs of Exp|  Egypt|Architecture, Eng...|            0.0|             0.0|           2 2|
|Junior Personnel ...|    El-Moasser Books|  Nasr City| 1-2 Yrs of Exp|  Cairo|Human Resources (...|            0.0|             1.0|           1 2|
|    Floor controller|           Atheel CC|      Cairo|null Yrs of Exp|  Egypt|Communication Ski...|            0.0|             1.0|          null|
|Procurement Speci...|      ZDS Architects|      Cairo|  3+ Yrs of Exp|  Egypt|Construction, Com...|            0.0|             0.0|           3 3|
|        Receptionist|           Atheel CC|      Cairo|null Yrs of Exp|  Egypt|Administration, O...|            0.0|             1.0|          null|
|Product Manager -...|           Silicon21|  Nasr City| 3-5 Yrs of Exp|  Cairo|Electronics, Info...|            0.0|             7.0|           3 5|
|Resident Engineer...|           Silicon21|  Nasr City| 1-2 Yrs of Exp|  Cairo|Telecom, IT, Comp...|            0.0|             1.0|           1 2|
+--------------------+--------------------+-----------+---------------+-------+--------------------+---------------+----------------+--------------+
only showing top 20 rows

+++++========++++++++Done
22:57:32.879 [SparkUI-33] DEBUG org.sparkproject.jetty.io.ManagedSelector - Selector sun.nio.ch.KQueueSelectorImpl@6247a142 woken with none selected
22:57:32.879 [SparkUI-31] DEBUG org.sparkproject.jetty.io.ManagedSelector - Selector sun.nio.ch.KQueueSelectorImpl@6e95f513 woken with none selected
22:57:32.880 [SparkUI-32] DEBUG org.sparkproject.jetty.io.ManagedSelector - Selector sun.nio.ch.KQueueSelectorImpl@26c2c984 woken with none selected
22:57:32.880 [SparkUI-34] DEBUG org.sparkproject.jetty.io.ManagedSelector - Selector sun.nio.ch.KQueueSelectorImpl@200bfc09 woken with none selected
22:57:32.882 [rpc-boss-3-1] DEBUG io.netty.channel.nio.NioEventLoop - Selector.select() returned prematurely because Thread.currentThread().interrupt() was called. Use NioEventLoop.shutdownGracefully() to shutdown the NioEventLoop.
22:57:32.884 [SparkUI-34] DEBUG org.sparkproject.jetty.io.ManagedSelector - Selector sun.nio.ch.KQueueSelectorImpl@200bfc09 woken up from select, 0/0/0 selected
22:57:32.885 [SparkUI-31] DEBUG org.sparkproject.jetty.io.ManagedSelector - Selector sun.nio.ch.KQueueSelectorImpl@6e95f513 woken up from select, 0/0/0 selected
22:57:32.885 [SparkUI-31] DEBUG org.sparkproject.jetty.io.ManagedSelector - Selector sun.nio.ch.KQueueSelectorImpl@6e95f513 processing 0 keys, 0 updates
22:57:32.884 [SparkUI-32] DEBUG org.sparkproject.jetty.io.ManagedSelector - Selector sun.nio.ch.KQueueSelectorImpl@26c2c984 woken up from select, 0/0/0 selected
22:57:32.885 [SparkUI-32] DEBUG org.sparkproject.jetty.io.ManagedSelector - Selector sun.nio.ch.KQueueSelectorImpl@26c2c984 processing 0 keys, 0 updates
22:57:32.883 [SparkUI-33] DEBUG org.sparkproject.jetty.io.ManagedSelector - Selector sun.nio.ch.KQueueSelectorImpl@6247a142 woken up from select, 0/0/0 selected
22:57:32.886 [SparkUI-33] DEBUG org.sparkproject.jetty.io.ManagedSelector - Selector sun.nio.ch.KQueueSelectorImpl@6247a142 processing 0 keys, 0 updates
22:57:32.887 [SparkUI-32] DEBUG org.sparkproject.jetty.io.ManagedSelector - updateable 0
22:57:32.887 [SparkUI-32] DEBUG org.sparkproject.jetty.io.ManagedSelector - updates 0
22:57:32.887 [SparkUI-31] DEBUG org.sparkproject.jetty.io.ManagedSelector - updateable 0
22:57:32.887 [SparkUI-31] DEBUG org.sparkproject.jetty.io.ManagedSelector - updates 0
22:57:32.887 [SparkUI-31] DEBUG org.sparkproject.jetty.io.ManagedSelector - Selector sun.nio.ch.KQueueSelectorImpl@6e95f513 waiting with 0 keys
22:57:32.887 [SparkUI-33] DEBUG org.sparkproject.jetty.io.ManagedSelector - updateable 0
22:57:32.887 [SparkUI-32] DEBUG org.sparkproject.jetty.io.ManagedSelector - Selector sun.nio.ch.KQueueSelectorImpl@26c2c984 waiting with 0 keys
22:57:32.888 [SparkUI-34] DEBUG org.sparkproject.jetty.io.ManagedSelector - Selector sun.nio.ch.KQueueSelectorImpl@200bfc09 processing 0 keys, 0 updates
22:57:32.888 [SparkUI-33] DEBUG org.sparkproject.jetty.io.ManagedSelector - updates 0
22:57:32.888 [SparkUI-34] DEBUG org.sparkproject.jetty.io.ManagedSelector - updateable 0
22:57:32.888 [SparkUI-33] DEBUG org.sparkproject.jetty.io.ManagedSelector - Selector sun.nio.ch.KQueueSelectorImpl@6247a142 waiting with 0 keys
22:57:32.888 [SparkUI-34] DEBUG org.sparkproject.jetty.io.ManagedSelector - updates 0
22:57:32.888 [SparkUI-34] DEBUG org.sparkproject.jetty.io.ManagedSelector - Selector sun.nio.ch.KQueueSelectorImpl@200bfc09 waiting with 0 keys
22:57:32.889 [SparkUI-35] DEBUG org.sparkproject.jetty.util.thread.QueuedThreadPool - ran SparkUI-35-acceptor-0@57c892-ServerConnector@71f030a{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
22:57:32.891 [SparkUI-36] DEBUG org.sparkproject.jetty.util.thread.QueuedThreadPool - Starting Thread[SparkUI-118,5,com.example.main.WuzzufJobsAnalysisApplication]
22:57:32.891 [SparkUI-37] DEBUG org.sparkproject.jetty.util.thread.QueuedThreadPool - Starting Thread[SparkUI-119,5,com.example.main.WuzzufJobsAnalysisApplication]
22:57:32.891 [SparkUI-38] DEBUG org.sparkproject.jetty.util.thread.QueuedThreadPool - Starting Thread[SparkUI-120,5,com.example.main.WuzzufJobsAnalysisApplication]
22:57:32.902 [shuffle-boss-6-1] DEBUG io.netty.channel.nio.NioEventLoop - Selector.select() returned prematurely because Thread.currentThread().interrupt() was called. Use NioEventLoop.shutdownGracefully() to shutdown the NioEventLoop.
22:57:32.915 [org.apache.hadoop.fs.FileSystem$Statistics$StatisticsDataReferenceCleaner] WARN org.apache.hadoop.fs.FileSystem - exception in the cleaner thread but it will continue to run
java.lang.InterruptedException: null
	at java.base/java.lang.Object.wait(Native Method)
	at java.base/java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:155)
	at java.base/java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:176)
	at org.apache.hadoop.fs.FileSystem$Statistics$StatisticsDataReferenceCleaner.run(FileSystem.java:3063)
	at java.base/java.lang.Thread.run(Thread.java:834)
22:57:32.915 [spark-listener-group-appStatus] ERROR org.apache.spark.util.Utils - uncaught error in thread spark-listener-group-appStatus, stopping SparkContext
java.lang.InterruptedException: null
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2090)
	at java.base/java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:433)
	at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:110)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1319)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)
22:57:32.915 [spark-listener-group-shared] ERROR org.apache.spark.util.Utils - uncaught error in thread spark-listener-group-shared, stopping SparkContext
java.lang.InterruptedException: null
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2090)
	at java.base/java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:433)
	at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:110)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1319)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)
22:57:32.917 [spark-listener-group-executorManagement] ERROR org.apache.spark.util.Utils - uncaught error in thread spark-listener-group-executorManagement, stopping SparkContext
java.lang.InterruptedException: null
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2090)
	at java.base/java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:433)
	at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:110)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1319)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)
22:57:32.917 [Spark Context Cleaner] ERROR org.apache.spark.ContextCleaner - Error in cleaning thread
java.lang.InterruptedException: null
	at java.base/java.lang.Object.wait(Native Method)
	at java.base/java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:155)
	at org.apache.spark.ContextCleaner.$anonfun$keepCleaning$1(ContextCleaner.scala:182)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1319)
	at org.apache.spark.ContextCleaner.org$apache$spark$ContextCleaner$$keepCleaning(ContextCleaner.scala:180)
	at org.apache.spark.ContextCleaner$$anon$1.run(ContextCleaner.scala:77)
22:57:32.932 [stop-spark-context] INFO org.apache.spark.SparkContext - SparkContext already stopped.
22:57:32.933 [stop-spark-context] INFO org.apache.spark.SparkContext - SparkContext already stopped.
22:57:32.933 [spark-listener-group-executorManagement] ERROR org.apache.spark.util.Utils - throw uncaught fatal error in thread spark-listener-group-executorManagement
java.lang.InterruptedException: null
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2090)
	at java.base/java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:433)
	at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:110)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1319)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)
22:57:32.933 [spark-listener-group-shared] ERROR org.apache.spark.util.Utils - throw uncaught fatal error in thread spark-listener-group-shared
java.lang.InterruptedException: null
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2090)
	at java.base/java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:433)
	at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:110)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1319)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)
22:57:32.932 [spark-listener-group-appStatus] ERROR org.apache.spark.util.Utils - throw uncaught fatal error in thread spark-listener-group-appStatus
java.lang.InterruptedException: null
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2090)
	at java.base/java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:433)
	at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:110)
	at scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.java:23)
	at scala.util.DynamicVariable.withValue(DynamicVariable.scala:62)
	at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:100)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:96)
	at org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1319)
	at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:96)
[WARNING] 
java.lang.InterruptedException
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait22:57:32.954 [stop-spark-context] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - stopping Server@1e82d188{STARTED}[9.4.z-SNAPSHOT]
 (AbstractQueuedSynchronizer.java:2056)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await (AbstractQueuedSynchronizer.java:2090)
    at java.util.concurrent.LinkedBlockingQueue.take (LinkedBlockingQueue.java:433)
    at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1 (AsyncEventQueue.scala:110)
    at scala.runtime.java8.JFunction0$mcJ$sp.apply (JFunction0$mcJ$sp.java:23)
    at scala.util.DynamicVariable.withValue (DynamicVariable.scala:62)
    at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch (AsyncEventQueue.scala:100)
    at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1 (AsyncEventQueue.scala:96)
    at org.apache.spark.util.Utils$.tryOrStopSparkContext (Utils.scala:1319)
    at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run (AsyncEventQueue.scala:96)
22:57:32.958 [stop-spark-context] DEBUG org.sparkproject.jetty.server.Server - doStop Server@1e82d188{STOPPING}[9.4.z-SNAPSHOT]
[WARNING] 
java.lang.InterruptedException
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait (AbstractQueuedSynchronizer.java:2056)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await (AbstractQueuedSynchronizer.java:2090)
    at java.util.concurrent.LinkedBlockingQueue.take (LinkedBlockingQueue.java:433)
    at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1 (AsyncEventQueue.scala:110)
    at scala.runtime.java8.JFunction0$mcJ$sp.apply (JFunction0$mcJ$sp.java:23)
    at scala.util.DynamicVariable.withValue (DynamicVariable.scala:62)
    at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch (AsyncEventQueue.scala:100)
    at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1 (AsyncEventQueue.scala:96)
    at org.apache.spark.util.Utils$.tryOrStopSparkContext (Utils.scala:1319)
    at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run (AsyncEventQueue.scala:96)
[WARNING] 
java.lang.InterruptedException
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait (AbstractQueuedSynchronizer.java:2056)
    at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await (AbstractQueuedSynchronizer.java:2090)
    at java.util.concurrent.LinkedBlockingQueue.take (LinkedBlockingQueue.java:433)
    at org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1 (AsyncEventQueue.scala:110)
    at scala.runtime.java8.JFunction0$mcJ$sp.apply (JFunction0$mcJ$sp.java:23)
    at scala.util.DynamicVariable.withValue (DynamicVariable.scala:62)
    at org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch (AsyncEventQueue.scala:100)
    at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1 (AsyncEventQueue.scala:96)
    at org.apache.spark.util.Utils$.tryOrStopSparkContext (Utils.scala:1319)
    at org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run (AsyncEventQueue.scala:96)
22:57:32.983 [stop-spark-context] DEBUG org.sparkproject.jetty.server.handler.AbstractHandlerContainer - Graceful shutdown Server@1e82d188{STOPPING}[9.4.z-SNAPSHOT] by 
22:57:32.986 [stop-spark-context] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - stopping Spark@71f030a{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
22:57:32.987 [stop-spark-context] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - stopping SelectorManager@Spark@71f030a{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
22:57:32.988 [stop-spark-context] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - stopping ManagedSelector@a312b2e{STARTED} id=3 keys=0 selected=0 updates=0
22:57:32.989 [stop-spark-context] DEBUG org.sparkproject.jetty.io.ManagedSelector - Queued change org.sparkproject.jetty.io.ManagedSelector$CloseConnections@62293ea3 on ManagedSelector@a312b2e{STOPPING} id=3 keys=0 selected=0 updates=0
22:57:32.989 [stop-spark-context] DEBUG org.sparkproject.jetty.io.ManagedSelector - Wakeup on submit ManagedSelector@a312b2e{STOPPING} id=3 keys=0 selected=0 updates=1
22:57:32.989 [SparkUI-34] DEBUG org.sparkproject.jetty.io.ManagedSelector - Selector sun.nio.ch.KQueueSelectorImpl@200bfc09 woken with none selected
22:57:32.989 [SparkUI-34] DEBUG org.sparkproject.jetty.io.ManagedSelector - Selector sun.nio.ch.KQueueSelectorImpl@200bfc09 woken up from select, 0/0/0 selected
22:57:32.989 [SparkUI-34] DEBUG org.sparkproject.jetty.io.ManagedSelector - Selector sun.nio.ch.KQueueSelectorImpl@200bfc09 processing 0 keys, 1 updates
22:57:32.989 [SparkUI-34] DEBUG org.sparkproject.jetty.io.ManagedSelector - updateable 1
22:57:32.989 [SparkUI-34] DEBUG org.sparkproject.jetty.io.ManagedSelector - update org.sparkproject.jetty.io.ManagedSelector$CloseConnections@62293ea3
22:57:32.989 [SparkUI-34] DEBUG org.sparkproject.jetty.io.ManagedSelector - Closing 0 connections on ManagedSelector@a312b2e{STOPPING} id=3 keys=0 selected=0 updates=0
22:57:32.989 [SparkUI-34] DEBUG org.sparkproject.jetty.io.ManagedSelector - updates 0
22:57:32.989 [SparkUI-34] DEBUG org.sparkproject.jetty.io.ManagedSelector - Selector sun.nio.ch.KQueueSelectorImpl@200bfc09 waiting with 0 keys
22:57:32.990 [stop-spark-context] DEBUG org.sparkproject.jetty.io.ManagedSelector - Queued change org.sparkproject.jetty.io.ManagedSelector$StopSelector@36e74f24 on ManagedSelector@a312b2e{STOPPING} id=3 keys=0 selected=0 updates=0
22:57:32.990 [stop-spark-context] DEBUG org.sparkproject.jetty.io.ManagedSelector - Wakeup on submit ManagedSelector@a312b2e{STOPPING} id=3 keys=0 selected=0 updates=1
22:57:32.990 [SparkUI-34] DEBUG org.sparkproject.jetty.io.ManagedSelector - Selector sun.nio.ch.KQueueSelectorImpl@200bfc09 woken with none selected
22:57:32.990 [SparkUI-34] DEBUG org.sparkproject.jetty.io.ManagedSelector - Selector sun.nio.ch.KQueueSelectorImpl@200bfc09 woken up from select, 0/0/0 selected
22:57:32.990 [SparkUI-34] DEBUG org.sparkproject.jetty.io.ManagedSelector - Selector sun.nio.ch.KQueueSelectorImpl@200bfc09 processing 0 keys, 1 updates
22:57:32.990 [SparkUI-34] DEBUG org.sparkproject.jetty.io.ManagedSelector - updateable 1
22:57:32.990 [SparkUI-34] DEBUG org.sparkproject.jetty.io.ManagedSelector - update org.sparkproject.jetty.io.ManagedSelector$StopSelector@36e74f24
22:57:32.991 [SparkUI-34] DEBUG org.sparkproject.jetty.io.ManagedSelector - updates 0
22:57:32.991 [SparkUI-34] DEBUG org.sparkproject.jetty.util.thread.QueuedThreadPool - ran org.sparkproject.jetty.io.ManagedSelector$$Lambda$439/0x000000080065b840@5c0f0bc2
22:57:32.992 [stop-spark-context] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - stopping EatWhatYouKill@7dd2bc67/SelectorProducer@1db273fe/IDLE/p=false/QueuedThreadPool[SparkUI]@5dac3d49{STARTED,8<=8<=200,i=5,r=8,q=0}[ReservedThreadExecutor@23198107{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-03-04T22:57:32.991788+02:00
22:57:32.992 [stop-spark-context] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STOPPED EatWhatYouKill@7dd2bc67/SelectorProducer@1db273fe/IDLE/p=false/QueuedThreadPool[SparkUI]@5dac3d49{STARTED,8<=8<=200,i=5,r=8,q=0}[ReservedThreadExecutor@23198107{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-03-04T22:57:32.992524+02:00
22:57:32.992 [stop-spark-context] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STOPPED ManagedSelector@a312b2e{STOPPED} id=3 keys=-1 selected=-1 updates=0
22:57:32.992 [stop-spark-context] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - stopping ManagedSelector@65a42623{STARTED} id=2 keys=0 selected=0 updates=0
22:57:32.992 [stop-spark-context] DEBUG org.sparkproject.jetty.io.ManagedSelector - Queued change org.sparkproject.jetty.io.ManagedSelector$CloseConnections@6b8ab548 on ManagedSelector@65a42623{STOPPING} id=2 keys=0 selected=0 updates=0
22:57:32.992 [stop-spark-context] DEBUG org.sparkproject.jetty.io.ManagedSelector - Wakeup on submit ManagedSelector@65a42623{STOPPING} id=2 keys=0 selected=0 updates=1
22:57:32.992 [SparkUI-33] DEBUG org.sparkproject.jetty.io.ManagedSelector - Selector sun.nio.ch.KQueueSelectorImpl@6247a142 woken with none selected
22:57:32.993 [SparkUI-33] DEBUG org.sparkproject.jetty.io.ManagedSelector - Selector sun.nio.ch.KQueueSelectorImpl@6247a142 woken up from select, 0/0/0 selected
22:57:32.993 [SparkUI-33] DEBUG org.sparkproject.jetty.io.ManagedSelector - Selector sun.nio.ch.KQueueSelectorImpl@6247a142 processing 0 keys, 1 updates
22:57:32.993 [SparkUI-33] DEBUG org.sparkproject.jetty.io.ManagedSelector - updateable 1
22:57:32.993 [SparkUI-33] DEBUG org.sparkproject.jetty.io.ManagedSelector - update org.sparkproject.jetty.io.ManagedSelector$CloseConnections@6b8ab548
22:57:32.993 [SparkUI-33] DEBUG org.sparkproject.jetty.io.ManagedSelector - Closing 0 connections on ManagedSelector@65a42623{STOPPING} id=2 keys=0 selected=0 updates=0
22:57:32.993 [SparkUI-33] DEBUG org.sparkproject.jetty.io.ManagedSelector - updates 0
22:57:32.993 [SparkUI-33] DEBUG org.sparkproject.jetty.io.ManagedSelector - Selector sun.nio.ch.KQueueSelectorImpl@6247a142 waiting with 0 keys
22:57:32.993 [stop-spark-context] DEBUG org.sparkproject.jetty.io.ManagedSelector - Queued change org.sparkproject.jetty.io.ManagedSelector$StopSelector@407d42bc on ManagedSelector@65a42623{STOPPING} id=2 keys=0 selected=0 updates=0
22:57:32.993 [stop-spark-context] DEBUG org.sparkproject.jetty.io.ManagedSelector - Wakeup on submit ManagedSelector@65a42623{STOPPING} id=2 keys=0 selected=0 updates=1
22:57:32.993 [SparkUI-33] DEBUG org.sparkproject.jetty.io.ManagedSelector - Selector sun.nio.ch.KQueueSelectorImpl@6247a142 woken with none selected
22:57:32.993 [SparkUI-33] DEBUG org.sparkproject.jetty.io.ManagedSelector - Selector sun.nio.ch.KQueueSelectorImpl@6247a142 woken up from select, 0/0/0 selected
22:57:32.993 [SparkUI-33] DEBUG org.sparkproject.jetty.io.ManagedSelector - Selector sun.nio.ch.KQueueSelectorImpl@6247a142 processing 0 keys, 1 updates
22:57:32.993 [SparkUI-33] DEBUG org.sparkproject.jetty.io.ManagedSelector - updateable 1
22:57:32.993 [SparkUI-33] DEBUG org.sparkproject.jetty.io.ManagedSelector - update org.sparkproject.jetty.io.ManagedSelector$StopSelector@407d42bc
22:57:32.993 [SparkUI-33] DEBUG org.sparkproject.jetty.io.ManagedSelector - updates 0
22:57:32.993 [SparkUI-33] DEBUG org.sparkproject.jetty.util.thread.QueuedThreadPool - ran org.sparkproject.jetty.io.ManagedSelector$$Lambda$439/0x000000080065b840@3669acbd
22:57:32.993 [stop-spark-context] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - stopping EatWhatYouKill@7a435863/SelectorProducer@5f2917b9/IDLE/p=false/QueuedThreadPool[SparkUI]@5dac3d49{STARTED,8<=8<=200,i=6,r=8,q=0}[ReservedThreadExecutor@23198107{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-03-04T22:57:32.993756+02:00
22:57:32.994 [stop-spark-context] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STOPPED EatWhatYouKill@7a435863/SelectorProducer@5f2917b9/IDLE/p=false/QueuedThreadPool[SparkUI]@5dac3d49{STARTED,8<=8<=200,i=6,r=8,q=0}[ReservedThreadExecutor@23198107{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-03-04T22:57:32.993958+02:00
22:57:32.994 [stop-spark-context] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STOPPED ManagedSelector@65a42623{STOPPED} id=2 keys=-1 selected=-1 updates=0
22:57:32.994 [stop-spark-context] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - stopping ManagedSelector@4daaa898{STARTED} id=1 keys=0 selected=0 updates=0
22:57:32.994 [stop-spark-context] DEBUG org.sparkproject.jetty.io.ManagedSelector - Queued change org.sparkproject.jetty.io.ManagedSelector$CloseConnections@6581c79a on ManagedSelector@4daaa898{STOPPING} id=1 keys=0 selected=0 updates=0
22:57:32.994 [stop-spark-context] DEBUG org.sparkproject.jetty.io.ManagedSelector - Wakeup on submit ManagedSelector@4daaa898{STOPPING} id=1 keys=0 selected=0 updates=1
22:57:32.994 [SparkUI-32] DEBUG org.sparkproject.jetty.io.ManagedSelector - Selector sun.nio.ch.KQueueSelectorImpl@26c2c984 woken with none selected
22:57:32.994 [SparkUI-32] DEBUG org.sparkproject.jetty.io.ManagedSelector - Selector sun.nio.ch.KQueueSelectorImpl@26c2c984 woken up from select, 0/0/0 selected
22:57:32.994 [SparkUI-32] DEBUG org.sparkproject.jetty.io.ManagedSelector - Selector sun.nio.ch.KQueueSelectorImpl@26c2c984 processing 0 keys, 1 updates
22:57:32.994 [SparkUI-32] DEBUG org.sparkproject.jetty.io.ManagedSelector - updateable 1
22:57:32.994 [SparkUI-32] DEBUG org.sparkproject.jetty.io.ManagedSelector - update org.sparkproject.jetty.io.ManagedSelector$CloseConnections@6581c79a
22:57:32.994 [SparkUI-32] DEBUG org.sparkproject.jetty.io.ManagedSelector - Closing 0 connections on ManagedSelector@4daaa898{STOPPING} id=1 keys=0 selected=0 updates=0
22:57:32.994 [SparkUI-32] DEBUG org.sparkproject.jetty.io.ManagedSelector - updates 0
22:57:32.994 [SparkUI-32] DEBUG org.sparkproject.jetty.io.ManagedSelector - Selector sun.nio.ch.KQueueSelectorImpl@26c2c984 waiting with 0 keys
22:57:32.994 [stop-spark-context] DEBUG org.sparkproject.jetty.io.ManagedSelector - Queued change org.sparkproject.jetty.io.ManagedSelector$StopSelector@69933a61 on ManagedSelector@4daaa898{STOPPING} id=1 keys=0 selected=0 updates=0
22:57:32.994 [stop-spark-context] DEBUG org.sparkproject.jetty.io.ManagedSelector - Wakeup on submit ManagedSelector@4daaa898{STOPPING} id=1 keys=0 selected=0 updates=1
22:57:32.994 [SparkUI-32] DEBUG org.sparkproject.jetty.io.ManagedSelector - Selector sun.nio.ch.KQueueSelectorImpl@26c2c984 woken with none selected
22:57:32.994 [SparkUI-32] DEBUG org.sparkproject.jetty.io.ManagedSelector - Selector sun.nio.ch.KQueueSelectorImpl@26c2c984 woken up from select, 0/0/0 selected
22:57:32.994 [SparkUI-32] DEBUG org.sparkproject.jetty.io.ManagedSelector - Selector sun.nio.ch.KQueueSelectorImpl@26c2c984 processing 0 keys, 1 updates
22:57:32.994 [SparkUI-32] DEBUG org.sparkproject.jetty.io.ManagedSelector - updateable 1
22:57:32.995 [SparkUI-32] DEBUG org.sparkproject.jetty.io.ManagedSelector - update org.sparkproject.jetty.io.ManagedSelector$StopSelector@69933a61
22:57:32.995 [SparkUI-32] DEBUG org.sparkproject.jetty.io.ManagedSelector - updates 0
22:57:32.995 [SparkUI-32] DEBUG org.sparkproject.jetty.util.thread.QueuedThreadPool - ran org.sparkproject.jetty.io.ManagedSelector$$Lambda$439/0x000000080065b840@72b56713
22:57:32.997 [stop-spark-context] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - stopping EatWhatYouKill@1a77f70d/SelectorProducer@5edc9d72/IDLE/p=false/QueuedThreadPool[SparkUI]@5dac3d49{STARTED,8<=8<=200,i=7,r=8,q=0}[ReservedThreadExecutor@23198107{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-03-04T22:57:32.995301+02:00
22:57:32.997 [stop-spark-context] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STOPPED EatWhatYouKill@1a77f70d/SelectorProducer@5edc9d72/IDLE/p=false/QueuedThreadPool[SparkUI]@5dac3d49{STARTED,8<=8<=200,i=7,r=8,q=0}[ReservedThreadExecutor@23198107{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-03-04T22:57:32.997515+02:00
22:57:32.997 [stop-spark-context] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STOPPED ManagedSelector@4daaa898{STOPPED} id=1 keys=-1 selected=-1 updates=0
22:57:32.997 [stop-spark-context] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - stopping ManagedSelector@1825ff79{STARTED} id=0 keys=0 selected=0 updates=0
22:57:32.997 [stop-spark-context] DEBUG org.sparkproject.jetty.io.ManagedSelector - Queued change org.sparkproject.jetty.io.ManagedSelector$CloseConnections@578df2ee on ManagedSelector@1825ff79{STOPPING} id=0 keys=0 selected=0 updates=0
22:57:32.997 [stop-spark-context] DEBUG org.sparkproject.jetty.io.ManagedSelector - Wakeup on submit ManagedSelector@1825ff79{STOPPING} id=0 keys=0 selected=0 updates=1
22:57:32.997 [SparkUI-31] DEBUG org.sparkproject.jetty.io.ManagedSelector - Selector sun.nio.ch.KQueueSelectorImpl@6e95f513 woken with none selected
22:57:32.997 [SparkUI-31] DEBUG org.sparkproject.jetty.io.ManagedSelector - Selector sun.nio.ch.KQueueSelectorImpl@6e95f513 woken up from select, 0/0/0 selected
22:57:32.998 [SparkUI-31] DEBUG org.sparkproject.jetty.io.ManagedSelector - Selector sun.nio.ch.KQueueSelectorImpl@6e95f513 processing 0 keys, 1 updates
22:57:32.998 [SparkUI-31] DEBUG org.sparkproject.jetty.io.ManagedSelector - updateable 1
22:57:32.998 [SparkUI-31] DEBUG org.sparkproject.jetty.io.ManagedSelector - update org.sparkproject.jetty.io.ManagedSelector$CloseConnections@578df2ee
22:57:32.998 [SparkUI-31] DEBUG org.sparkproject.jetty.io.ManagedSelector - Closing 0 connections on ManagedSelector@1825ff79{STOPPING} id=0 keys=0 selected=0 updates=0
22:57:32.998 [SparkUI-31] DEBUG org.sparkproject.jetty.io.ManagedSelector - updates 0
22:57:32.998 [SparkUI-31] DEBUG org.sparkproject.jetty.io.ManagedSelector - Selector sun.nio.ch.KQueueSelectorImpl@6e95f513 waiting with 0 keys
22:57:32.998 [stop-spark-context] DEBUG org.sparkproject.jetty.io.ManagedSelector - Queued change org.sparkproject.jetty.io.ManagedSelector$StopSelector@7d4f2201 on ManagedSelector@1825ff79{STOPPING} id=0 keys=0 selected=0 updates=0
22:57:32.998 [stop-spark-context] DEBUG org.sparkproject.jetty.io.ManagedSelector - Wakeup on submit ManagedSelector@1825ff79{STOPPING} id=0 keys=0 selected=0 updates=1
22:57:32.998 [SparkUI-31] DEBUG org.sparkproject.jetty.io.ManagedSelector - Selector sun.nio.ch.KQueueSelectorImpl@6e95f513 woken with none selected
22:57:32.998 [SparkUI-31] DEBUG org.sparkproject.jetty.io.ManagedSelector - Selector sun.nio.ch.KQueueSelectorImpl@6e95f513 woken up from select, 0/0/0 selected
22:57:32.998 [SparkUI-31] DEBUG org.sparkproject.jetty.io.ManagedSelector - Selector sun.nio.ch.KQueueSelectorImpl@6e95f513 processing 0 keys, 1 updates
22:57:32.998 [SparkUI-31] DEBUG org.sparkproject.jetty.io.ManagedSelector - updateable 1
22:57:32.998 [SparkUI-31] DEBUG org.sparkproject.jetty.io.ManagedSelector - update org.sparkproject.jetty.io.ManagedSelector$StopSelector@7d4f2201
22:57:32.998 [SparkUI-31] DEBUG org.sparkproject.jetty.io.ManagedSelector - updates 0
22:57:32.998 [SparkUI-31] DEBUG org.sparkproject.jetty.util.thread.QueuedThreadPool - ran org.sparkproject.jetty.io.ManagedSelector$$Lambda$439/0x000000080065b840@289a97f0
22:57:32.998 [stop-spark-context] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - stopping EatWhatYouKill@4fac2436/SelectorProducer@46b8db83/IDLE/p=false/QueuedThreadPool[SparkUI]@5dac3d49{STARTED,8<=8<=200,i=8,r=8,q=0}[ReservedThreadExecutor@23198107{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-03-04T22:57:32.998781+02:00
22:57:32.999 [stop-spark-context] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STOPPED EatWhatYouKill@4fac2436/SelectorProducer@46b8db83/IDLE/p=false/QueuedThreadPool[SparkUI]@5dac3d49{STARTED,8<=8<=200,i=8,r=8,q=0}[ReservedThreadExecutor@23198107{s=0/8,p=0}][pc=0,pic=0,pec=0,epc=0]@2022-03-04T22:57:32.998983+02:00
22:57:32.999 [stop-spark-context] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STOPPED ManagedSelector@1825ff79{STOPPED} id=0 keys=-1 selected=-1 updates=0
22:57:32.999 [stop-spark-context] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STOPPED SelectorManager@Spark@71f030a{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
22:57:32.999 [stop-spark-context] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - stopping HttpConnectionFactory@2acef75d[HTTP/1.1]
22:57:33.000 [stop-spark-context] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STOPPED HttpConnectionFactory@2acef75d[HTTP/1.1]
22:57:33.000 [stop-spark-context] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - stopping ScheduledExecutorScheduler@71d5e1a6{STARTED}
22:57:33.000 [stop-spark-context] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STOPPED ScheduledExecutorScheduler@71d5e1a6{STOPPED}
22:57:33.000 [stop-spark-context] INFO org.sparkproject.jetty.server.AbstractConnector - Stopped Spark@71f030a{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
22:57:33.000 [stop-spark-context] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STOPPED Spark@71f030a{HTTP/1.1,[http/1.1]}{0.0.0.0:4040}
22:57:33.000 [stop-spark-context] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - stopping Server@1e82d188{STOPPING}[9.4.z-SNAPSHOT]
22:57:33.000 [stop-spark-context] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - stopping ContextHandlerCollection@7d170ea6{STARTED}
22:57:33.000 [stop-spark-context] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - stopping ContextHandlerCollection@7d170ea6{STOPPING}
22:57:33.000 [stop-spark-context] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STOPPED ContextHandlerCollection@7d170ea6{STOPPED}
22:57:33.000 [stop-spark-context] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - stopping ErrorHandler@397cd7ee{STARTED}
22:57:33.000 [stop-spark-context] DEBUG org.sparkproject.jetty.server.handler.AbstractHandler - stopping ErrorHandler@397cd7ee{STOPPING}
22:57:33.000 [stop-spark-context] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STOPPED ErrorHandler@397cd7ee{STOPPED}
22:57:33.001 [stop-spark-context] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - stopping QueuedThreadPool[SparkUI]@5dac3d49{STARTED,8<=8<=200,i=8,r=8,q=0}[ReservedThreadExecutor@23198107{s=0/8,p=0}]
22:57:33.001 [stop-spark-context] DEBUG org.sparkproject.jetty.util.thread.QueuedThreadPool - Stopping QueuedThreadPool[SparkUI]@5dac3d49{STOPPING,8<=8<=200,i=8,r=-1,q=0}[ReservedThreadExecutor@23198107{s=0/8,p=0}]
22:57:33.001 [stop-spark-context] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - stopping ReservedThreadExecutor@23198107{s=0/8,p=0}
22:57:33.001 [stop-spark-context] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STOPPED ReservedThreadExecutor@23198107{s=0/8,p=0}
22:57:33.002 [stop-spark-context] DEBUG org.sparkproject.jetty.util.thread.QueuedThreadPool - Waiting for Thread[SparkUI-118,5,com.example.main.WuzzufJobsAnalysisApplication] for 14999
22:57:33.002 [SparkUI-35] DEBUG org.sparkproject.jetty.util.thread.QueuedThreadPool - run org.sparkproject.jetty.util.thread.QueuedThreadPool$$Lambda$2915/0x000000080142f440@7bb7f690
22:57:33.002 [SparkUI-35] DEBUG org.sparkproject.jetty.util.thread.QueuedThreadPool - ran org.sparkproject.jetty.util.thread.QueuedThreadPool$$Lambda$2915/0x000000080142f440@7bb7f690
22:57:33.002 [SparkUI-118] DEBUG org.sparkproject.jetty.util.thread.QueuedThreadPool - run org.sparkproject.jetty.util.thread.QueuedThreadPool$$Lambda$2915/0x000000080142f440@7bb7f690
22:57:33.002 [SparkUI-118] DEBUG org.sparkproject.jetty.util.thread.QueuedThreadPool - ran org.sparkproject.jetty.util.thread.QueuedThreadPool$$Lambda$2915/0x000000080142f440@7bb7f690
22:57:33.002 [SparkUI-120] DEBUG org.sparkproject.jetty.util.thread.QueuedThreadPool - run org.sparkproject.jetty.util.thread.QueuedThreadPool$$Lambda$2915/0x000000080142f440@7bb7f690
22:57:33.002 [SparkUI-34] DEBUG org.sparkproject.jetty.util.thread.QueuedThreadPool - run org.sparkproject.jetty.util.thread.QueuedThreadPool$$Lambda$2915/0x000000080142f440@7bb7f690
22:57:33.002 [SparkUI-119] DEBUG org.sparkproject.jetty.util.thread.QueuedThreadPool - run org.sparkproject.jetty.util.thread.QueuedThreadPool$$Lambda$2915/0x000000080142f440@7bb7f690
22:57:33.002 [SparkUI-33] DEBUG org.sparkproject.jetty.util.thread.QueuedThreadPool - run org.sparkproject.jetty.util.thread.QueuedThreadPool$$Lambda$2915/0x000000080142f440@7bb7f690
22:57:33.002 [SparkUI-120] DEBUG org.sparkproject.jetty.util.thread.QueuedThreadPool - ran org.sparkproject.jetty.util.thread.QueuedThreadPool$$Lambda$2915/0x000000080142f440@7bb7f690
22:57:33.002 [SparkUI-31] DEBUG org.sparkproject.jetty.util.thread.QueuedThreadPool - run org.sparkproject.jetty.util.thread.QueuedThreadPool$$Lambda$2915/0x000000080142f440@7bb7f690
22:57:33.002 [SparkUI-34] DEBUG org.sparkproject.jetty.util.thread.QueuedThreadPool - ran org.sparkproject.jetty.util.thread.QueuedThreadPool$$Lambda$2915/0x000000080142f440@7bb7f690
22:57:33.002 [SparkUI-119] DEBUG org.sparkproject.jetty.util.thread.QueuedThreadPool - ran org.sparkproject.jetty.util.thread.QueuedThreadPool$$Lambda$2915/0x000000080142f440@7bb7f690
22:57:33.002 [stop-spark-context] DEBUG org.sparkproject.jetty.util.thread.QueuedThreadPool - Waiting for Thread[SparkUI-35,5,] for 14999
22:57:33.002 [SparkUI-33] DEBUG org.sparkproject.jetty.util.thread.QueuedThreadPool - ran org.sparkproject.jetty.util.thread.QueuedThreadPool$$Lambda$2915/0x000000080142f440@7bb7f690
22:57:33.002 [SparkUI-32] DEBUG org.sparkproject.jetty.util.thread.QueuedThreadPool - run org.sparkproject.jetty.util.thread.QueuedThreadPool$$Lambda$2915/0x000000080142f440@7bb7f690
22:57:33.002 [stop-spark-context] DEBUG org.sparkproject.jetty.util.thread.QueuedThreadPool - Waiting for Thread[SparkUI-120,5,] for 14999
22:57:33.002 [SparkUI-31] DEBUG org.sparkproject.jetty.util.thread.QueuedThreadPool - ran org.sparkproject.jetty.util.thread.QueuedThreadPool$$Lambda$2915/0x000000080142f440@7bb7f690
22:57:33.002 [stop-spark-context] DEBUG org.sparkproject.jetty.util.thread.QueuedThreadPool - Waiting for Thread[SparkUI-31,5,com.example.main.WuzzufJobsAnalysisApplication] for 14999
22:57:33.002 [SparkUI-32] DEBUG org.sparkproject.jetty.util.thread.QueuedThreadPool - ran org.sparkproject.jetty.util.thread.QueuedThreadPool$$Lambda$2915/0x000000080142f440@7bb7f690
22:57:33.003 [stop-spark-context] DEBUG org.sparkproject.jetty.util.thread.QueuedThreadPool - Waiting for Thread[SparkUI-32,5,] for 14999
22:57:33.003 [stop-spark-context] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STOPPED QueuedThreadPool[SparkUI]@5dac3d49{STOPPED,8<=0<=200,i=0,r=-1,q=0}[NO_TRY]
22:57:33.005 [stop-spark-context] DEBUG org.sparkproject.jetty.util.component.AbstractLifeCycle - STOPPED Server@1e82d188{STOPPED}[9.4.z-SNAPSHOT]
22:57:33.006 [stop-spark-context] INFO org.apache.spark.ui.SparkUI - Stopped Spark web UI at http://saras-air:4040
22:57:36.249 [executor-heartbeater] WARN org.apache.spark.executor.Executor - Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:302)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
	at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:913)
	at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:200)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)
	at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.SparkException: Could not find HeartbeatReceiver.
	at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:169)
	at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:144)
	at org.apache.spark.rpc.netty.NettyRpcEnv.askAbortable(NettyRpcEnv.scala:242)
	at org.apache.spark.rpc.netty.NettyRpcEndpointRef.askAbortable(NettyRpcEnv.scala:548)
	at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:552)
	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:102)
	... 11 common frames omitted
22:57:46.249 [executor-heartbeater] WARN org.apache.spark.executor.Executor - Issue communicating with driver in heartbeater
org.apache.spark.SparkException: Exception thrown in awaitResult: 
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:302)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
	at org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:913)
	at org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:200)
	at scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1932)
	at org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)
	at java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515)
	at java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:305)
	at java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)
	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)
	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)
	at java.base/java.lang.Thread.run(Thread.java:834)
Caused by: org.apache.spark.SparkException: Could not find HeartbeatReceiver.
	at org.apache.spark.rpc.netty.Dispatcher.postMessage(Dispatcher.scala:169)
	at org.apache.spark.rpc.netty.Dispatcher.postLocalMessage(Dispatcher.scala:144)
	at org.apache.spark.rpc.netty.NettyRpcEnv.askAbortable(NettyRpcEnv.scala:242)
	at org.apache.spark.rpc.netty.NettyRpcEndpointRef.askAbortable(NettyRpcEnv.scala:548)
	at org.apache.spark.rpc.netty.NettyRpcEndpointRef.ask(NettyRpcEnv.scala:552)
	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:102)
	... 11 common frames omitted
[WARNING] thread Thread[rpc-boss-3-1,5,com.example.main.WuzzufJobsAnalysisApplication] was interrupted but is still alive after waiting at least 14981msecs
[WARNING] thread Thread[rpc-boss-3-1,5,com.example.main.WuzzufJobsAnalysisApplication] will linger despite being asked to die via interruption
[WARNING] thread Thread[dispatcher-event-loop-0,5,com.example.main.WuzzufJobsAnalysisApplication] will linger despite being asked to die via interruption
[WARNING] thread Thread[dispatcher-event-loop-1,5,com.example.main.WuzzufJobsAnalysisApplication] will linger despite being asked to die via interruption
[WARNING] thread Thread[dispatcher-BlockManagerMaster,5,com.example.main.WuzzufJobsAnalysisApplication] will linger despite being asked to die via interruption
[WARNING] thread Thread[dispatcher-BlockManagerEndpoint1,5,com.example.main.WuzzufJobsAnalysisApplication] will linger despite being asked to die via interruption
[WARNING] thread Thread[RemoteBlock-temp-file-clean-thread,5,com.example.main.WuzzufJobsAnalysisApplication] will linger despite being asked to die via interruption
[WARNING] thread Thread[heartbeat-receiver-event-loop-thread,5,com.example.main.WuzzufJobsAnalysisApplication] will linger despite being asked to die via interruption
[WARNING] thread Thread[netty-rpc-env-timeout,5,com.example.main.WuzzufJobsAnalysisApplication] will linger despite being asked to die via interruption
[WARNING] thread Thread[executor-heartbeater,5,com.example.main.WuzzufJobsAnalysisApplication] will linger despite being asked to die via interruption
[WARNING] thread Thread[shuffle-boss-6-1,5,com.example.main.WuzzufJobsAnalysisApplication] will linger despite being asked to die via interruption
[WARNING] thread Thread[element-tracking-store-worker,5,com.example.main.WuzzufJobsAnalysisApplication] will linger despite being asked to die via interruption
[WARNING] thread Thread[org.apache.hadoop.fs.FileSystem$Statistics$StatisticsDataReferenceCleaner,5,com.example.main.WuzzufJobsAnalysisApplication] will linger despite being asked to die via interruption
[WARNING] thread Thread[Executor task launch worker for task 5,5,com.example.main.WuzzufJobsAnalysisApplication] will linger despite being asked to die via interruption
[WARNING] thread Thread[block-manager-slave-async-thread-pool-0,5,com.example.main.WuzzufJobsAnalysisApplication] will linger despite being asked to die via interruption
[WARNING] thread Thread[block-manager-slave-async-thread-pool-1,5,com.example.main.WuzzufJobsAnalysisApplication] will linger despite being asked to die via interruption
[WARNING] thread Thread[block-manager-slave-async-thread-pool-2,5,com.example.main.WuzzufJobsAnalysisApplication] will linger despite being asked to die via interruption
[WARNING] thread Thread[block-manager-ask-thread-pool-0,5,com.example.main.WuzzufJobsAnalysisApplication] will linger despite being asked to die via interruption
[WARNING] thread Thread[block-manager-ask-thread-pool-1,5,com.example.main.WuzzufJobsAnalysisApplication] will linger despite being asked to die via interruption
[WARNING] thread Thread[block-manager-slave-async-thread-pool-3,5,com.example.main.WuzzufJobsAnalysisApplication] will linger despite being asked to die via interruption
[WARNING] thread Thread[block-manager-slave-async-thread-pool-4,5,com.example.main.WuzzufJobsAnalysisApplication] will linger despite being asked to die via interruption
[WARNING] thread Thread[block-manager-slave-async-thread-pool-5,5,com.example.main.WuzzufJobsAnalysisApplication] will linger despite being asked to die via interruption
[WARNING] thread Thread[block-manager-ask-thread-pool-2,5,com.example.main.WuzzufJobsAnalysisApplication] will linger despite being asked to die via interruption
[WARNING] thread Thread[block-manager-ask-thread-pool-3,5,com.example.main.WuzzufJobsAnalysisApplication] will linger despite being asked to die via interruption
[WARNING] thread Thread[block-manager-slave-async-thread-pool-6,5,com.example.main.WuzzufJobsAnalysisApplication] will linger despite being asked to die via interruption
[WARNING] thread Thread[block-manager-slave-async-thread-pool-7,5,com.example.main.WuzzufJobsAnalysisApplication] will linger despite being asked to die via interruption
[WARNING] thread Thread[block-manager-slave-async-thread-pool-8,5,com.example.main.WuzzufJobsAnalysisApplication] will linger despite being asked to die via interruption
[WARNING] thread Thread[block-manager-ask-thread-pool-4,5,com.example.main.WuzzufJobsAnalysisApplication] will linger despite being asked to die via interruption
[WARNING] thread Thread[block-manager-ask-thread-pool-5,5,com.example.main.WuzzufJobsAnalysisApplication] will linger despite being asked to die via interruption
[WARNING] thread Thread[block-manager-slave-async-thread-pool-9,5,com.example.main.WuzzufJobsAnalysisApplication] will linger despite being asked to die via interruption
[WARNING] thread Thread[block-manager-slave-async-thread-pool-10,5,com.example.main.WuzzufJobsAnalysisApplication] will linger despite being asked to die via interruption
[WARNING] thread Thread[block-manager-slave-async-thread-pool-11,5,com.example.main.WuzzufJobsAnalysisApplication] will linger despite being asked to die via interruption
[WARNING] thread Thread[block-manager-ask-thread-pool-6,5,com.example.main.WuzzufJobsAnalysisApplication] will linger despite being asked to die via interruption
[WARNING] thread Thread[block-manager-ask-thread-pool-7,5,com.example.main.WuzzufJobsAnalysisApplication] will linger despite being asked to die via interruption
[WARNING] thread Thread[block-manager-slave-async-thread-pool-12,5,com.example.main.WuzzufJobsAnalysisApplication] will linger despite being asked to die via interruption
[WARNING] thread Thread[block-manager-slave-async-thread-pool-13,5,com.example.main.WuzzufJobsAnalysisApplication] will linger despite being asked to die via interruption
[WARNING] thread Thread[block-manager-slave-async-thread-pool-14,5,com.example.main.WuzzufJobsAnalysisApplication] will linger despite being asked to die via interruption
[WARNING] thread Thread[block-manager-ask-thread-pool-8,5,com.example.main.WuzzufJobsAnalysisApplication] will linger despite being asked to die via interruption
[WARNING] thread Thread[block-manager-ask-thread-pool-9,5,com.example.main.WuzzufJobsAnalysisApplication] will linger despite being asked to die via interruption
[WARNING] thread Thread[block-manager-slave-async-thread-pool-15,5,com.example.main.WuzzufJobsAnalysisApplication] will linger despite being asked to die via interruption
[WARNING] thread Thread[block-manager-slave-async-thread-pool-16,5,com.example.main.WuzzufJobsAnalysisApplication] will linger despite being asked to die via interruption
[WARNING] thread Thread[block-manager-slave-async-thread-pool-17,5,com.example.main.WuzzufJobsAnalysisApplication] will linger despite being asked to die via interruption
[WARNING] thread Thread[block-manager-slave-async-thread-pool-18,5,com.example.main.WuzzufJobsAnalysisApplication] will linger despite being asked to die via interruption
[WARNING] thread Thread[block-manager-ask-thread-pool-10,5,com.example.main.WuzzufJobsAnalysisApplication] will linger despite being asked to die via interruption
[WARNING] thread Thread[block-manager-slave-async-thread-pool-19,5,com.example.main.WuzzufJobsAnalysisApplication] will linger despite being asked to die via interruption
[WARNING] thread Thread[block-manager-slave-async-thread-pool-20,5,com.example.main.WuzzufJobsAnalysisApplication] will linger despite being asked to die via interruption
[WARNING] thread Thread[block-manager-ask-thread-pool-11,5,com.example.main.WuzzufJobsAnalysisApplication] will linger despite being asked to die via interruption
[WARNING] thread Thread[block-manager-ask-thread-pool-12,5,com.example.main.WuzzufJobsAnalysisApplication] will linger despite being asked to die via interruption
[WARNING] thread Thread[block-manager-slave-async-thread-pool-21,5,com.example.main.WuzzufJobsAnalysisApplication] will linger despite being asked to die via interruption
[WARNING] thread Thread[block-manager-slave-async-thread-pool-22,5,com.example.main.WuzzufJobsAnalysisApplication] will linger despite being asked to die via interruption
[WARNING] thread Thread[block-manager-slave-async-thread-pool-23,5,com.example.main.WuzzufJobsAnalysisApplication] will linger despite being asked to die via interruption
[WARNING] thread Thread[block-manager-ask-thread-pool-13,5,com.example.main.WuzzufJobsAnalysisApplication] will linger despite being asked to die via interruption
[WARNING] thread Thread[block-manager-ask-thread-pool-14,5,com.example.main.WuzzufJobsAnalysisApplication] will linger despite being asked to die via interruption
[WARNING] thread Thread[block-manager-slave-async-thread-pool-24,5,com.example.main.WuzzufJobsAnalysisApplication] will linger despite being asked to die via interruption
[WARNING] thread Thread[block-manager-slave-async-thread-pool-25,5,com.example.main.WuzzufJobsAnalysisApplication] will linger despite being asked to die via interruption
[WARNING] thread Thread[block-manager-slave-async-thread-pool-26,5,com.example.main.WuzzufJobsAnalysisApplication] will linger despite being asked to die via interruption
[WARNING] thread Thread[block-manager-ask-thread-pool-15,5,com.example.main.WuzzufJobsAnalysisApplication] will linger despite being asked to die via interruption
[WARNING] thread Thread[block-manager-ask-thread-pool-16,5,com.example.main.WuzzufJobsAnalysisApplication] will linger despite being asked to die via interruption
[WARNING] thread Thread[block-manager-slave-async-thread-pool-27,5,com.example.main.WuzzufJobsAnalysisApplication] will linger despite being asked to die via interruption
[WARNING] thread Thread[block-manager-slave-async-thread-pool-28,5,com.example.main.WuzzufJobsAnalysisApplication] will linger despite being asked to die via interruption
[WARNING] thread Thread[block-manager-slave-async-thread-pool-29,5,com.example.main.WuzzufJobsAnalysisApplication] will linger despite being asked to die via interruption
[WARNING] thread Thread[block-manager-ask-thread-pool-17,5,com.example.main.WuzzufJobsAnalysisApplication] will linger despite being asked to die via interruption
[WARNING] thread Thread[block-manager-slave-async-thread-pool-30,5,com.example.main.WuzzufJobsAnalysisApplication] will linger despite being asked to die via interruption
[WARNING] thread Thread[block-manager-slave-async-thread-pool-31,5,com.example.main.WuzzufJobsAnalysisApplication] will linger despite being asked to die via interruption
[WARNING] thread Thread[block-manager-slave-async-thread-pool-32,5,com.example.main.WuzzufJobsAnalysisApplication] will linger despite being asked to die via interruption
[WARNING] thread Thread[block-manager-ask-thread-pool-18,5,com.example.main.WuzzufJobsAnalysisApplication] will linger despite being asked to die via interruption
[WARNING] thread Thread[block-manager-ask-thread-pool-19,5,com.example.main.WuzzufJobsAnalysisApplication] will linger despite being asked to die via interruption
[WARNING] thread Thread[stop-spark-context,5,com.example.main.WuzzufJobsAnalysisApplication] will linger despite being asked to die via interruption
[WARNING] NOTE: 67 thread(s) did not finish despite being asked to  via interruption. This is not a problem with exec:java, it is a problem with the running code. Although not serious, it should be remedied.
[WARNING] Couldn't destroy threadgroup org.codehaus.mojo.exec.ExecJavaMojo$IsolatedThreadGroup[name=com.example.main.WuzzufJobsAnalysisApplication,maxpri=10]
java.lang.IllegalThreadStateException
    at java.lang.ThreadGroup.destroy (ThreadGroup.java:776)
    at org.codehaus.mojo.exec.ExecJavaMojo.execute (ExecJavaMojo.java:293)
    at org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo (DefaultBuildPluginManager.java:137)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:210)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:156)
    at org.apache.maven.lifecycle.internal.MojoExecutor.execute (MojoExecutor.java:148)
    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:117)
    at org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject (LifecycleModuleBuilder.java:81)
    at org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build (SingleThreadedBuilder.java:56)
    at org.apache.maven.lifecycle.internal.LifecycleStarter.execute (LifecycleStarter.java:128)
    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:305)
    at org.apache.maven.DefaultMaven.doExecute (DefaultMaven.java:192)
    22:57:47.901 [stop-spark-context] ERROR org.apache.spark.MapOutputTrackerMaster - Error communicating with MapOutputTracker
java.lang.InterruptedException: null
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1369)
	at scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:248)
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:258)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:263)
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:294)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)
	at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:305)
	at org.apache.spark.MapOutputTracker.sendTracker(MapOutputTracker.scala:315)
	at org.apache.spark.MapOutputTrackerMaster.stop(MapOutputTracker.scala:778)
	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:89)
	at org.apache.spark.SparkContext.$anonfun$stop$23(SparkContext.scala:2012)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1357)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:2012)
	at org.apache.spark.SparkContext$$anon$3.run(SparkContext.scala:1922)
at org.apache.maven.DefaultMaven.execute (DefaultMaven.java:105)
    at org.apache.maven.cli.MavenCli.execute (MavenCli.java:972)
    at org.apache.maven.cli.MavenCli.doMain (MavenCli.java:293)
    at org.apache.maven.cli.MavenCli.main (MavenCli.java:196)
    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke0 (Native Method)
    at jdk.internal.reflect.NativeMethodAccessorImpl.invoke (NativeMethodAccessorImpl.java:62)
    at jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke (DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke (Method.java:566)
    at org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced (Launcher.java:282)
    at org.codehaus.plexus.classworlds.launcher.Launcher.launch (Launcher.java:225)
    at org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode (Launcher.java:406)
    at org.codehaus.plexus.classworlds.launcher.Launcher.main (Launcher.java:347)
22:57:47.914 [stop-spark-context] ERROR org.apache.spark.util.Utils - Uncaught exception in thread stop-spark-context
org.apache.spark.SparkException: Error communicating with MapOutputTracker
	at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:309)
	at org.apache.spark.MapOutputTracker.sendTracker(MapOutputTracker.scala:315)
	at org.apache.spark.MapOutputTrackerMaster.stop(MapOutputTracker.scala:778)
	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:89)
	at org.apache.spark.SparkContext.$anonfun$stop$23(SparkContext.scala:2012)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1357)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:2012)
	at org.apache.spark.SparkContext$$anon$3.run(SparkContext.scala:1922)
Caused by: java.lang.InterruptedException: null
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedNanos(AbstractQueuedSynchronizer.java:1081)
	at java.base/java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1369)
	at scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:248)
	at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:258)
	at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:263)
	at org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:294)
	at org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)
	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:103)
	at org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:87)
	at org.apache.spark.MapOutputTracker.askTracker(MapOutputTracker.scala:305)
	... 7 common frames omitted
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
22:57:47.919 [stop-spark-context] INFO org.apache.spark.SparkContext - Successfully stopped SparkContext
[INFO] Total time:  42.114 s
[INFO] Finished at: 2022-03-04T22:57:47+02:00
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.codehaus.mojo:exec-maven-plugin:3.0.0:java (default-cli) on project WuzzufJobsAnalysis: An exception occured while executing the Java class. null: InterruptedException -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
